{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff57ecf",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Transformers are a sophisiticated modern neural network best adapted to sequential data. They are in the process of replacing the recurrent neural network as the de-facto model for sequence analysis. As a brief review: recurrent neural networks incorporate information about sequential dependencies through recurrent connections which allow a hidden state to incorporate variables that have been previously exposed to the network. There are some principle shortcomings with this approach: short memory, exploding/vanishing gradients, and $O(n)$ complexity. Tokens that are historically adjacent to each other have more weight and thus the information in the sequence decays over long ranges; the network has a relatively short memory. The hidden state is called recursively over the course of training and thus the gradients have a tendency to either explode or vanish and various regularisation tricks are needed to stablise the training routine. Finally, the current hidden state depends on the previous hidden state and the data must therefore be parsed sequentially - there is no parallelisation routine. This results in very computationally intensive training.\n",
    "\n",
    "The Transformer was introduced in 2016 in the seminal paper [Attention is all you need](https://arxiv.org/abs/1706.03762) and since then have been the basis of most large language models. They neatly solve all three problems with reccurent neural networks with the idea of attention. It should be noted that attention is not a pioneering feature of this paper. However, the attention mechanisms neatly allows the author to encode long range and importantly asymetric dependencies between sequence tokens. These attention representations are bidirectional and all-to-all allowing for massive parrallelisation within an attention layer. They are also relatively stable in their gradients because attention representations are not recursively computed.\n",
    "\n",
    "The outcomes of this notebook are:\n",
    "\n",
    "* Understanding attention and multi-head attention.\n",
    "* Creating a custom Flux layer to represent attention heads.\n",
    "* Understanding sequence generation.\n",
    "* A comprenhensive look at the transformer architecture.\n",
    "* Implementing a custom Transformer network.\n",
    "* Training a Transformer on biological sequence data.\n",
    "\n",
    "## Self Attention\n",
    "\n",
    "Attention is the key concept in the Transformer model. It heuristically refers to how much each element in the sequence relates to one and other. The simplest form of this is the Euclidean projection of one vector onto another and the attention matrix is accordingly defined by dot-product; recall that the dot product has the geometric intepretation of how much one element projects onto another. Suppose that the sequence elements are $v_i$\n",
    "\n",
    "$$ W_{ij} = v_j v_i^T $$\n",
    "\n",
    "Now that we have created the attention relationships we want to output some intepretable configuration of vectors that incorporate these relationships. The most straightforward approach is a weighted linear combination of all vectors in the sequence: the attention weights are used to incorporate each vectors influence on each other vector.\n",
    "\n",
    "$$ a_i = \\sum_j W_{ij} v_j $$\n",
    "\n",
    "Finally, we want to scale these outputs into the \"interpretable\" regime therefore we apply a normalisation factor (scaling by the sqrt of the dimensionality) and the `softmax` function to convert these output into a probability distribution. These are just implementation details. The next thing we can note, and the algebra suggests it automatically, is that this can be parallelised. We can concatenate all the input vectors into a matrix $A$ and pre-multiply it by the weight matrix $W$ completing the attention transformation in a single step.\n",
    "\n",
    "$$ V = \\text{softmax}\\left(\\frac{W}{\\sqrt{d}}\\right) V$$\n",
    "\n",
    "## Keys, Queries, and Values\n",
    "Up until this point we have not provided any mechanism to *learn*. It would be perculiar if the primary component of a learning algorithm had no learnable parameters. Before we rectify this we need to clarify some terminology. If we apply the same vector concatenation trick to constructing the attention matrix we find that the attention matrix $W$ is the product of two matrices $A$ and $A^T$ which for now we will label $K$ and $Q$. Let's imagine that each of the input vectors where one-hot encoded: they had a 1 in a singular index and a 0 elsewhere. The resulting attention weights would be one if and only if the input indexes were identical: $i = j$. This means that the operation is acting like a dictionary or look-up table would act: if the first vector matches the second vector output a Boolean true. In computer science these are generally labelled *keys* and *queries*. These terms motivate our matrices $K$ and $Q$. The final component of the look up table is to output the value associated with a queried key. These are nothing more than the input vectors again and are encoded in our $V$ matrix.\n",
    "\n",
    "So far, the input vectors have played the key role in all the operations and we cant expect to learn much about their relational structure. Let's suppose that there is a more efficient lookup table that could encoded the data. This would amount to an abitrary set of keys and queries, but the values would remain the same. We can imagine the keys and queries as embedding matrices and this would allow us to embedd our inputs in a lower (or higher) dimensional space. Therefore, if the input tokens $v_i$ are vectors of length $N$ and we want to embed them into a space of dimension $P$ then our key and query matrices should have dimensions $N \\times P$ and $P \\times N$ respectively. These matrices will now be intialised in a way that does not need to depend on $v_i$, usually just a random number in each slot. The goal will be to learn the best form of these matrices. This can be thought of in much the same way as a convolutional filter where the goal was to learn the kernel weights. We finally arrive at the complete attention equation:\n",
    "\n",
    "$$ A(K, Q, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d}}\\right)V $$.\n",
    "\n",
    "## Positional Encoding\n",
    "The key and query matrices allow the data to interact with each other bidirectionally and asymetrically (one element can pay more attention to other elements than those elements pay to it: think of a fan that dotes on a celebrity). However, this comes at the cost of losing sequence information as the indexes are not encoded in the learning protocol. The ordering is usually extremely important for sequentially organised data and we do not want to through out this ordering. To incorporate it we augment our input vectors with a number that indicates the relative ordering. This is commonly chosen to be a sinusoid (but this choice is abitrary). The encoding is typically:\n",
    "\n",
    "$$ p_d(x, i) = \\sin \\left( \\frac{{x^{\\frac{2i}{d}}}}{10000}  \\right) $$\n",
    "\n",
    "This encoding is acting as another embedding in the same fashion as the keys and queries matrices. This, in principle, could also be learned. However, we choose a fixed function because in practice the learned results do not vastly outperform a fixed embedding and they come at a larger cost.\n",
    "\n",
    "## Attention Heads\n",
    "\n",
    "The attention operation in conjunction with the positional encoding form an *attention head*. This is nothing more than a learned representation of what and where each element in the sequence should be pay attention to. It is certainly possible that different sub-sequences mean different things to one and other in different contexts. This can be represented with multiple attention heads operating on the same inputs. As an analogy, think of the different features that can be extracted by having multiple convolutional kernels in each layer. Multihead attention is exteremely easy to implement. Simply concatentate the key/query matrices together linearly. In a similar analogy to convolutional networks attention heads may be composed in layers. The outputs of an attention head are a vector of weighted attention cues. These can be passed to a new set of attention matrices. In doing so we can learn deep and convoluted relationships. We are now ready to create our custom attention head and multiattention head types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b617a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Attention\n",
    "    Q::Matrix\n",
    "    K::Matrix\n",
    "    d::Int\n",
    "    function Attention(sequencedimension, embeddingdimension)\n",
    "        Q = rand(embeddingdimension, sequencedimension)\n",
    "        K = rand(sequencedimension, embeddingdimension)\n",
    "        new(Q, K, sequencedimension)\n",
    "    end\n",
    "end\n",
    "\n",
    "struct MultiHeadAttention\n",
    "    Q::Matrix\n",
    "    K::Matrix\n",
    "    d::Int\n",
    "    function MultiHeadAttention(attentionheads...)\n",
    "        Q = hcat(attentionheads.Q...)\n",
    "        K = vcat(attentionheads.K...)\n",
    "        d = attentionheads[1].d\n",
    "        new(Q, K, d)\n",
    "    end\n",
    "end\n",
    "\n",
    "(a::Union{Attention, MultiHeadAttention})(V) = Flux.softmax(a.K * a.Q ./ sqrt(d)) .* V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372fbb6",
   "metadata": {},
   "source": [
    "# The Transformer Architecture\n",
    "The Transformer uses attention as its principle working mechanism but it itself is a neural network architecture. It is composed of an encoder and a decoder. Each of these has several layers of multi-head attention and in-between the layers are Dense perceptron layers. This allows the vectors to be appropriately transformed/classified at each step. The encoder component is relatively straightforward - feed it the input vectors and record the final output. The decoder layer is slightly more sophisticated.\n",
    "\n",
    "A single pass of the decoder proceeds as follows: take a new input (not one of the encoder inputs) and pass it through an attention layer. This is the self-attention layer of the decoder. For every subsequent attention layer the encoder outputs will be combined with the decoder pass. These will go through multi-head attention layers, be normalised, and then passed through a Dense perceptron layer as in the encoder. \n",
    "\n",
    "[IMG Transfomer]\n",
    "\n",
    "\n",
    "## Output Decoding\n",
    "\n",
    "To generate the output sequence the Transformer proceeds in much the same way as a traditional sequence decoder. It takes a `start` symbol and generates data sequentially until a `stop` symbol is generated. The outputs are encoded in an arbitrarily long vector that is expected to be longer than the length of the output sentence e.g. 512. The output layer is decoded using outputs generated up until the point of the sequence as inputs with a mask of `-Inf` for future ouputs. \n",
    "\n",
    "The masked output is used to generate its own query values and is combined with a positional encoding in the same fashion as the input layer. Then it is generally passed through its own self attention layer with masked multi-head attention before interacting with the encoded keys and values. The queries are combined with the encoder results to generate the next sequential output. This is autoregression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f300639",
   "metadata": {},
   "source": [
    "## Implementing the Transformer\n",
    "\n",
    "The Transformer is an inherently simple architecture and it is fairly straightforward to take a custom type as defined before and proceed through Zygote backpropogation and Flux API calls to complete training. However, as always, implementing this will require uninformative boiler-plate code. We have covered how one might implement custom architectures through custom layer types in previous notebooks and these can be used for reference. In this implementation we will use an existing package in the Flux ecosystem: `Transformers.jl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b20133",
   "metadata": {},
   "source": [
    "## Acid-Amino Sequence Prediction: Learning the Language of Codons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327fa97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: method definition for CuArray at /Users/nicholas_gale/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable A but does not use it.\n",
      "WARNING: method definition for CuArray at /Users/nicholas_gale/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable N+1 but does not use it.\n",
      "WARNING: method definition for CuArray at /Users/nicholas_gale/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable K but does not use it.\n"
     ]
    }
   ],
   "source": [
    "using CSV, DataFrames, Transformers, Flux, Random, ProgressMeter\n",
    "\n",
    "amino_codon = Dict( # the amino acid to codon relationship\n",
    "    \"A\" => [\"GCU\", \"GCC\", \"GCA\", \"GCG\"],\n",
    "    \"R\" => [\"CGU\", \"CGC\", \"CGA\", \"CGG\", \"AGA\", \"AGG\"],\n",
    "    \"N\" => [\"AAU\", \"AAC\"],\n",
    "    \"D\" => [\"GAU\", \"GAC\"],\n",
    "    \"B\" => [\"AAU\", \"AAC\", \"GAU\", \"GAC\"],\n",
    "    \"Q\" => [\"CAA\", \"CAG\"],\n",
    "    \"E\" => [\"GAA\", \"GAG\"],\n",
    "    \"Z\" => [\"CAA\", \"CAG\", \"GAA\", \"GAG\"],\n",
    "    \"G\" => [\"GGU\", \"GGC\", \"GGA\", \"GGG\"],\n",
    "    \"H\" => [\"CAU\", \"CAC\"],\n",
    "    \"I\" => [\"AUU\", \"AUC\", \"AUA\"],\n",
    "    \"L\" => [\"CUU\", \"CUC\", \"CUA\", \"CUG\", \"UUA\", \"UUG\"],\n",
    "    \"K\" => [\"AAA\", \"AAG\"],\n",
    "    \"M\" => [\"AUG\"],\n",
    "    \"F\" => [\"UUU\", \"UUC\"],\n",
    "    \"P\" => [\"CCU\", \"CCC\", \"CCA\", \"CCG\"],\n",
    "    \"S\" => [\"UCU\", \"UCC\", \"UCA\", \"UCG\", \"AGU\", \"AGC\"],\n",
    "    \"T\" => [\"ACU\", \"ACC\", \"ACA\", \"ACG\"],\n",
    "    \"W\" => [\"UGG\"],\n",
    "    \"Y\" => [\"UAU\", \"UAC\"],\n",
    "    \"V\" => [\"GUU\", \"GUC\", \"GUA\", \"GUG\"],\n",
    "    #\"1\" => [\"AUG\"],\n",
    "    #\"9\" => [\"UAA\", \"UGA\", \"UAG\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "122b97af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"./data/codon.csv\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminos = [rand(collect(keys(amino_codon)), rand(40:60)) for i in 1:1]\n",
    "create_genome(v) = prod(map(x->rand(amino_codon[x]), v))\n",
    "genomes = [create_genome(v) for v in aminos]\n",
    "df = DataFrame(acids=prod.(aminos), genomes=genomes)\n",
    "CSV.write(\"./data/codon.csv\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1be02c",
   "metadata": {},
   "source": [
    "Let's import and inspect the data. They are in the form of strings of letters. Indexing a single element of a string returns a `Char` character type but we want the tokens to be in form of strings. Let's create a processing function `string_split` to do this for us and create test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "40bd63e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ANMKSQHKAY\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"GCAAACAUGA\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the data\n",
    "string_split(v) = map(x -> string(x), collect(v))\n",
    "df = CSV.File(\"./data/codon.csv\") |> DataFrame;\n",
    "display(df[1,:acids][1:10])\n",
    "display(df[1,:genomes][1:10])\n",
    "\n",
    "dl = length(df[!, :acids])\n",
    "ds = round(Int, 0.8 * dl)\n",
    "\n",
    "train_x = string_split.(df[1:ds, :genomes]);\n",
    "train_y = string_split.(df[1:ds, :acids]);\n",
    "test_x = string_split.(df[ds+1:end, :genomes]);\n",
    "test_y = string_split.(df[ds+1:end, :acids]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a340a0",
   "metadata": {},
   "source": [
    "It's time to start constructing the Transfomer. First, create a convenience function to append start and stop tokens to a vector. Then, create a vocabulary of available tokens. Finally, encode the data with the tokenizer and use these tokens to embed tokens into a space of dimension 64. These will be augmented with positional embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "06ed219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisers\n",
    "pre_process(v) = cat(\"1\", v..., \"9\"; dims=1)\n",
    "labels_y = cat(\"1\", unique(train_y[1])..., \"9\", \"0\"; dims=1)\n",
    "labels_x = cat(\"1\", unique(train_x[1])..., \"9\", \"0\"; dims=1)\n",
    "tokenizer_x = Transformers.Vocabulary(labels_x, \"0\")\n",
    "tokenizer_y = Transformers.Vocabulary(labels_y, \"0\")\n",
    "encoded_x = tokenizer_x.(pre_process.(train_x))\n",
    "encoded_y = tokenizer_y.(pre_process.(train_y));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db202d99",
   "metadata": {},
   "source": [
    "Create a simple embedding function. We will choose the dimensionality to be 64 to keep the model relatively small. The transfomer encoder will be composed of three blocks each with 4 heads and and inner dimensionality of 128. These are much smaller than the original model. The decoder will be similarly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "942af38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 64\n",
    "h = 8\n",
    "hd = 8\n",
    "innerd = 256\n",
    "\n",
    "embed_x = Transformers.Embed(d, length(tokenizer_x))\n",
    "embed_y = Transformers.Embed(d, length(tokenizer_y))\n",
    "position_embed = Transformers.PositionEmbedding(d)\n",
    "\n",
    "\n",
    "function embeddingx(x)\n",
    "  sem_em = embed_x(x, inv(sqrt(d)))\n",
    "  em = sem_em .+ position_embed(sem_em)\n",
    "  return em\n",
    "end\n",
    "\n",
    "Tencoder = Flux.Chain(embeddingx, Transformer(d, h, hd, innerd), Transformer(d, h, hd, innerd), Transformer(d, h, hd, innerd))\n",
    "\n",
    "function embeddingy(y)\n",
    "  sem_em = embed_y(y, inv(sqrt(d)))\n",
    "  em = sem_em .+ position_embed(sem_em)\n",
    "  return em\n",
    "end\n",
    "\n",
    "Dec1 = TransformerDecoder(d, h, hd, innerd)\n",
    "Dec2 = TransformerDecoder(d, h, hd, innerd)\n",
    "Dec3 = TransformerDecoder(d, h, hd, innerd)\n",
    "ffn = Transformers.Positionwise(Dense(d, length(tokenizer_y)), softmax)\n",
    "\n",
    "function Tdecoder((y, mx))\n",
    "    emy = embeddingy(y)\n",
    "    d1 = Dec1(emy, mx)\n",
    "    d2 = Dec1(d1, mx)\n",
    "    d3 = Dec1(d2, mx)\n",
    "    ffn(d3)\n",
    "end\n",
    "    \n",
    "ps = Flux.params(Tencoder, Dec1, Dec2, Dec3, ffn);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4639c92",
   "metadata": {},
   "source": [
    "Let's now define our loss. Our final layer is the `softmax` layer converting the tokens into probabilities for the tokens in the output space. It makes sense to `onehot` encode the outputs and perform label smoothing to convert these outputs to a probability distribution. Then, an appropriate loss function is `crossentropy` a measure of the similarity between two probability distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "87542b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(xdata, ydata)\n",
    "    L = 0\n",
    "    for i in 1:length(ydata)\n",
    "        ytarget = Flux.label_smoothing(Flux.onehot(tokenizer_y, ydata[i]), 0.2f0)\n",
    "        ypred = Tdecoder((ydata[i], Tencoder(xdata[i])))\n",
    "        L += Flux.crossentropy(ytarget, ypred)\n",
    "    end\n",
    "    return L\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d01fd65a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip890\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip890)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip891\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip890)\" d=\"\n",
       "M112.177 1486.45 L2352.76 1486.45 L2352.76 47.2441 L112.177 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip892\">\n",
       "    <rect x=\"112\" y=\"47\" width=\"2242\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  171.354,1486.45 171.354,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  594.952,1486.45 594.952,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1018.55,1486.45 1018.55,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1442.15,1486.45 1442.15,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1865.75,1486.45 1865.75,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2289.34,1486.45 2289.34,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  171.354,1486.45 171.354,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  594.952,1486.45 594.952,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1018.55,1486.45 1018.55,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1442.15,1486.45 1442.15,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1865.75,1486.45 1865.75,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2289.34,1486.45 2289.34,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip890)\" d=\"M171.354 1517.37 Q167.743 1517.37 165.914 1520.93 Q164.108 1524.47 164.108 1531.6 Q164.108 1538.71 165.914 1542.27 Q167.743 1545.82 171.354 1545.82 Q174.988 1545.82 176.794 1542.27 Q178.622 1538.71 178.622 1531.6 Q178.622 1524.47 176.794 1520.93 Q174.988 1517.37 171.354 1517.37 M171.354 1513.66 Q177.164 1513.66 180.219 1518.27 Q183.298 1522.85 183.298 1531.6 Q183.298 1540.33 180.219 1544.94 Q177.164 1549.52 171.354 1549.52 Q165.544 1549.52 162.465 1544.94 Q159.409 1540.33 159.409 1531.6 Q159.409 1522.85 162.465 1518.27 Q165.544 1513.66 171.354 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M554.558 1544.91 L562.197 1544.91 L562.197 1518.55 L553.887 1520.21 L553.887 1515.95 L562.151 1514.29 L566.827 1514.29 L566.827 1544.91 L574.466 1544.91 L574.466 1548.85 L554.558 1548.85 L554.558 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M593.91 1517.37 Q590.299 1517.37 588.47 1520.93 Q586.665 1524.47 586.665 1531.6 Q586.665 1538.71 588.47 1542.27 Q590.299 1545.82 593.91 1545.82 Q597.544 1545.82 599.35 1542.27 Q601.179 1538.71 601.179 1531.6 Q601.179 1524.47 599.35 1520.93 Q597.544 1517.37 593.91 1517.37 M593.91 1513.66 Q599.72 1513.66 602.776 1518.27 Q605.854 1522.85 605.854 1531.6 Q605.854 1540.33 602.776 1544.94 Q599.72 1549.52 593.91 1549.52 Q588.1 1549.52 585.021 1544.94 Q581.966 1540.33 581.966 1531.6 Q581.966 1522.85 585.021 1518.27 Q588.1 1513.66 593.91 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M624.072 1517.37 Q620.461 1517.37 618.632 1520.93 Q616.827 1524.47 616.827 1531.6 Q616.827 1538.71 618.632 1542.27 Q620.461 1545.82 624.072 1545.82 Q627.706 1545.82 629.512 1542.27 Q631.34 1538.71 631.34 1531.6 Q631.34 1524.47 629.512 1520.93 Q627.706 1517.37 624.072 1517.37 M624.072 1513.66 Q629.882 1513.66 632.938 1518.27 Q636.016 1522.85 636.016 1531.6 Q636.016 1540.33 632.938 1544.94 Q629.882 1549.52 624.072 1549.52 Q618.262 1549.52 615.183 1544.94 Q612.128 1540.33 612.128 1531.6 Q612.128 1522.85 615.183 1518.27 Q618.262 1513.66 624.072 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M982.242 1544.91 L998.561 1544.91 L998.561 1548.85 L976.617 1548.85 L976.617 1544.91 Q979.279 1542.16 983.862 1537.53 Q988.469 1532.88 989.649 1531.53 Q991.895 1529.01 992.774 1527.27 Q993.677 1525.51 993.677 1523.82 Q993.677 1521.07 991.733 1519.33 Q989.811 1517.6 986.709 1517.6 Q984.51 1517.6 982.057 1518.36 Q979.626 1519.13 976.848 1520.68 L976.848 1515.95 Q979.672 1514.82 982.126 1514.24 Q984.58 1513.66 986.617 1513.66 Q991.987 1513.66 995.182 1516.35 Q998.376 1519.03 998.376 1523.52 Q998.376 1525.65 997.566 1527.57 Q996.779 1529.47 994.672 1532.07 Q994.094 1532.74 990.992 1535.95 Q987.89 1539.15 982.242 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1018.38 1517.37 Q1014.76 1517.37 1012.94 1520.93 Q1011.13 1524.47 1011.13 1531.6 Q1011.13 1538.71 1012.94 1542.27 Q1014.76 1545.82 1018.38 1545.82 Q1022.01 1545.82 1023.82 1542.27 Q1025.64 1538.71 1025.64 1531.6 Q1025.64 1524.47 1023.82 1520.93 Q1022.01 1517.37 1018.38 1517.37 M1018.38 1513.66 Q1024.19 1513.66 1027.24 1518.27 Q1030.32 1522.85 1030.32 1531.6 Q1030.32 1540.33 1027.24 1544.94 Q1024.19 1549.52 1018.38 1549.52 Q1012.57 1549.52 1009.49 1544.94 Q1006.43 1540.33 1006.43 1531.6 Q1006.43 1522.85 1009.49 1518.27 Q1012.57 1513.66 1018.38 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1048.54 1517.37 Q1044.93 1517.37 1043.1 1520.93 Q1041.29 1524.47 1041.29 1531.6 Q1041.29 1538.71 1043.1 1542.27 Q1044.93 1545.82 1048.54 1545.82 Q1052.17 1545.82 1053.98 1542.27 Q1055.81 1538.71 1055.81 1531.6 Q1055.81 1524.47 1053.98 1520.93 Q1052.17 1517.37 1048.54 1517.37 M1048.54 1513.66 Q1054.35 1513.66 1057.4 1518.27 Q1060.48 1522.85 1060.48 1531.6 Q1060.48 1540.33 1057.4 1544.94 Q1054.35 1549.52 1048.54 1549.52 Q1042.73 1549.52 1039.65 1544.94 Q1036.59 1540.33 1036.59 1531.6 Q1036.59 1522.85 1039.65 1518.27 Q1042.73 1513.66 1048.54 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1415.91 1530.21 Q1419.27 1530.93 1421.14 1533.2 Q1423.04 1535.47 1423.04 1538.8 Q1423.04 1543.92 1419.52 1546.72 Q1416 1549.52 1409.52 1549.52 Q1407.34 1549.52 1405.03 1549.08 Q1402.74 1548.66 1400.28 1547.81 L1400.28 1543.29 Q1402.23 1544.43 1404.54 1545.01 Q1406.86 1545.58 1409.38 1545.58 Q1413.78 1545.58 1416.07 1543.85 Q1418.39 1542.11 1418.39 1538.8 Q1418.39 1535.75 1416.23 1534.03 Q1414.1 1532.3 1410.28 1532.3 L1406.26 1532.3 L1406.26 1528.45 L1410.47 1528.45 Q1413.92 1528.45 1415.75 1527.09 Q1417.58 1525.7 1417.58 1523.11 Q1417.58 1520.45 1415.68 1519.03 Q1413.8 1517.6 1410.28 1517.6 Q1408.36 1517.6 1406.16 1518.01 Q1403.96 1518.43 1401.33 1519.31 L1401.33 1515.14 Q1403.99 1514.4 1406.3 1514.03 Q1408.64 1513.66 1410.7 1513.66 Q1416.02 1513.66 1419.13 1516.09 Q1422.23 1518.5 1422.23 1522.62 Q1422.23 1525.49 1420.59 1527.48 Q1418.94 1529.45 1415.91 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1441.9 1517.37 Q1438.29 1517.37 1436.46 1520.93 Q1434.66 1524.47 1434.66 1531.6 Q1434.66 1538.71 1436.46 1542.27 Q1438.29 1545.82 1441.9 1545.82 Q1445.54 1545.82 1447.34 1542.27 Q1449.17 1538.71 1449.17 1531.6 Q1449.17 1524.47 1447.34 1520.93 Q1445.54 1517.37 1441.9 1517.37 M1441.9 1513.66 Q1447.71 1513.66 1450.77 1518.27 Q1453.85 1522.85 1453.85 1531.6 Q1453.85 1540.33 1450.77 1544.94 Q1447.71 1549.52 1441.9 1549.52 Q1436.09 1549.52 1433.02 1544.94 Q1429.96 1540.33 1429.96 1531.6 Q1429.96 1522.85 1433.02 1518.27 Q1436.09 1513.66 1441.9 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1472.07 1517.37 Q1468.46 1517.37 1466.63 1520.93 Q1464.82 1524.47 1464.82 1531.6 Q1464.82 1538.71 1466.63 1542.27 Q1468.46 1545.82 1472.07 1545.82 Q1475.7 1545.82 1477.51 1542.27 Q1479.33 1538.71 1479.33 1531.6 Q1479.33 1524.47 1477.51 1520.93 Q1475.7 1517.37 1472.07 1517.37 M1472.07 1513.66 Q1477.88 1513.66 1480.93 1518.27 Q1484.01 1522.85 1484.01 1531.6 Q1484.01 1540.33 1480.93 1544.94 Q1477.88 1549.52 1472.07 1549.52 Q1466.26 1549.52 1463.18 1544.94 Q1460.12 1540.33 1460.12 1531.6 Q1460.12 1522.85 1463.18 1518.27 Q1466.26 1513.66 1472.07 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1838.84 1518.36 L1827.03 1536.81 L1838.84 1536.81 L1838.84 1518.36 M1837.61 1514.29 L1843.49 1514.29 L1843.49 1536.81 L1848.42 1536.81 L1848.42 1540.7 L1843.49 1540.7 L1843.49 1548.85 L1838.84 1548.85 L1838.84 1540.7 L1823.23 1540.7 L1823.23 1536.19 L1837.61 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1866.15 1517.37 Q1862.54 1517.37 1860.71 1520.93 Q1858.91 1524.47 1858.91 1531.6 Q1858.91 1538.71 1860.71 1542.27 Q1862.54 1545.82 1866.15 1545.82 Q1869.78 1545.82 1871.59 1542.27 Q1873.42 1538.71 1873.42 1531.6 Q1873.42 1524.47 1871.59 1520.93 Q1869.78 1517.37 1866.15 1517.37 M1866.15 1513.66 Q1871.96 1513.66 1875.02 1518.27 Q1878.09 1522.85 1878.09 1531.6 Q1878.09 1540.33 1875.02 1544.94 Q1871.96 1549.52 1866.15 1549.52 Q1860.34 1549.52 1857.26 1544.94 Q1854.21 1540.33 1854.21 1531.6 Q1854.21 1522.85 1857.26 1518.27 Q1860.34 1513.66 1866.15 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M1896.31 1517.37 Q1892.7 1517.37 1890.87 1520.93 Q1889.07 1524.47 1889.07 1531.6 Q1889.07 1538.71 1890.87 1542.27 Q1892.7 1545.82 1896.31 1545.82 Q1899.95 1545.82 1901.75 1542.27 Q1903.58 1538.71 1903.58 1531.6 Q1903.58 1524.47 1901.75 1520.93 Q1899.95 1517.37 1896.31 1517.37 M1896.31 1513.66 Q1902.12 1513.66 1905.18 1518.27 Q1908.26 1522.85 1908.26 1531.6 Q1908.26 1540.33 1905.18 1544.94 Q1902.12 1549.52 1896.31 1549.52 Q1890.5 1549.52 1887.42 1544.94 Q1884.37 1540.33 1884.37 1531.6 Q1884.37 1522.85 1887.42 1518.27 Q1890.5 1513.66 1896.31 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2248.96 1514.29 L2267.32 1514.29 L2267.32 1518.22 L2253.24 1518.22 L2253.24 1526.7 Q2254.26 1526.35 2255.28 1526.19 Q2256.3 1526 2257.32 1526 Q2263.11 1526 2266.48 1529.17 Q2269.86 1532.34 2269.86 1537.76 Q2269.86 1543.34 2266.39 1546.44 Q2262.92 1549.52 2256.6 1549.52 Q2254.42 1549.52 2252.16 1549.15 Q2249.91 1548.78 2247.5 1548.04 L2247.5 1543.34 Q2249.59 1544.47 2251.81 1545.03 Q2254.03 1545.58 2256.51 1545.58 Q2260.51 1545.58 2262.85 1543.48 Q2265.19 1541.37 2265.19 1537.76 Q2265.19 1534.15 2262.85 1532.04 Q2260.51 1529.94 2256.51 1529.94 Q2254.63 1529.94 2252.76 1530.35 Q2250.91 1530.77 2248.96 1531.65 L2248.96 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2289.08 1517.37 Q2285.47 1517.37 2283.64 1520.93 Q2281.83 1524.47 2281.83 1531.6 Q2281.83 1538.71 2283.64 1542.27 Q2285.47 1545.82 2289.08 1545.82 Q2292.71 1545.82 2294.52 1542.27 Q2296.35 1538.71 2296.35 1531.6 Q2296.35 1524.47 2294.52 1520.93 Q2292.71 1517.37 2289.08 1517.37 M2289.08 1513.66 Q2294.89 1513.66 2297.94 1518.27 Q2301.02 1522.85 2301.02 1531.6 Q2301.02 1540.33 2297.94 1544.94 Q2294.89 1549.52 2289.08 1549.52 Q2283.27 1549.52 2280.19 1544.94 Q2277.13 1540.33 2277.13 1531.6 Q2277.13 1522.85 2280.19 1518.27 Q2283.27 1513.66 2289.08 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2319.24 1517.37 Q2315.63 1517.37 2313.8 1520.93 Q2311.99 1524.47 2311.99 1531.6 Q2311.99 1538.71 2313.8 1542.27 Q2315.63 1545.82 2319.24 1545.82 Q2322.87 1545.82 2324.68 1542.27 Q2326.51 1538.71 2326.51 1531.6 Q2326.51 1524.47 2324.68 1520.93 Q2322.87 1517.37 2319.24 1517.37 M2319.24 1513.66 Q2325.05 1513.66 2328.1 1518.27 Q2331.18 1522.85 2331.18 1531.6 Q2331.18 1540.33 2328.1 1544.94 Q2325.05 1549.52 2319.24 1549.52 Q2313.43 1549.52 2310.35 1544.94 Q2307.29 1540.33 2307.29 1531.6 Q2307.29 1522.85 2310.35 1518.27 Q2313.43 1513.66 2319.24 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,1249.66 2352.76,1249.66 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,916.884 2352.76,916.884 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,584.103 2352.76,584.103 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip892)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,251.323 2352.76,251.323 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1486.45 112.177,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1249.66 131.075,1249.66 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,916.884 131.075,916.884 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,584.103 131.075,584.103 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,251.323 131.075,251.323 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip890)\" d=\"M56.2699 1263.01 L63.9087 1263.01 L63.9087 1236.64 L55.5986 1238.31 L55.5986 1234.05 L63.8624 1232.38 L68.5383 1232.38 L68.5383 1263.01 L76.1772 1263.01 L76.1772 1266.94 L56.2699 1266.94 L56.2699 1263.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M59.8578 930.229 L76.1772 930.229 L76.1772 934.164 L54.2328 934.164 L54.2328 930.229 Q56.8949 927.474 61.4782 922.845 Q66.0846 918.192 67.2652 916.849 Q69.5105 914.326 70.3902 912.59 Q71.2929 910.831 71.2929 909.141 Q71.2929 906.386 69.3485 904.65 Q67.4272 902.914 64.3254 902.914 Q62.1263 902.914 59.6726 903.678 Q57.2421 904.442 54.4643 905.993 L54.4643 901.271 Q57.2884 900.136 59.7421 899.558 Q62.1958 898.979 64.2328 898.979 Q69.6031 898.979 72.7976 901.664 Q75.992 904.349 75.992 908.84 Q75.992 910.97 75.1818 912.891 Q74.3948 914.789 72.2883 917.382 Q71.7096 918.053 68.6078 921.271 Q65.5059 924.465 59.8578 930.229 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M69.0476 582.749 Q72.404 583.467 74.279 585.735 Q76.1772 588.004 76.1772 591.337 Q76.1772 596.453 72.6587 599.254 Q69.1402 602.055 62.6587 602.055 Q60.4828 602.055 58.168 601.615 Q55.8764 601.198 53.4227 600.342 L53.4227 595.828 Q55.3671 596.962 57.6819 597.541 Q59.9967 598.119 62.5198 598.119 Q66.918 598.119 69.2096 596.383 Q71.5244 594.647 71.5244 591.337 Q71.5244 588.282 69.3717 586.569 Q67.242 584.833 63.4226 584.833 L59.3949 584.833 L59.3949 580.99 L63.6078 580.99 Q67.0569 580.99 68.8855 579.624 Q70.7142 578.235 70.7142 575.643 Q70.7142 572.981 68.8161 571.569 Q66.9411 570.134 63.4226 570.134 Q61.5013 570.134 59.3023 570.55 Q57.1032 570.967 54.4643 571.846 L54.4643 567.68 Q57.1264 566.939 59.4412 566.569 Q61.7791 566.198 63.8393 566.198 Q69.1633 566.198 72.2652 568.629 Q75.367 571.036 75.367 575.157 Q75.367 578.027 73.7235 580.018 Q72.08 581.985 69.0476 582.749 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M66.5939 238.117 L54.7884 256.566 L66.5939 256.566 L66.5939 238.117 M65.367 234.043 L71.2466 234.043 L71.2466 256.566 L76.1772 256.566 L76.1772 260.455 L71.2466 260.455 L71.2466 268.603 L66.5939 268.603 L66.5939 260.455 L50.9921 260.455 L50.9921 255.941 L65.367 234.043 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip892)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.59,87.9763 179.826,150.352 184.062,206.048 188.298,213.961 192.534,214.95 196.77,217.251 201.006,217.976 205.242,233.089 209.478,243.011 213.714,245.622 \n",
       "  217.95,248.848 222.186,258.014 226.422,250.117 230.658,265.627 234.893,251.243 239.129,259.508 243.365,252.707 247.601,270.615 251.837,256.724 256.073,291.409 \n",
       "  260.309,268.419 264.545,303.853 268.781,315.659 273.017,311.65 277.253,323.367 281.489,321.556 285.725,253.092 289.961,330.102 294.197,310.194 298.433,324.031 \n",
       "  302.669,344.272 306.905,355.415 311.141,310.402 315.377,374.639 319.613,374.94 323.849,348.241 328.085,356.119 332.321,347.243 336.557,374.069 340.793,351.596 \n",
       "  345.029,380.929 349.265,379.231 353.501,381.183 357.737,381.024 361.973,378.346 366.209,381.86 370.445,381.52 374.681,381.584 378.917,382.573 383.153,382.335 \n",
       "  387.389,381.653 391.625,383.185 395.861,384.043 400.097,391.473 404.333,375.471 408.569,409.023 412.805,285.437 417.041,414.268 421.277,387.116 425.513,409.307 \n",
       "  429.749,422.486 433.985,413.696 438.22,440.5 442.456,390.69 446.692,424.596 450.928,429.67 455.164,488.204 459.4,426.716 463.636,448.444 467.872,410.58 \n",
       "  472.108,446.956 476.344,431.498 480.58,369.722 484.816,446.892 489.052,405.381 493.288,448.605 497.524,417.7 501.76,447.851 505.996,429.614 510.232,436.187 \n",
       "  514.468,470.913 518.704,420.705 522.94,451.247 527.176,460.758 531.412,471.861 535.648,447.532 539.884,479.359 544.12,484.259 548.356,479.296 552.592,469.899 \n",
       "  556.828,479.588 561.064,509.516 565.3,484.693 569.536,491.454 573.772,478.383 578.008,511.071 582.244,471.783 586.48,508.53 590.716,512.502 594.952,486.686 \n",
       "  599.188,529.437 603.424,507.252 607.66,504.833 611.896,507.8 616.132,536.944 620.368,503.497 624.604,520.679 628.84,527.415 633.076,560.991 637.311,525.378 \n",
       "  641.547,557.01 645.783,549.9 650.019,569.463 654.255,553.79 658.491,561.95 662.727,571.977 666.963,551.781 671.199,573.128 675.435,541.531 679.671,577.385 \n",
       "  683.907,580.529 688.143,577.672 692.379,554.85 696.615,566.617 700.851,572.48 705.087,579.324 709.323,601.423 713.559,599.675 717.795,597.927 722.031,590.53 \n",
       "  726.267,597.225 730.503,606.297 734.739,589.65 738.975,515.922 743.211,611.707 747.447,615.8 751.683,627.484 755.919,619.012 760.155,602.181 764.391,654.033 \n",
       "  768.627,614.397 772.863,570.488 777.099,656.036 781.335,621.484 785.571,645.643 789.807,665.81 794.043,670.232 798.279,657.57 802.515,679.415 806.751,654.615 \n",
       "  810.987,657.006 815.223,691.05 819.459,660.53 823.695,706.463 827.931,698.53 832.167,708.993 836.403,698.433 840.638,733.316 844.874,729.37 849.11,702.819 \n",
       "  853.346,655.492 857.582,738.436 861.818,707.543 866.054,775.438 870.29,767.993 874.526,786.076 878.762,770.788 882.998,748.58 887.234,792.356 891.47,818.698 \n",
       "  895.706,816.796 899.942,805.24 904.178,836.947 908.414,814.119 912.65,854.99 916.886,794.546 921.122,862.392 925.358,859.2 929.594,876.833 933.83,894.752 \n",
       "  938.066,887.826 942.302,892.864 946.538,898.145 950.774,847.572 955.01,917.664 959.246,877.062 963.482,943.909 967.718,924.051 971.954,918.768 976.19,899.642 \n",
       "  980.426,935.187 984.662,876.949 988.898,945.016 993.134,925.279 997.37,913.846 1001.61,959.866 1005.84,931.618 1010.08,957.193 1014.31,937.167 1018.55,956.593 \n",
       "  1022.79,959.793 1027.02,981.051 1031.26,983.091 1035.49,966.279 1039.73,990.471 1043.97,990.504 1048.2,1012.84 1052.44,980.192 1056.67,1018.18 1060.91,1006.36 \n",
       "  1065.15,1032.3 1069.38,1010.45 1073.62,1045.5 1077.85,1018.94 1082.09,1050.36 1086.33,1036.19 1090.56,1053.91 1094.8,1029.15 1099.03,1043.8 1103.27,1058.47 \n",
       "  1107.51,1011.88 1111.74,1058.75 1115.98,1045.7 1120.21,1048.63 1124.45,1037.48 1128.69,1082.53 1132.92,1029.03 1137.16,1061.04 1141.39,1063.99 1145.63,1066.55 \n",
       "  1149.86,1074.09 1154.1,1069.66 1158.34,1070.71 1162.57,1090.91 1166.81,1048.86 1171.04,1092.06 1175.28,1038.46 1179.52,1088.09 1183.75,1055.03 1187.99,1079.76 \n",
       "  1192.22,1069.61 1196.46,1082.02 1200.7,1085.06 1204.93,1099.33 1209.17,1090.55 1213.4,1078.56 1217.64,1087.89 1221.88,1109.87 1226.11,1098.6 1230.35,1094.12 \n",
       "  1234.58,1113.72 1238.82,1093.83 1243.06,1128.32 1247.29,1099.6 1251.53,1119 1255.76,1096.79 1260,1129.68 1264.24,1130.9 1268.47,1113.8 1272.71,1108.25 \n",
       "  1276.94,1083.4 1281.18,1117.88 1285.42,1117.3 1289.65,1139.49 1293.89,1115.65 1298.12,1138.56 1302.36,1107.55 1306.6,1142.9 1310.83,1134.15 1315.07,1145.16 \n",
       "  1319.3,1136.57 1323.54,1107.11 1327.78,1143.32 1332.01,1142.51 1336.25,1136.93 1340.48,1120.47 1344.72,1142.54 1348.96,1141.58 1353.19,1144.29 1357.43,1137.12 \n",
       "  1361.66,1119.96 1365.9,1149.1 1370.14,1149.06 1374.37,1124.62 1378.61,1164.55 1382.84,1143.88 1387.08,1158.16 1391.32,1127.13 1395.55,1164.14 1399.79,1126.76 \n",
       "  1404.02,1173.94 1408.26,1146.3 1412.5,1176.26 1416.73,1168.71 1420.97,1152.43 1425.2,1191.34 1429.44,1141.46 1433.68,1189.82 1437.91,1180.76 1442.15,1207.05 \n",
       "  1446.38,1179.64 1450.62,1227.27 1454.86,1223.22 1459.09,1232.06 1463.33,1201.82 1467.56,1232.05 1471.8,1168.85 1476.04,1232.22 1480.27,1204.91 1484.51,1237.9 \n",
       "  1488.74,1188.89 1492.98,1240.72 1497.22,1214.45 1501.45,1240.84 1505.69,1239.47 1509.92,1213.17 1514.16,1234.49 1518.4,1238.83 1522.63,1212.94 1526.87,1232 \n",
       "  1531.1,1241.37 1535.34,1242.67 1539.58,1206.19 1543.81,1269.83 1548.05,1241.64 1552.28,1272.26 1556.52,1246.07 1560.75,1292.62 1564.99,1248.35 1569.23,1260.09 \n",
       "  1573.46,1267.68 1577.7,1302.26 1581.93,1297.07 1586.17,1310.6 1590.41,1262.26 1594.64,1281.53 1598.88,1280 1603.11,1286.29 1607.35,1311.71 1611.59,1300.17 \n",
       "  1615.82,1310.58 1620.06,1311.29 1624.29,1301.93 1628.53,1308.69 1632.77,1310.6 1637,1311.95 1641.24,1311.33 1645.47,1311.03 1649.71,1308.93 1653.95,1307.16 \n",
       "  1658.18,1312.07 1662.42,1311.98 1666.65,1312.1 1670.89,1312.28 1675.13,1296.39 1679.36,1312.73 1683.6,1282.11 1687.83,1312.67 1692.07,1314.54 1696.31,1312.94 \n",
       "  1700.54,1312.78 1704.78,1312.15 1709.01,1320.03 1713.25,1330.18 1717.49,1322.84 1721.72,1298.83 1725.96,1324.52 1730.19,1332.63 1734.43,1338.79 1738.67,1315.54 \n",
       "  1742.9,1331.93 1747.14,1339.6 1751.37,1312.57 1755.61,1331.35 1759.85,1341.21 1764.08,1337.35 1768.32,1346.3 1772.55,1334.31 1776.79,1354.78 1781.03,1327.92 \n",
       "  1785.26,1351.05 1789.5,1349.28 1793.73,1360.01 1797.97,1358.01 1802.21,1371.68 1806.44,1390.23 1810.68,1385.04 1814.91,1360.71 1819.15,1383.76 1823.39,1379.3 \n",
       "  1827.62,1370.31 1831.86,1407.79 1836.09,1403.14 1840.33,1383.26 1844.57,1397.39 1848.8,1393.05 1853.04,1408.49 1857.27,1388.84 1861.51,1377.81 1865.75,1407.69 \n",
       "  1869.98,1356.86 1874.22,1409.67 1878.45,1399.14 1882.69,1386.21 1886.93,1407.92 1891.16,1409.31 1895.4,1394.32 1899.63,1399.2 1903.87,1404.28 1908.11,1402 \n",
       "  1912.34,1406.32 1916.58,1396.78 1920.81,1401.58 1925.05,1407.02 1929.29,1392.05 1933.52,1410.69 1937.76,1397.97 1941.99,1403.22 1946.23,1410.43 1950.46,1396.01 \n",
       "  1954.7,1411.11 1958.94,1410.28 1963.17,1410.23 1967.41,1405.69 1971.64,1411.45 1975.88,1407.43 1980.12,1411.25 1984.35,1400.27 1988.59,1410.63 1992.82,1411.09 \n",
       "  1997.06,1397.78 2001.3,1410.53 2005.53,1403.57 2009.77,1412.02 2014,1411.73 2018.24,1411.2 2022.48,1409.81 2026.71,1410.74 2030.95,1411.86 2035.18,1385.59 \n",
       "  2039.42,1410.3 2043.66,1411.42 2047.89,1412.24 2052.13,1411.16 2056.36,1411.54 2060.6,1411.89 2064.84,1412.37 2069.07,1412.15 2073.31,1411.45 2077.54,1412.41 \n",
       "  2081.78,1412.47 2086.02,1412.06 2090.25,1412.4 2094.49,1412.27 2098.72,1412.43 2102.96,1420 2107.2,1341.41 2111.43,1434.93 2115.67,1409.81 2119.9,1414.3 \n",
       "  2124.14,1442.11 2128.38,1412.74 2132.61,1441.53 2136.85,1430.95 2141.08,1418.08 2145.32,1415.79 2149.56,1443.1 2153.79,1437.93 2158.03,1444.14 2162.26,1411.97 \n",
       "  2166.5,1444.13 2170.74,1403.29 2174.97,1445.32 2179.21,1444.56 2183.44,1409.93 2187.68,1445.49 2191.92,1444.34 2196.15,1444.94 2200.39,1441.31 2204.62,1445.34 \n",
       "  2208.86,1414.53 2213.1,1445.09 2217.33,1437.92 2221.57,1445.51 2225.8,1443.95 2230.04,1445.34 2234.28,1445.57 2238.51,1445.37 2242.75,1424.93 2246.98,1445.64 \n",
       "  2251.22,1445.65 2255.46,1445.09 2259.69,1445.24 2263.93,1445.2 2268.16,1445.04 2272.4,1431.87 2276.64,1438.87 2280.87,1445.72 2285.11,1445.39 2289.34,1445.7 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip890)\" d=\"\n",
       "M1957 198.898 L2278.07 198.898 L2278.07 95.2176 L1957 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1957,198.898 2278.07,198.898 2278.07,95.2176 1957,95.2176 1957,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip890)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1981.89,147.058 2131.27,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip890)\" d=\"M2156.16 129.778 L2160.84 129.778 L2160.84 160.402 L2177.67 160.402 L2177.67 164.338 L2156.16 164.338 L2156.16 129.778 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2191.6 141.398 Q2188.17 141.398 2186.18 144.083 Q2184.19 146.745 2184.19 151.398 Q2184.19 156.051 2186.16 158.736 Q2188.15 161.398 2191.6 161.398 Q2195 161.398 2196.99 158.713 Q2198.99 156.027 2198.99 151.398 Q2198.99 146.791 2196.99 144.106 Q2195 141.398 2191.6 141.398 M2191.6 137.787 Q2197.16 137.787 2200.33 141.398 Q2203.5 145.009 2203.5 151.398 Q2203.5 157.764 2200.33 161.398 Q2197.16 165.009 2191.6 165.009 Q2186.02 165.009 2182.85 161.398 Q2179.7 157.764 2179.7 151.398 Q2179.7 145.009 2182.85 141.398 Q2186.02 137.787 2191.6 137.787 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2227.09 139.176 L2227.09 143.203 Q2225.28 142.277 2223.34 141.815 Q2221.39 141.352 2219.31 141.352 Q2216.14 141.352 2214.54 142.324 Q2212.97 143.296 2212.97 145.24 Q2212.97 146.722 2214.1 147.578 Q2215.24 148.412 2218.66 149.176 L2220.12 149.5 Q2224.66 150.472 2226.55 152.254 Q2228.48 154.014 2228.48 157.185 Q2228.48 160.796 2225.61 162.902 Q2222.76 165.009 2217.76 165.009 Q2215.67 165.009 2213.41 164.592 Q2211.16 164.199 2208.66 163.388 L2208.66 158.99 Q2211.02 160.217 2213.31 160.842 Q2215.61 161.444 2217.85 161.444 Q2220.86 161.444 2222.48 160.426 Q2224.1 159.384 2224.1 157.509 Q2224.1 155.773 2222.92 154.847 Q2221.76 153.921 2217.8 153.064 L2216.32 152.717 Q2212.36 151.884 2210.61 150.171 Q2208.85 148.435 2208.85 145.426 Q2208.85 141.768 2211.44 139.778 Q2214.03 137.787 2218.8 137.787 Q2221.16 137.787 2223.24 138.134 Q2225.33 138.481 2227.09 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip890)\" d=\"M2251.79 139.176 L2251.79 143.203 Q2249.98 142.277 2248.04 141.815 Q2246.09 141.352 2244.01 141.352 Q2240.84 141.352 2239.24 142.324 Q2237.67 143.296 2237.67 145.24 Q2237.67 146.722 2238.8 147.578 Q2239.93 148.412 2243.36 149.176 L2244.82 149.5 Q2249.36 150.472 2251.25 152.254 Q2253.17 154.014 2253.17 157.185 Q2253.17 160.796 2250.3 162.902 Q2247.46 165.009 2242.46 165.009 Q2240.37 165.009 2238.11 164.592 Q2235.86 164.199 2233.36 163.388 L2233.36 158.99 Q2235.72 160.217 2238.01 160.842 Q2240.3 161.444 2242.55 161.444 Q2245.56 161.444 2247.18 160.426 Q2248.8 159.384 2248.8 157.509 Q2248.8 155.773 2247.62 154.847 Q2246.46 153.921 2242.5 153.064 L2241.02 152.717 Q2237.06 151.884 2235.3 150.171 Q2233.55 148.435 2233.55 145.426 Q2233.55 141.768 2236.14 139.778 Q2238.73 137.787 2243.5 137.787 Q2245.86 137.787 2247.94 138.134 Q2250.03 138.481 2251.79 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_data() = Flux.DataLoader((encoded_x, encoded_y); batchsize=1, shuffle=true)\n",
    "nepochs = 500\n",
    "L = []\n",
    "using Plots\n",
    "l = 0\n",
    "for e in 1:nepochs\n",
    "    data = epoch_data()\n",
    "    for dat in data\n",
    "        grad = gradient(ps) do\n",
    "            loss(dat...)            \n",
    "        end\n",
    "        push!(L, loss(dat...))\n",
    "        IJulia.clear_output(true)\n",
    "        Flux.update!(ADAM(0.001), ps, grad)\n",
    "        display(plot(L, label=\"Loss\"))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "008dd879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pointwise_accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function transcribe_protein(x)\n",
    "    seq = [tokenizer_y(\"1\")]\n",
    "    tok = [\"1\"]\n",
    "    enc = Tencoder(x)\n",
    "    for i = 1:2*length(x)\n",
    "        dec = Tdecoder((seq, enc))\n",
    "        seqnext = argmax(vec(dec[1:end-1,end]))\n",
    "        append!(seq, seqnext)\n",
    "        toknext = Transformers.decode(tokenizer_y, seqnext)\n",
    "        push!(tok, toknext)\n",
    "        toknext == \"9\" && break\n",
    "    end\n",
    "    tok\n",
    "end\n",
    "\n",
    "function pointwise_accuracy(xtrain, ytrain)\n",
    "    global_acc = 0\n",
    "    @showprogress for i in 1:length(xtrain)\n",
    "        xi = transcribe_protein(xtrain[i])\n",
    "        yi = ytrain[i]\n",
    "        \n",
    "\n",
    "#         diff = abs(length(xi) - length(yi))\n",
    "#         if length(xi) < length(yi)\n",
    "#             post = [\"uu\" for i in 1:diff]\n",
    "#             xi = cat(xi, post; dims=1)\n",
    "#         elseif length(yi) < length(xi)\n",
    "#             post = [\"uu\" for i in 1:diff]\n",
    "#             yi = cat(yi, post; dims=1)\n",
    "#         end\n",
    "        local_acc = sum( xi .== yi ) / length(yi)\n",
    "        global_acc += local_acc\n",
    "    end\n",
    "    return global_acc / length(xtrain)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "38228050",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Vector{String}}:\n",
       " [\"1\", \"1\", \"1\", \"M\", \"M\", \"M\", \"S\", \"S\", \"H\", \"H\"    \"T\", \"T\", \"T\", \"T\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"9\"]\n",
       " [\"1\", \"A\", \"N\", \"M\", \"K\", \"S\", \"Q\", \"H\", \"K\", \"A\"    \"H\", \"V\", \"K\", \"T\", \"D\", \"Y\", \"Y\", \"E\", \"Y\", \"9\"]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[transcribe_protein(encoded_x[1]), pre_process(train_y[1])]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4f1eedd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

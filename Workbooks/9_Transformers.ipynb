{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff57ecf",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Transformers are a sophisiticated modern neural network best adapted to sequential data. They are in the process of replacing the recurrent neural network as the de-facto model for sequence analysis. As a brief review: recurrent neural networks incorporate information about sequential dependencies through recurrent connections which allow a hidden state to incorporate variables that have been previously exposed to the network. There are some principle shortcomings with this approach: short memory, exploding/vanishing gradients, and $O(n)$ complexity. Tokens that are historically adjacent to each other have more weight and thus the information in the sequence decays over long ranges; the network has a relatively short memory. The hidden state is called recursively over the course of training and thus the gradients have a tendency to either explode or vanish and various regularisation tricks are needed to stablise the training routine. Finally, the current hidden state depends on the previous hidden state and the data must therefore be parsed sequentially - there is no parallelisation routine. This results in very computationally intensive training.\n",
    "\n",
    "The Transformer was introduced in 2016 in the seminal paper [Attention is all you need](https://arxiv.org/abs/1706.03762) and since then have been the basis of most large language models. They neatly solve all three problems with reccurent neural networks with the idea of attention. It should be noted that attention is not a pioneering feature of this paper. However, the attention mechanisms neatly allows the author to encode long range and importantly asymetric dependencies between sequence tokens. These attention representations are bidirectional and all-to-all allowing for massive parrallelisation within an attention layer. They are also relatively stable in their gradients because attention representations are not recursively computed.\n",
    "\n",
    "The outcomes of this notebook are:\n",
    "\n",
    "* Understanding attention and multi-head attention.\n",
    "* Creating a custom Flux layer to represent attention heads.\n",
    "* Understanding sequence generation.\n",
    "* A comprenhensive look at the transformer architecture.\n",
    "* Implementing a custom Transformer network.\n",
    "* Training a Transformer on biological sequence data.\n",
    "\n",
    "## Attention\n",
    "\n",
    "At"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372fbb6",
   "metadata": {},
   "source": [
    "## The Transformer Architecture\n",
    "\n",
    "The basic Transformer architecutre.\n",
    "\n",
    "\n",
    "\n",
    "### Keys\n",
    "\n",
    "### Queries\n",
    "\n",
    "### Values\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "### Input Encoding\n",
    "\n",
    "### Output Decoding\n",
    "\n",
    "To generate the output sequence the Transformer proceeds in much the same way as a traditional sequence decoder. It takes a `start` symbol and generates data sequentially until a `stop` symbol is generated. The outputs are encoded in an arbitrarily long vector that is expected to be longer than the length of the output sentence e.g. 512. The output layer is decoded using outputs generated up until the point of the sequence as inputs with a mask of `-Inf` for future ouputs. \n",
    "\n",
    "The masked output is used to generate its own query values and is combined with a positional encoding in the same fashion as the input layer. Then it is generally passed through its own self attention layer with masked multi-head attention before interacting with the encoded keys and values. The queries are combined with the encoder results to generate the next sequential output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f300639",
   "metadata": {},
   "source": [
    "## Implementing the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b20133",
   "metadata": {},
   "source": [
    "## Gene-Protein Sequence Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca2584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a47443a",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "We have already seen a recurrent neural network: the Hopfield-Tank network. However, this is not typically how they are thought of in the modern context. In this notebook we will go through recurrent neural networks more generally and discuss some of their implementations, limitations, and data types and tasks that they work well on. The *recurrent* implies that the output can *feed-back* again to the network as an input. Let's construct this graphically in the style we have been developing throughout the text.\n",
    "\n",
    "[IMG]\n",
    "\n",
    "Notice that after the activation the data is given a path to flow back into input. How do we make sense of this? In the Hopfield-Tank network we applied this rule recursively until a stable point. For more generic neural networks we tend to visualise it as a sequence where input is fed in as x(t) and combined with output y(t-1). This is known as unrolling and it is our first clue that RNNs will work well with sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a13e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

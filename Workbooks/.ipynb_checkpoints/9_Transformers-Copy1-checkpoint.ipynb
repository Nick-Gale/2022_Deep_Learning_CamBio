{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff57ecf",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Transformers are a sophisiticated modern neural network best adapted to sequential data. They are in the process of replacing the recurrent neural network as the de-facto model for sequence analysis. As a brief review: recurrent neural networks incorporate information about sequential dependencies through recurrent connections which allow a hidden state to incorporate variables that have been previously exposed to the network. There are some principle shortcomings with this approach: short memory, exploding/vanishing gradients, and $O(n)$ complexity. Tokens that are historically adjacent to each other have more weight and thus the information in the sequence decays over long ranges; the network has a relatively short memory. The hidden state is called recursively over the course of training and thus the gradients have a tendency to either explode or vanish and various regularisation tricks are needed to stablise the training routine. Finally, the current hidden state depends on the previous hidden state and the data must therefore be parsed sequentially - there is no parallelisation routine. This results in very computationally intensive training.\n",
    "\n",
    "The Transformer was introduced in 2016 in the seminal paper [Attention is all you need](https://arxiv.org/abs/1706.03762) and since then have been the basis of most large language models. They neatly solve all three problems with reccurent neural networks with the idea of attention. It should be noted that attention is not a pioneering feature of this paper. However, the attention mechanisms neatly allows the author to encode long range and importantly asymetric dependencies between sequence tokens. These attention representations are bidirectional and all-to-all allowing for massive parrallelisation within an attention layer. They are also relatively stable in their gradients because attention representations are not recursively computed.\n",
    "\n",
    "## Self Attention\n",
    "\n",
    "Attention is the key concept in the Transformer model. It heuristically refers to how much each element in the sequence relates to one and other. The simplest form of this is the Euclidean projection of one vector onto another and the attention matrix is accordingly defined by dot-product; recall that the dot product has the geometric intepretation of how much one element projects onto another. Suppose that the sequence elements are $v_i$\n",
    "\n",
    "$$ W_{ij} = v_j v_i^T $$\n",
    "\n",
    "Now that we have created the attention relationships we want to output some intepretable configuration of vectors that incorporate these relationships. The most straightforward approach is a weighted linear combination of all vectors in the sequence: the attention weights are used to incorporate each vectors influence on each other vector.\n",
    "\n",
    "$$ a_i = \\sum_j W_{ij} v_j $$\n",
    "\n",
    "Finally, we want to scale these outputs into the \"interpretable\" regime therefore we apply a normalisation factor (scaling by the sqrt of the dimensionality) and the `softmax` function to convert these output into a probability distribution. These are just implementation details. The next thing we can note, and the algebra suggests it automatically, is that this can be parallelised. We can concatenate all the input vectors into a matrix $A$ and pre-multiply it by the weight matrix $W$ completing the attention transformation in a single step.\n",
    "\n",
    "$$ V = \\text{softmax}\\left(\\frac{W}{\\sqrt{d}}\\right) V$$\n",
    "\n",
    "## Keys, Queries, and Values\n",
    "Up until this point we have not provided any mechanism to *learn*. It would be perculiar if the primary component of a learning algorithm had no learnable parameters. Before we rectify this we need to clarify some terminology. If we apply the same vector concatenation trick to constructing the attention matrix we find that the attention matrix $W$ is the product of two matrices $A$ and $A^T$ which for now we will label $K$ and $Q$. Let's imagine that each of the input vectors where one-hot encoded: they had a 1 in a singular index and a 0 elsewhere. The resulting attention weights would be one if and only if the input indexes were identical: $i = j$. This means that the operation is acting like a dictionary or look-up table would act: if the first vector matches the second vector output a Boolean true. In computer science these are generally labelled *keys* and *queries*. These terms motivate our matrices $K$ and $Q$. The final component of the look up table is to output the value associated with a queried key. These are nothing more than the input vectors again and are encoded in our $V$ matrix.\n",
    "\n",
    "So far, the input vectors have played the key role in all the operations and we cant expect to learn much about their relational structure. Let's suppose that there is a more efficient lookup table that could encoded the data. This would amount to an abitrary set of keys and queries, but the values would remain the same. We can imagine the keys and queries as embedding matrices and this would allow us to embedd our inputs in a lower (or higher) dimensional space. Therefore, if the input tokens $v_i$ are vectors of length $N$ and we want to embed them into a space of dimension $P$ then our key and query matrices should have dimensions $N \\times P$ and $P \\times N$ respectively. These matrices will now be intialised in a way that does not need to depend on $v_i$, usually just a random number in each slot. The goal will be to learn the best form of these matrices. This can be thought of in much the same way as a convolutional filter where the goal was to learn the kernel weights. We finally arrive at the complete attention equation:\n",
    "\n",
    "$$ A(K, Q, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d}}\\right)V $$.\n",
    "\n",
    "## Positional Encoding\n",
    "The key and query matrices allow the data to interact with each other bidirectionally and asymetrically (one element can pay more attention to other elements than those elements pay to it: think of a fan that dotes on a celebrity). However, this comes at the cost of losing sequence information as the indexes are not encoded in the learning protocol. The ordering is usually extremely important for sequentially organised data and we do not want to through out this ordering. To incorporate it we augment our input vectors with a number that indicates the relative ordering. This is commonly chosen to be a sinusoid (but this choice is abitrary). The encoding is typically:\n",
    "\n",
    "$$ p_d(x, i) = \\sin \\left( \\frac{{x^{\\frac{2i}{d}}}}{10000}  \\right) $$\n",
    "\n",
    "This encoding is acting as another embedding in the same fashion as the keys and queries matrices. This, in principle, could also be learned. However, we choose a fixed function because in practice the learned results do not vastly outperform a fixed embedding and they come at a larger cost.\n",
    "\n",
    "## Attention Heads\n",
    "\n",
    "The attention operation in conjunction with the positional encoding form an *attention head*. This is nothing more than a learned representation of what and where each element in the sequence should be pay attention to. It is certainly possible that different sub-sequences mean different things to one and other in different contexts. This can be represented with multiple attention heads operating on the same inputs. As an analogy, think of the different features that can be extracted by having multiple convolutional kernels in each layer. Multihead attention is exteremely easy to implement. Simply concatentate the key/query matrices together linearly. In a similar analogy to convolutional networks attention heads may be composed in layers. The outputs of an attention head are a vector of weighted attention cues. These can be passed to a new set of attention matrices. In doing so we can learn deep and convoluted relationships. We are now ready to create our custom attention head and multiattention head types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b617a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Attention\n",
    "    Q::Matrix\n",
    "    K::Matrix\n",
    "    d::Int\n",
    "    function Attention(sequencedimension, embeddingdimension)\n",
    "        Q = rand(embeddingdimension, sequencedimension)\n",
    "        K = rand(sequencedimension, embeddingdimension)\n",
    "        new(Q, K, sequencedimension)\n",
    "    end\n",
    "end\n",
    "\n",
    "struct MultiHeadAttention\n",
    "    Q::Matrix\n",
    "    K::Matrix\n",
    "    d::Int\n",
    "    function MultiHeadAttention(attentionheads...)\n",
    "        Q = hcat(attentionheads.Q...)\n",
    "        K = vcat(attentionheads.K...)\n",
    "        d = attentionheads[1].d\n",
    "        new(Q, K, d)\n",
    "    end\n",
    "end\n",
    "\n",
    "(a::Union{Attention, MultiHeadAttention})(V) = Flux.softmax(a.K * a.Q ./ sqrt(d)) .* V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372fbb6",
   "metadata": {},
   "source": [
    "# The Transformer Architecture\n",
    "The Transformer uses attention as its principle working mechanism but it itself is a neural network architecture. It is composed of an encoder and a decoder. Each of these has several layers of multi-head attention and in-between the layers are Dense perceptron layers. This allows the vectors to be appropriately transformed/classified at each step. The encoder component is relatively straightforward - feed it the input vectors and record the final output. The decoder layer is slightly more sophisticated.\n",
    "\n",
    "A single pass of the decoder proceeds as follows: take a new input (not one of the encoder inputs) and pass it through an attention layer. This is the self-attention layer of the decoder. For every subsequent attention layer the encoder outputs will be combined with the decoder pass. These will go through multi-head attention layers, be normalised, and then passed through a Dense perceptron layer as in the encoder. \n",
    "\n",
    "[Transfomer](./images/transformer.png)\n",
    "\n",
    "*The Transformer architecture proposed in \"Attention is All You Need\"*\n",
    "\n",
    "## Output Decoding\n",
    "\n",
    "To generate the output sequence the Transformer proceeds in much the same way as a traditional sequence decoder. It takes a `start` symbol and generates data sequentially until a `stop` symbol is generated. The outputs are encoded in an arbitrarily long vector that is expected to be longer than the length of the output sentence e.g. 512. The output layer is decoded using outputs generated up until the point of the sequence as inputs with a mask of `-Inf` for future ouputs. \n",
    "\n",
    "The masked output is used to generate its own query values and is combined with a positional encoding in the same fashion as the input layer. Then it is generally passed through its own self attention layer with masked multi-head attention before interacting with the encoded keys and values. The queries are combined with the encoder results to generate the next sequential output. This is autoregression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f300639",
   "metadata": {},
   "source": [
    "## Implementing the Transformer\n",
    "\n",
    "The Transformer is an inherently simple architecture and it is fairly straightforward to take a custom type as defined before and proceed through Zygote backpropogation and Flux API calls to complete training. However, as always, implementing this will require uninformative boiler-plate code. We have covered how one might implement custom architectures through custom layer types in previous notebooks and these can be used for reference. In this implementation we will use an existing package in the Flux ecosystem: `Transformers.jl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b20133",
   "metadata": {},
   "source": [
    "## Acid-Amino Sequence Prediction: Learning the Language of Codons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327fa97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, String} with 4 entries:\n",
       "  \"A\" => \"A\"\n",
       "  \"U\" => \"U\"\n",
       "  \"C\" => \"C\"\n",
       "  \"G\" => \"G\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV, DataFrames, Transformers, Flux, Random, ProgressMeter\n",
    "\n",
    "amino_codon = Dict( # the amino acid to codon relationship\n",
    "    \"A\" => [\"GCU\"],#, \"GCC\", \"GCA\", \"GCG\"],\n",
    "    \"R\" => [\"CGU\"],#, \"CGC\", \"CGA\", \"CGG\", \"AGA\", \"AGG\"],\n",
    "    \"N\" => [\"AAU\"],#, \"AAC\"],\n",
    "    \"D\" => [\"GAU\"],#, \"GAC\"],\n",
    "    \"B\" => [\"AAU\"],#, \"AAC\", \"GAU\", \"GAC\"],\n",
    "    \"Q\" => [\"CAA\"],#, \"CAG\"],\n",
    "    \"E\" => [\"GAA\"],#, \"GAG\"],\n",
    "    \"Z\" => [\"CAA\"],#, \"CAG\", \"GAA\", \"GAG\"],\n",
    "    \"G\" => [\"GGU\"],#, \"GGC\", \"GGA\", \"GGG\"],\n",
    "    \"H\" => [\"CAU\"],#, \"CAC\"],\n",
    "    \"I\" => [\"AUU\"],#, \"AUC\", \"AUA\"],\n",
    "    \"L\" => [\"CUU\"],#, \"CUC\", \"CUA\", \"CUG\", \"UUA\", \"UUG\"],\n",
    "    \"K\" => [\"AAA\"],#, \"AAG\"],\n",
    "    \"M\" => [\"AUG\"],\n",
    "    \"F\" => [\"UUU\"],#, \"UUC\"],\n",
    "    \"P\" => [\"CCU\"],#, \"CCC\", \"CCA\", \"CCG\"],\n",
    "    \"S\" => [\"UCU\"],#, \"UCC\", \"UCA\", \"UCG\", \"AGU\", \"AGC\"],\n",
    "    \"T\" => [\"ACU\"],#, \"ACC\", \"ACA\", \"ACG\"],\n",
    "    \"W\" => [\"UGG\"],\n",
    "    \"Y\" => [\"UAU\"],#, \"UAC\"],\n",
    "    \"V\" => [\"GUU\"],#, \"GUC\", \"GUA\", \"GUG\"],||\n",
    "    #\"1\" => [\"AUG\"],\n",
    "    #\"9\" => [\"UAA\", \"UGA\", \"UAG\"],\n",
    ");\n",
    "\n",
    "amino_codon = Dict( # the amino acid to codon relationship\n",
    "    \"A\" => [\"GCU\", \"GCC\", \"GCA\", \"GCG\"],\n",
    "    \"R\" => [\"CGU\", \"CGC\", \"CGA\", \"CGG\", \"AGA\", \"AGG\"],\n",
    "    \"N\" => [\"AAU\", \"AAC\"],\n",
    "    \"D\" => [\"GAU\", \"GAC\"],\n",
    "    \"B\" => [\"AAU\", \"AAC\", \"GAU\", \"GAC\"],\n",
    "    \"Q\" => [\"CAA\", \"CAG\"],\n",
    "    \"E\" => [\"GAA\", \"GAG\"],\n",
    "    \"Z\" => [\"CAA\", \"CAG\", \"GAA\", \"GAG\"],\n",
    "    \"G\" => [\"GGU\", \"GGC\", \"GGA\", \"GGG\"],\n",
    "    \"H\" => [\"CAU\", \"CAC\"],\n",
    "    \"I\" => [\"AUU\", \"AUC\", \"AUA\"],\n",
    "    \"L\" => [\"CUU\", \"CUC\", \"CUA\", \"CUG\", \"UUA\", \"UUG\"],\n",
    "    \"K\" => [\"AAA\", \"AAG\"],\n",
    "    \"M\" => [\"AUG\"],\n",
    "    \"F\" => [\"UUU\", \"UUC\"],\n",
    "    \"P\" => [\"CCU\", \"CCC\", \"CCA\", \"CCG\"],\n",
    "    \"S\" => [\"UCU\", \"UCC\", \"UCA\", \"UCG\", \"AGU\", \"AGC\"],\n",
    "    \"T\" => [\"ACU\", \"ACC\", \"ACA\", \"ACG\"],\n",
    "    \"W\" => [\"UGG\"],\n",
    "    \"Y\" => [\"UAU\", \"UAC\"],\n",
    "    \"V\" => [\"GUU\", \"GUC\", \"GUA\", \"GUG\"],\n",
    "    #\"1\" => [\"AUG\"],\n",
    "    #\"9\" => [\"UAA\", \"UGA\", \"UAG\"],\n",
    ");\n",
    "\n",
    "amino_codon = Dict(\"A\"=>\"A\", \"C\"=>\"C\", \"G\"=>\"G\", \"U\"=>\"U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122b97af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"./data/codon.csv\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminos = [rand(collect(keys(amino_codon)), 10) for i in 1:10]\n",
    "create_genome(v) = prod(map(x->rand(amino_codon[x]), v))\n",
    "genomes = [create_genome(v) for v in aminos]\n",
    "df = DataFrame(acids=prod.(aminos), genomes=genomes)\n",
    "CSV.write(\"./data/codon.csv\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1be02c",
   "metadata": {},
   "source": [
    "Let's import and inspect the data. They are in the form of strings of letters. Indexing a single element of a string returns a `Char` character type but we want the tokens to be in form of strings. Let's create a processing function `string_split` to do this for us and create test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40bd63e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"UGUUUAAGGG\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"UGUUUAAGGG\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Vector{String}}:\n",
       " [\"G\", \"C\", \"A\", \"A\", \"U\", \"C\", \"C\", \"G\", \"A\", \"A\"]\n",
       " [\"G\", \"U\", \"G\", \"U\", \"U\", \"C\", \"C\", \"A\", \"G\", \"U\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "string_split(v) = map(x -> string(x), collect(v))\n",
    "df = CSV.File(\"./data/codon.csv\") |> DataFrame;\n",
    "display(df[1,:acids][1:10])\n",
    "display(df[1,:genomes][1:10])\n",
    "\n",
    "dl = length(df[!, :acids])\n",
    "ds = round(Int, 0.8 * dl)\n",
    "\n",
    "train_x = string_split.(df[1:ds, :genomes]);\n",
    "train_y = string_split.(df[1:ds, :acids]);\n",
    "test_x = string_split.(df[ds+1:end, :genomes])\n",
    "test_y = string_split.(df[ds+1:end, :acids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a340a0",
   "metadata": {},
   "source": [
    "It's time to start constructing the Transfomer. First, create a convenience function to append start and stop tokens to a vector. Then, create a vocabulary of available tokens. Finally, encode the data with the tokenizer and use these tokens to embed tokens into a space of dimension 64. These will be augmented with positional embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ed219f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary{String}(6, unk=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenisers\n",
    "pre_process(v) = cat(\"1\", v..., \"9\"; dims=1)\n",
    "labels_y = cat(\"0\", \"1\", \"9\", unique(train_y[1])...; dims=1)\n",
    "labels_x = cat(\"0\", \"1\", \"9\", unique(train_x[1])...; dims=1)\n",
    "tokenizer_x = Transformers.Vocabulary(labels_x, \"0\")\n",
    "tokenizer_y = Transformers.Vocabulary(labels_y, \"0\")\n",
    "encoded_x = tokenizer_x.(pre_process.(train_x))\n",
    "encoded_y = tokenizer_y.(pre_process.(train_y));\n",
    "tokenizer_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db202d99",
   "metadata": {},
   "source": [
    "Create a simple embedding function. We will choose the dimensionality to be 64 to keep the model relatively small. The transfomer encoder will be composed of three blocks each with 4 heads and and inner dimensionality of 128. These are much smaller than the original model. The decoder will be similarly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942af38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 512\n",
    "h = 8\n",
    "hd = 64\n",
    "innerd = 2048\n",
    "\n",
    "embed_x = Transformers.Embed(d, length(tokenizer_x))\n",
    "embed_y = Transformers.Embed(d, length(tokenizer_y))\n",
    "position_embed = Transformers.Basic.PositionEmbedding(d)\n",
    "\n",
    "function embeddingx(x)\n",
    "  sem_em = embed_x(x, inv(sqrt(d)))\n",
    "  em = sem_em .+ position_embed(sem_em)\n",
    "  return em\n",
    "end\n",
    "\n",
    "te1 = Transformer(d, h, hd, innerd; pdrop=0.1)\n",
    "te2 = Transformer(d, h, hd, innerd; pdrop=0.1)\n",
    "te3 = Transformer(d, h, hd, innerd; pdrop=0.1)\n",
    "te4 = Transformer(d, h, hd, innerd; pdrop=0.1)\n",
    "te5 = Transformer(d, h, hd, innerd; pdrop=0.1)\n",
    "te6 = Transformer(d, h, hd, innerd; pdrop=0.1)\n",
    "\n",
    "# Tencoder = Flux.Chain(\n",
    "#     embeddingx, \n",
    "#     te1,\n",
    "#     te2\n",
    "# #    te3,\n",
    "# #     te4,\n",
    "# #     te5,\n",
    "# #     te6,\n",
    "# )\n",
    "\n",
    "function Tencoder(x)\n",
    "    em = embeddingx(x)\n",
    "    t1 = te1(em)\n",
    "    t2 = te2(t1)\n",
    "    return t2\n",
    "end\n",
    "\n",
    "function embeddingy(y)\n",
    "  sem_em = embed_y(y, inv(sqrt(d)))\n",
    "  em = sem_em .+ position_embed(sem_em)\n",
    "  return em\n",
    "end\n",
    "\n",
    "Dec1 = TransformerDecoder(d, h, hd, innerd; pdrop=0.1)\n",
    "Dec2 = TransformerDecoder(d, h, hd, innerd; pdrop=0.1)\n",
    "Dec3 = TransformerDecoder(d, h, hd, innerd; pdrop=0.1)\n",
    "Dec4 = TransformerDecoder(d, h, hd, innerd; pdrop=0.1)\n",
    "Dec5 = TransformerDecoder(d, h, hd, innerd; pdrop=0.1)\n",
    "\n",
    "ffn = Transformers.Positionwise(Dense(d, length(tokenizer_y)), logsoftmax)\n",
    "\n",
    "function Tdecoder(y, mx)\n",
    "    emy = embeddingx(y)\n",
    "    d1 = Dec1(emy, mx)\n",
    "    d2 = Dec2(d1, mx)\n",
    "#     d3 = Dec3(d2, mx)\n",
    "#     d4 = Dec4(d3, mx)\n",
    "#     d5 = Dec5(d4, mx)\n",
    "    p = ffn(d2)\n",
    "    return p\n",
    "end\n",
    "    \n",
    "#ps = Flux.params(embed_x, position_embed, te1, te2, te3, te4, te5, te6, Dec1, Dec2, Dec3, Dec4, Dec5, ffn);\n",
    "ps = Flux.params(embed_x, position_embed, te1, te2, Dec1, Dec2, ffn);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4639c92",
   "metadata": {},
   "source": [
    "Let's now define our loss. Our final layer is the `softmax` layer converting the tokens into probabilities for the tokens in the output space. It makes sense to `onehot` encode the outputs and perform label smoothing to convert these outputs to a probability distribution. Then, an appropriate loss function is `crossentropy` a measure of the similarity between two probability distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f927b23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function loss(xdata, ydata)\n",
    "#     ytarget = Flux.label_smoothing(Flux.onehot(tokenizer_y, ydata[1]), 0.002)\n",
    "#     ypred = Tdecoder(ydata[1], Tencoder(xdata[1]))\n",
    "#     for i in 2:length(ydata)\n",
    "#         ytarget = cat(ytarget, Flux.label_smoothing(Flux.onehot(tokenizer_y, ydata[i]), 0.002), dims=3)# Flux.onehot(tokenizer_y, ydata[i]) # \n",
    "#         ypred = cat(ypred, Tdecoder(ydata[i], Tencoder(xdata[i])), dims=3)\n",
    "#     end\n",
    "#           #  println(ypred)\n",
    "#     return Transformers.Basic.logcrossentropy(ypred, ytarget) # L/length(ydata)\n",
    "# end\n",
    "\n",
    "function smooth(et)\n",
    "    sm = fill!(similar(et, Float32), 1e-6/size(embed_y, 2))\n",
    "    p = sm .* (1 .+ -et)\n",
    "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
    "    label\n",
    "end\n",
    "Flux.@nograd smooth\n",
    "\n",
    "function loss(x, y)\n",
    "  label = Flux.onehot(tokenizer_y, y) #turn the index to one-hot encoding\n",
    "  label = smooth(label) #perform label smoothing\n",
    "  enc = Tencoder(x)\n",
    "  probs = Tdecoder(y, enc)\n",
    "  l = Transformers.Basic.logkldivergence(label[:, 2:end, :], probs[:, 1:end-1, :])\n",
    "  return l\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01fd65a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip050\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip050)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip051\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip050)\" d=\"\n",
       "M169.121 1486.45 L2352.76 1486.45 L2352.76 123.472 L169.121 123.472  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip052\">\n",
       "    <rect x=\"169\" y=\"123\" width=\"2185\" height=\"1364\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip052)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  228.86,1486.45 228.86,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip052)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  744.384,1486.45 744.384,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip052)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1259.91,1486.45 1259.91,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip052)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1775.43,1486.45 1775.43,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip052)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2290.95,1486.45 2290.95,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  169.121,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  228.86,1486.45 228.86,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  744.384,1486.45 744.384,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1259.91,1486.45 1259.91,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1775.43,1486.45 1775.43,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2290.95,1486.45 2290.95,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip050)\" d=\"M228.86 1517.37 Q225.249 1517.37 223.42 1520.93 Q221.615 1524.47 221.615 1531.6 Q221.615 1538.71 223.42 1542.27 Q225.249 1545.82 228.86 1545.82 Q232.494 1545.82 234.3 1542.27 Q236.129 1538.71 236.129 1531.6 Q236.129 1524.47 234.3 1520.93 Q232.494 1517.37 228.86 1517.37 M228.86 1513.66 Q234.67 1513.66 237.726 1518.27 Q240.805 1522.85 240.805 1531.6 Q240.805 1540.33 237.726 1544.94 Q234.67 1549.52 228.86 1549.52 Q223.05 1549.52 219.971 1544.94 Q216.916 1540.33 216.916 1531.6 Q216.916 1522.85 219.971 1518.27 Q223.05 1513.66 228.86 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M708.076 1544.91 L724.396 1544.91 L724.396 1548.85 L702.451 1548.85 L702.451 1544.91 Q705.113 1542.16 709.697 1537.53 Q714.303 1532.88 715.484 1531.53 Q717.729 1529.01 718.609 1527.27 Q719.511 1525.51 719.511 1523.82 Q719.511 1521.07 717.567 1519.33 Q715.646 1517.6 712.544 1517.6 Q710.345 1517.6 707.891 1518.36 Q705.46 1519.13 702.683 1520.68 L702.683 1515.95 Q705.507 1514.82 707.96 1514.24 Q710.414 1513.66 712.451 1513.66 Q717.822 1513.66 721.016 1516.35 Q724.21 1519.03 724.21 1523.52 Q724.21 1525.65 723.4 1527.57 Q722.613 1529.47 720.507 1532.07 Q719.928 1532.74 716.826 1535.95 Q713.724 1539.15 708.076 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M734.257 1514.29 L752.613 1514.29 L752.613 1518.22 L738.539 1518.22 L738.539 1526.7 Q739.558 1526.35 740.576 1526.19 Q741.595 1526 742.613 1526 Q748.4 1526 751.78 1529.17 Q755.159 1532.34 755.159 1537.76 Q755.159 1543.34 751.687 1546.44 Q748.215 1549.52 741.895 1549.52 Q739.72 1549.52 737.451 1549.15 Q735.206 1548.78 732.798 1548.04 L732.798 1543.34 Q734.882 1544.47 737.104 1545.03 Q739.326 1545.58 741.803 1545.58 Q745.807 1545.58 748.145 1543.48 Q750.483 1541.37 750.483 1537.76 Q750.483 1534.15 748.145 1532.04 Q745.807 1529.94 741.803 1529.94 Q739.928 1529.94 738.053 1530.35 Q736.201 1530.77 734.257 1531.65 L734.257 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M774.372 1517.37 Q770.761 1517.37 768.932 1520.93 Q767.127 1524.47 767.127 1531.6 Q767.127 1538.71 768.932 1542.27 Q770.761 1545.82 774.372 1545.82 Q778.006 1545.82 779.812 1542.27 Q781.641 1538.71 781.641 1531.6 Q781.641 1524.47 779.812 1520.93 Q778.006 1517.37 774.372 1517.37 M774.372 1513.66 Q780.182 1513.66 783.238 1518.27 Q786.317 1522.85 786.317 1531.6 Q786.317 1540.33 783.238 1544.94 Q780.182 1549.52 774.372 1549.52 Q768.562 1549.52 765.483 1544.94 Q762.428 1540.33 762.428 1531.6 Q762.428 1522.85 765.483 1518.27 Q768.562 1513.66 774.372 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1219.53 1514.29 L1237.88 1514.29 L1237.88 1518.22 L1223.81 1518.22 L1223.81 1526.7 Q1224.83 1526.35 1225.85 1526.19 Q1226.86 1526 1227.88 1526 Q1233.67 1526 1237.05 1529.17 Q1240.43 1532.34 1240.43 1537.76 Q1240.43 1543.34 1236.96 1546.44 Q1233.48 1549.52 1227.16 1549.52 Q1224.99 1549.52 1222.72 1549.15 Q1220.47 1548.78 1218.07 1548.04 L1218.07 1543.34 Q1220.15 1544.47 1222.37 1545.03 Q1224.6 1545.58 1227.07 1545.58 Q1231.08 1545.58 1233.41 1543.48 Q1235.75 1541.37 1235.75 1537.76 Q1235.75 1534.15 1233.41 1532.04 Q1231.08 1529.94 1227.07 1529.94 Q1225.2 1529.94 1223.32 1530.35 Q1221.47 1530.77 1219.53 1531.65 L1219.53 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1259.64 1517.37 Q1256.03 1517.37 1254.2 1520.93 Q1252.4 1524.47 1252.4 1531.6 Q1252.4 1538.71 1254.2 1542.27 Q1256.03 1545.82 1259.64 1545.82 Q1263.28 1545.82 1265.08 1542.27 Q1266.91 1538.71 1266.91 1531.6 Q1266.91 1524.47 1265.08 1520.93 Q1263.28 1517.37 1259.64 1517.37 M1259.64 1513.66 Q1265.45 1513.66 1268.51 1518.27 Q1271.59 1522.85 1271.59 1531.6 Q1271.59 1540.33 1268.51 1544.94 Q1265.45 1549.52 1259.64 1549.52 Q1253.83 1549.52 1250.75 1544.94 Q1247.7 1540.33 1247.7 1531.6 Q1247.7 1522.85 1250.75 1518.27 Q1253.83 1513.66 1259.64 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1289.8 1517.37 Q1286.19 1517.37 1284.36 1520.93 Q1282.56 1524.47 1282.56 1531.6 Q1282.56 1538.71 1284.36 1542.27 Q1286.19 1545.82 1289.8 1545.82 Q1293.44 1545.82 1295.24 1542.27 Q1297.07 1538.71 1297.07 1531.6 Q1297.07 1524.47 1295.24 1520.93 Q1293.44 1517.37 1289.8 1517.37 M1289.8 1513.66 Q1295.61 1513.66 1298.67 1518.27 Q1301.75 1522.85 1301.75 1531.6 Q1301.75 1540.33 1298.67 1544.94 Q1295.61 1549.52 1289.8 1549.52 Q1283.99 1549.52 1280.91 1544.94 Q1277.86 1540.33 1277.86 1531.6 Q1277.86 1522.85 1280.91 1518.27 Q1283.99 1513.66 1289.8 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1733.71 1514.29 L1755.93 1514.29 L1755.93 1516.28 L1743.38 1548.85 L1738.5 1548.85 L1750.3 1518.22 L1733.71 1518.22 L1733.71 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1765.1 1514.29 L1783.45 1514.29 L1783.45 1518.22 L1769.38 1518.22 L1769.38 1526.7 Q1770.4 1526.35 1771.42 1526.19 Q1772.43 1526 1773.45 1526 Q1779.24 1526 1782.62 1529.17 Q1786 1532.34 1786 1537.76 Q1786 1543.34 1782.53 1546.44 Q1779.05 1549.52 1772.73 1549.52 Q1770.56 1549.52 1768.29 1549.15 Q1766.04 1548.78 1763.64 1548.04 L1763.64 1543.34 Q1765.72 1544.47 1767.94 1545.03 Q1770.17 1545.58 1772.64 1545.58 Q1776.65 1545.58 1778.98 1543.48 Q1781.32 1541.37 1781.32 1537.76 Q1781.32 1534.15 1778.98 1532.04 Q1776.65 1529.94 1772.64 1529.94 Q1770.77 1529.94 1768.89 1530.35 Q1767.04 1530.77 1765.1 1531.65 L1765.1 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1805.21 1517.37 Q1801.6 1517.37 1799.77 1520.93 Q1797.97 1524.47 1797.97 1531.6 Q1797.97 1538.71 1799.77 1542.27 Q1801.6 1545.82 1805.21 1545.82 Q1808.85 1545.82 1810.65 1542.27 Q1812.48 1538.71 1812.48 1531.6 Q1812.48 1524.47 1810.65 1520.93 Q1808.85 1517.37 1805.21 1517.37 M1805.21 1513.66 Q1811.02 1513.66 1814.08 1518.27 Q1817.16 1522.85 1817.16 1531.6 Q1817.16 1540.33 1814.08 1544.94 Q1811.02 1549.52 1805.21 1549.52 Q1799.4 1549.52 1796.32 1544.94 Q1793.27 1540.33 1793.27 1531.6 Q1793.27 1522.85 1796.32 1518.27 Q1799.4 1513.66 1805.21 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2235.48 1544.91 L2243.12 1544.91 L2243.12 1518.55 L2234.81 1520.21 L2234.81 1515.95 L2243.07 1514.29 L2247.75 1514.29 L2247.75 1544.91 L2255.39 1544.91 L2255.39 1548.85 L2235.48 1548.85 L2235.48 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2274.83 1517.37 Q2271.22 1517.37 2269.39 1520.93 Q2267.59 1524.47 2267.59 1531.6 Q2267.59 1538.71 2269.39 1542.27 Q2271.22 1545.82 2274.83 1545.82 Q2278.47 1545.82 2280.27 1542.27 Q2282.1 1538.71 2282.1 1531.6 Q2282.1 1524.47 2280.27 1520.93 Q2278.47 1517.37 2274.83 1517.37 M2274.83 1513.66 Q2280.64 1513.66 2283.7 1518.27 Q2286.78 1522.85 2286.78 1531.6 Q2286.78 1540.33 2283.7 1544.94 Q2280.64 1549.52 2274.83 1549.52 Q2269.02 1549.52 2265.94 1544.94 Q2262.89 1540.33 2262.89 1531.6 Q2262.89 1522.85 2265.94 1518.27 Q2269.02 1513.66 2274.83 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2304.99 1517.37 Q2301.38 1517.37 2299.55 1520.93 Q2297.75 1524.47 2297.75 1531.6 Q2297.75 1538.71 2299.55 1542.27 Q2301.38 1545.82 2304.99 1545.82 Q2308.63 1545.82 2310.43 1542.27 Q2312.26 1538.71 2312.26 1531.6 Q2312.26 1524.47 2310.43 1520.93 Q2308.63 1517.37 2304.99 1517.37 M2304.99 1513.66 Q2310.8 1513.66 2313.86 1518.27 Q2316.94 1522.85 2316.94 1531.6 Q2316.94 1540.33 2313.86 1544.94 Q2310.8 1549.52 2304.99 1549.52 Q2299.18 1549.52 2296.11 1544.94 Q2293.05 1540.33 2293.05 1531.6 Q2293.05 1522.85 2296.11 1518.27 Q2299.18 1513.66 2304.99 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2335.16 1517.37 Q2331.54 1517.37 2329.72 1520.93 Q2327.91 1524.47 2327.91 1531.6 Q2327.91 1538.71 2329.72 1542.27 Q2331.54 1545.82 2335.16 1545.82 Q2338.79 1545.82 2340.6 1542.27 Q2342.42 1538.71 2342.42 1531.6 Q2342.42 1524.47 2340.6 1520.93 Q2338.79 1517.37 2335.16 1517.37 M2335.16 1513.66 Q2340.97 1513.66 2344.02 1518.27 Q2347.1 1522.85 2347.1 1531.6 Q2347.1 1540.33 2344.02 1544.94 Q2340.97 1549.52 2335.16 1549.52 Q2329.35 1549.52 2326.27 1544.94 Q2323.21 1540.33 2323.21 1531.6 Q2323.21 1522.85 2326.27 1518.27 Q2329.35 1513.66 2335.16 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip052)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  169.121,1333.03 2352.76,1333.03 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip052)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  169.121,1089.05 2352.76,1089.05 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip052)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  169.121,845.057 2352.76,845.057 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip052)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  169.121,601.069 2352.76,601.069 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip052)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  169.121,357.081 2352.76,357.081 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  169.121,1486.45 169.121,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  169.121,1333.03 188.019,1333.03 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  169.121,1089.05 188.019,1089.05 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  169.121,845.057 188.019,845.057 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  169.121,601.069 188.019,601.069 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  169.121,357.081 188.019,357.081 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip050)\" d=\"M81.0614 1315.75 L99.4178 1315.75 L99.4178 1319.69 L85.3438 1319.69 L85.3438 1328.16 Q86.3623 1327.81 87.3808 1327.65 Q88.3993 1327.47 89.4178 1327.47 Q95.2049 1327.47 98.5845 1330.64 Q101.964 1333.81 101.964 1339.23 Q101.964 1344.8 98.4919 1347.91 Q95.0197 1350.98 88.7003 1350.98 Q86.5243 1350.98 84.2558 1350.61 Q82.0105 1350.24 79.6031 1349.5 L79.6031 1344.8 Q81.6864 1345.94 83.9086 1346.49 Q86.1308 1347.05 88.6077 1347.05 Q92.6123 1347.05 94.9502 1344.94 Q97.2882 1342.84 97.2882 1339.23 Q97.2882 1335.61 94.9502 1333.51 Q92.6123 1331.4 88.6077 1331.4 Q86.7327 1331.4 84.8577 1331.82 Q83.0058 1332.23 81.0614 1333.11 L81.0614 1315.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M121.177 1318.83 Q117.566 1318.83 115.737 1322.4 Q113.932 1325.94 113.932 1333.07 Q113.932 1340.17 115.737 1343.74 Q117.566 1347.28 121.177 1347.28 Q124.811 1347.28 126.617 1343.74 Q128.445 1340.17 128.445 1333.07 Q128.445 1325.94 126.617 1322.4 Q124.811 1318.83 121.177 1318.83 M121.177 1315.13 Q126.987 1315.13 130.043 1319.73 Q133.121 1324.32 133.121 1333.07 Q133.121 1341.79 130.043 1346.4 Q126.987 1350.98 121.177 1350.98 Q115.367 1350.98 112.288 1346.4 Q109.233 1341.79 109.233 1333.07 Q109.233 1324.32 112.288 1319.73 Q115.367 1315.13 121.177 1315.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M80.8299 1071.77 L103.052 1071.77 L103.052 1073.76 L90.5058 1106.33 L85.6216 1106.33 L97.4271 1075.7 L80.8299 1075.7 L80.8299 1071.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M112.219 1071.77 L130.575 1071.77 L130.575 1075.7 L116.501 1075.7 L116.501 1084.17 Q117.52 1083.83 118.538 1083.66 Q119.557 1083.48 120.575 1083.48 Q126.362 1083.48 129.742 1086.65 Q133.121 1089.82 133.121 1095.24 Q133.121 1100.82 129.649 1103.92 Q126.177 1107 119.857 1107 Q117.682 1107 115.413 1106.63 Q113.168 1106.26 110.76 1105.51 L110.76 1100.82 Q112.844 1101.95 115.066 1102.51 Q117.288 1103.06 119.765 1103.06 Q123.77 1103.06 126.107 1100.95 Q128.445 1098.85 128.445 1095.24 Q128.445 1091.63 126.107 1089.52 Q123.77 1087.41 119.765 1087.41 Q117.89 1087.41 116.015 1087.83 Q114.163 1088.25 112.219 1089.13 L112.219 1071.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M51.6634 858.402 L59.3023 858.402 L59.3023 832.036 L50.9921 833.703 L50.9921 829.444 L59.256 827.777 L63.9319 827.777 L63.9319 858.402 L71.5707 858.402 L71.5707 862.337 L51.6634 862.337 L51.6634 858.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M91.0151 830.856 Q87.404 830.856 85.5753 834.42 Q83.7697 837.962 83.7697 845.092 Q83.7697 852.198 85.5753 855.763 Q87.404 859.305 91.0151 859.305 Q94.6493 859.305 96.4548 855.763 Q98.2835 852.198 98.2835 845.092 Q98.2835 837.962 96.4548 834.42 Q94.6493 830.856 91.0151 830.856 M91.0151 827.152 Q96.8252 827.152 99.8808 831.758 Q102.959 836.342 102.959 845.092 Q102.959 853.818 99.8808 858.425 Q96.8252 863.008 91.0151 863.008 Q85.2049 863.008 82.1262 858.425 Q79.0707 853.818 79.0707 845.092 Q79.0707 836.342 82.1262 831.758 Q85.2049 827.152 91.0151 827.152 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M121.177 830.856 Q117.566 830.856 115.737 834.42 Q113.932 837.962 113.932 845.092 Q113.932 852.198 115.737 855.763 Q117.566 859.305 121.177 859.305 Q124.811 859.305 126.617 855.763 Q128.445 852.198 128.445 845.092 Q128.445 837.962 126.617 834.42 Q124.811 830.856 121.177 830.856 M121.177 827.152 Q126.987 827.152 130.043 831.758 Q133.121 836.342 133.121 845.092 Q133.121 853.818 130.043 858.425 Q126.987 863.008 121.177 863.008 Q115.367 863.008 112.288 858.425 Q109.233 853.818 109.233 845.092 Q109.233 836.342 112.288 831.758 Q115.367 827.152 121.177 827.152 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M52.6588 614.414 L60.2976 614.414 L60.2976 588.048 L51.9875 589.715 L51.9875 585.456 L60.2513 583.789 L64.9272 583.789 L64.9272 614.414 L72.5661 614.414 L72.5661 618.349 L52.6588 618.349 L52.6588 614.414 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M86.0382 614.414 L102.358 614.414 L102.358 618.349 L80.4133 618.349 L80.4133 614.414 Q83.0753 611.659 87.6586 607.03 Q92.2651 602.377 93.4456 601.034 Q95.691 598.511 96.5706 596.775 Q97.4734 595.016 97.4734 593.326 Q97.4734 590.571 95.5289 588.835 Q93.6076 587.099 90.5058 587.099 Q88.3067 587.099 85.8531 587.863 Q83.4225 588.627 80.6447 590.178 L80.6447 585.456 Q83.4688 584.321 85.9225 583.743 Q88.3762 583.164 90.4132 583.164 Q95.7836 583.164 98.978 585.849 Q102.172 588.534 102.172 593.025 Q102.172 595.155 101.362 597.076 Q100.575 598.974 98.4687 601.567 Q97.89 602.238 94.7882 605.455 Q91.6864 608.65 86.0382 614.414 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M112.219 583.789 L130.575 583.789 L130.575 587.724 L116.501 587.724 L116.501 596.196 Q117.52 595.849 118.538 595.687 Q119.557 595.502 120.575 595.502 Q126.362 595.502 129.742 598.673 Q133.121 601.844 133.121 607.261 Q133.121 612.84 129.649 615.942 Q126.177 619.02 119.857 619.02 Q117.682 619.02 115.413 618.65 Q113.168 618.279 110.76 617.539 L110.76 612.84 Q112.844 613.974 115.066 614.529 Q117.288 615.085 119.765 615.085 Q123.77 615.085 126.107 612.979 Q128.445 610.872 128.445 607.261 Q128.445 603.65 126.107 601.543 Q123.77 599.437 119.765 599.437 Q117.89 599.437 116.015 599.854 Q114.163 600.27 112.219 601.15 L112.219 583.789 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M51.6634 370.426 L59.3023 370.426 L59.3023 344.06 L50.9921 345.727 L50.9921 341.468 L59.256 339.801 L63.9319 339.801 L63.9319 370.426 L71.5707 370.426 L71.5707 374.361 L51.6634 374.361 L51.6634 370.426 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M81.0614 339.801 L99.4178 339.801 L99.4178 343.736 L85.3438 343.736 L85.3438 352.208 Q86.3623 351.861 87.3808 351.699 Q88.3993 351.514 89.4178 351.514 Q95.2049 351.514 98.5845 354.685 Q101.964 357.856 101.964 363.273 Q101.964 368.852 98.4919 371.953 Q95.0197 375.032 88.7003 375.032 Q86.5243 375.032 84.2558 374.662 Q82.0105 374.291 79.6031 373.551 L79.6031 368.852 Q81.6864 369.986 83.9086 370.541 Q86.1308 371.097 88.6077 371.097 Q92.6123 371.097 94.9502 368.991 Q97.2882 366.884 97.2882 363.273 Q97.2882 359.662 94.9502 357.555 Q92.6123 355.449 88.6077 355.449 Q86.7327 355.449 84.8577 355.866 Q83.0058 356.282 81.0614 357.162 L81.0614 339.801 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M121.177 342.88 Q117.566 342.88 115.737 346.444 Q113.932 349.986 113.932 357.116 Q113.932 364.222 115.737 367.787 Q117.566 371.328 121.177 371.328 Q124.811 371.328 126.617 367.787 Q128.445 364.222 128.445 357.116 Q128.445 349.986 126.617 346.444 Q124.811 342.88 121.177 342.88 M121.177 339.176 Q126.987 339.176 130.043 343.782 Q133.121 348.366 133.121 357.116 Q133.121 365.842 130.043 370.449 Q126.987 375.032 121.177 375.032 Q115.367 375.032 112.288 370.449 Q109.233 365.842 109.233 357.116 Q109.233 348.366 112.288 343.782 Q115.367 339.176 121.177 339.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M265.287 12.096 L316.45 12.096 L316.45 18.9825 L294.98 18.9825 L294.98 72.576 L286.757 72.576 L286.757 18.9825 L265.287 18.9825 L265.287 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M338.122 34.1734 Q336.867 33.4443 335.368 33.1202 Q333.909 32.7556 332.127 32.7556 Q325.808 32.7556 322.405 36.8875 Q319.043 40.9789 319.043 48.6757 L319.043 72.576 L311.548 72.576 L311.548 27.2059 L319.043 27.2059 L319.043 34.2544 Q321.392 30.1225 325.159 28.1376 Q328.927 26.1121 334.315 26.1121 Q335.084 26.1121 336.016 26.2337 Q336.948 26.3147 338.082 26.5172 L338.122 34.1734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M366.56 49.7694 Q357.526 49.7694 354.042 51.8354 Q350.559 53.9013 350.559 58.8839 Q350.559 62.8538 353.151 65.2034 Q355.784 67.5124 360.281 67.5124 Q366.479 67.5124 370.206 63.1374 Q373.973 58.7219 373.973 51.4303 L373.973 49.7694 L366.56 49.7694 M381.427 46.6907 L381.427 72.576 L373.973 72.576 L373.973 65.6895 Q371.421 69.8214 367.613 71.8063 Q363.805 73.7508 358.296 73.7508 Q351.328 73.7508 347.196 69.8619 Q343.105 65.9325 343.105 59.3701 Q343.105 51.7138 348.209 47.825 Q353.354 43.9361 363.522 43.9361 L373.973 43.9361 L373.973 43.2069 Q373.973 38.0623 370.57 35.2672 Q367.208 32.4315 361.091 32.4315 Q357.202 32.4315 353.516 33.3632 Q349.829 34.295 346.427 36.1584 L346.427 29.2718 Q350.518 27.692 354.367 26.9223 Q358.215 26.1121 361.861 26.1121 Q371.704 26.1121 376.565 31.2163 Q381.427 36.3204 381.427 46.6907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M396.779 27.2059 L404.233 27.2059 L404.233 72.576 L396.779 72.576 L396.779 27.2059 M396.779 9.54393 L404.233 9.54393 L404.233 18.9825 L396.779 18.9825 L396.779 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M457.543 45.1919 L457.543 72.576 L450.089 72.576 L450.089 45.4349 Q450.089 38.994 447.578 35.7938 Q445.066 32.5936 440.043 32.5936 Q434.007 32.5936 430.524 36.4419 Q427.04 40.2903 427.04 46.9338 L427.04 72.576 L419.546 72.576 L419.546 27.2059 L427.04 27.2059 L427.04 34.2544 Q429.713 30.163 433.319 28.1376 Q436.964 26.1121 441.704 26.1121 Q449.522 26.1121 453.533 30.9732 Q457.543 35.7938 457.543 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M472.41 27.2059 L479.864 27.2059 L479.864 72.576 L472.41 72.576 L472.41 27.2059 M472.41 9.54393 L479.864 9.54393 L479.864 18.9825 L472.41 18.9825 L472.41 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M533.173 45.1919 L533.173 72.576 L525.72 72.576 L525.72 45.4349 Q525.72 38.994 523.208 35.7938 Q520.697 32.5936 515.674 32.5936 Q509.638 32.5936 506.154 36.4419 Q502.67 40.2903 502.67 46.9338 L502.67 72.576 L495.176 72.576 L495.176 27.2059 L502.67 27.2059 L502.67 34.2544 Q505.344 30.163 508.949 28.1376 Q512.595 26.1121 517.334 26.1121 Q525.153 26.1121 529.163 30.9732 Q533.173 35.7938 533.173 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M577.895 49.3643 Q577.895 41.2625 574.533 36.8065 Q571.211 32.3505 565.176 32.3505 Q559.18 32.3505 555.818 36.8065 Q552.496 41.2625 552.496 49.3643 Q552.496 57.4256 555.818 61.8816 Q559.18 66.3376 565.176 66.3376 Q571.211 66.3376 574.533 61.8816 Q577.895 57.4256 577.895 49.3643 M585.349 66.9452 Q585.349 78.5308 580.204 84.1616 Q575.06 89.8329 564.446 89.8329 Q560.517 89.8329 557.033 89.2252 Q553.549 88.6581 550.268 87.4428 L550.268 80.1917 Q553.549 81.9741 556.75 82.8248 Q559.95 83.6755 563.272 83.6755 Q570.604 83.6755 574.25 79.8271 Q577.895 76.0193 577.895 68.282 L577.895 64.5957 Q575.586 68.6061 571.981 70.5911 Q568.376 72.576 563.353 72.576 Q555.008 72.576 549.904 66.2161 Q544.8 59.8562 544.8 49.3643 Q544.8 38.832 549.904 32.472 Q555.008 26.1121 563.353 26.1121 Q568.376 26.1121 571.981 28.0971 Q575.586 30.082 577.895 34.0924 L577.895 27.2059 L585.349 27.2059 L585.349 66.9452 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M627.397 12.096 L665.638 12.096 L665.638 18.9825 L635.58 18.9825 L635.58 36.8875 L664.382 36.8875 L664.382 43.7741 L635.58 43.7741 L635.58 65.6895 L666.367 65.6895 L666.367 72.576 L627.397 72.576 L627.397 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M686.703 65.7705 L686.703 89.8329 L679.209 89.8329 L679.209 27.2059 L686.703 27.2059 L686.703 34.0924 Q689.052 30.0415 692.617 28.0971 Q696.222 26.1121 701.205 26.1121 Q709.469 26.1121 714.613 32.6746 Q719.799 39.2371 719.799 49.9314 Q719.799 60.6258 714.613 67.1883 Q709.469 73.7508 701.205 73.7508 Q696.222 73.7508 692.617 71.8063 Q689.052 69.8214 686.703 65.7705 M712.061 49.9314 Q712.061 41.7081 708.659 37.0496 Q705.296 32.3505 699.382 32.3505 Q693.468 32.3505 690.065 37.0496 Q686.703 41.7081 686.703 49.9314 Q686.703 58.1548 690.065 62.8538 Q693.468 67.5124 699.382 67.5124 Q705.296 67.5124 708.659 62.8538 Q712.061 58.1548 712.061 49.9314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M749.735 32.4315 Q743.739 32.4315 740.256 37.1306 Q736.772 41.7891 736.772 49.9314 Q736.772 58.0738 740.215 62.7728 Q743.699 67.4314 749.735 67.4314 Q755.69 67.4314 759.173 62.7323 Q762.657 58.0333 762.657 49.9314 Q762.657 41.8701 759.173 37.1711 Q755.69 32.4315 749.735 32.4315 M749.735 26.1121 Q759.457 26.1121 765.007 32.4315 Q770.556 38.7509 770.556 49.9314 Q770.556 61.0714 765.007 67.4314 Q759.457 73.7508 749.735 73.7508 Q739.972 73.7508 734.422 67.4314 Q728.913 61.0714 728.913 49.9314 Q728.913 38.7509 734.422 32.4315 Q739.972 26.1121 749.735 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M815.562 28.9478 L815.562 35.9153 Q812.402 34.1734 809.202 33.3227 Q806.042 32.4315 802.802 32.4315 Q795.55 32.4315 791.54 37.0496 Q787.53 41.6271 787.53 49.9314 Q787.53 58.2358 791.54 62.8538 Q795.55 67.4314 802.802 67.4314 Q806.042 67.4314 809.202 66.5807 Q812.402 65.6895 815.562 63.9476 L815.562 70.8341 Q812.443 72.2924 809.08 73.0216 Q805.759 73.7508 801.991 73.7508 Q791.743 73.7508 785.707 67.3098 Q779.671 60.8689 779.671 49.9314 Q779.671 38.832 785.747 32.472 Q791.864 26.1121 802.478 26.1121 Q805.921 26.1121 809.202 26.8413 Q812.483 27.5299 815.562 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M866.239 45.1919 L866.239 72.576 L858.785 72.576 L858.785 45.4349 Q858.785 38.994 856.274 35.7938 Q853.762 32.5936 848.739 32.5936 Q842.703 32.5936 839.219 36.4419 Q835.735 40.2903 835.735 46.9338 L835.735 72.576 L828.241 72.576 L828.241 9.54393 L835.735 9.54393 L835.735 34.2544 Q838.409 30.163 842.014 28.1376 Q845.66 26.1121 850.4 26.1121 Q858.218 26.1121 862.228 30.9732 Q866.239 35.7938 866.239 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M909.948 65.6895 L923.316 65.6895 L923.316 19.5497 L908.773 22.4663 L908.773 15.0127 L923.235 12.096 L931.418 12.096 L931.418 65.6895 L944.786 65.6895 L944.786 72.576 L909.948 72.576 L909.948 65.6895 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M978.813 17.4837 Q972.494 17.4837 969.294 23.7221 Q966.134 29.92 966.134 42.3968 Q966.134 54.833 969.294 61.0714 Q972.494 67.2693 978.813 67.2693 Q985.173 67.2693 988.333 61.0714 Q991.533 54.833 991.533 42.3968 Q991.533 29.92 988.333 23.7221 Q985.173 17.4837 978.813 17.4837 M978.813 11.0023 Q988.981 11.0023 994.328 19.0636 Q999.716 27.0843 999.716 42.3968 Q999.716 57.6687 994.328 65.73 Q988.981 73.7508 978.813 73.7508 Q968.646 73.7508 963.258 65.73 Q957.911 57.6687 957.911 42.3968 Q957.911 27.0843 963.258 19.0636 Q968.646 11.0023 978.813 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1031.6 17.4837 Q1025.28 17.4837 1022.08 23.7221 Q1018.92 29.92 1018.92 42.3968 Q1018.92 54.833 1022.08 61.0714 Q1025.28 67.2693 1031.6 67.2693 Q1037.96 67.2693 1041.12 61.0714 Q1044.32 54.833 1044.32 42.3968 Q1044.32 29.92 1041.12 23.7221 Q1037.96 17.4837 1031.6 17.4837 M1031.6 11.0023 Q1041.76 11.0023 1047.11 19.0636 Q1052.5 27.0843 1052.5 42.3968 Q1052.5 57.6687 1047.11 65.73 Q1041.76 73.7508 1031.6 73.7508 Q1021.43 73.7508 1016.04 65.73 Q1010.69 57.6687 1010.69 42.3968 Q1010.69 27.0843 1016.04 19.0636 Q1021.43 11.0023 1031.6 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1084.38 17.4837 Q1078.06 17.4837 1074.86 23.7221 Q1071.7 29.92 1071.7 42.3968 Q1071.7 54.833 1074.86 61.0714 Q1078.06 67.2693 1084.38 67.2693 Q1090.74 67.2693 1093.9 61.0714 Q1097.1 54.833 1097.1 42.3968 Q1097.1 29.92 1093.9 23.7221 Q1090.74 17.4837 1084.38 17.4837 M1084.38 11.0023 Q1094.55 11.0023 1099.89 19.0636 Q1105.28 27.0843 1105.28 42.3968 Q1105.28 57.6687 1099.89 65.73 Q1094.55 73.7508 1084.38 73.7508 Q1074.21 73.7508 1068.82 65.73 Q1063.48 57.6687 1063.48 42.3968 Q1063.48 27.0843 1068.82 19.0636 Q1074.21 11.0023 1084.38 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1162.56 32.4315 Q1156.57 32.4315 1153.08 37.1306 Q1149.6 41.7891 1149.6 49.9314 Q1149.6 58.0738 1153.04 62.7728 Q1156.53 67.4314 1162.56 67.4314 Q1168.52 67.4314 1172 62.7323 Q1175.48 58.0333 1175.48 49.9314 Q1175.48 41.8701 1172 37.1711 Q1168.52 32.4315 1162.56 32.4315 M1162.56 26.1121 Q1172.28 26.1121 1177.83 32.4315 Q1183.38 38.7509 1183.38 49.9314 Q1183.38 61.0714 1177.83 67.4314 Q1172.28 73.7508 1162.56 73.7508 Q1152.8 73.7508 1147.25 67.4314 Q1141.74 61.0714 1141.74 49.9314 Q1141.74 38.7509 1147.25 32.4315 Q1152.8 26.1121 1162.56 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1218.71 9.54393 L1218.71 15.7418 L1211.58 15.7418 Q1207.57 15.7418 1205.99 17.3622 Q1204.45 18.9825 1204.45 23.1955 L1204.45 27.2059 L1216.72 27.2059 L1216.72 32.9987 L1204.45 32.9987 L1204.45 72.576 L1196.95 72.576 L1196.95 32.9987 L1189.82 32.9987 L1189.82 27.2059 L1196.95 27.2059 L1196.95 24.0462 Q1196.95 16.471 1200.48 13.0277 Q1204 9.54393 1211.66 9.54393 L1218.71 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1253.79 65.6895 L1267.16 65.6895 L1267.16 19.5497 L1252.61 22.4663 L1252.61 15.0127 L1267.08 12.096 L1275.26 12.096 L1275.26 65.6895 L1288.63 65.6895 L1288.63 72.576 L1253.79 72.576 L1253.79 65.6895 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1322.65 17.4837 Q1316.33 17.4837 1313.13 23.7221 Q1309.97 29.92 1309.97 42.3968 Q1309.97 54.833 1313.13 61.0714 Q1316.33 67.2693 1322.65 67.2693 Q1329.01 67.2693 1332.17 61.0714 Q1335.37 54.833 1335.37 42.3968 Q1335.37 29.92 1332.17 23.7221 Q1329.01 17.4837 1322.65 17.4837 M1322.65 11.0023 Q1332.82 11.0023 1338.17 19.0636 Q1343.56 27.0843 1343.56 42.3968 Q1343.56 57.6687 1338.17 65.73 Q1332.82 73.7508 1322.65 73.7508 Q1312.49 73.7508 1307.1 65.73 Q1301.75 57.6687 1301.75 42.3968 Q1301.75 27.0843 1307.1 19.0636 Q1312.49 11.0023 1322.65 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1375.44 17.4837 Q1369.12 17.4837 1365.92 23.7221 Q1362.76 29.92 1362.76 42.3968 Q1362.76 54.833 1365.92 61.0714 Q1369.12 67.2693 1375.44 67.2693 Q1381.8 67.2693 1384.96 61.0714 Q1388.16 54.833 1388.16 42.3968 Q1388.16 29.92 1384.96 23.7221 Q1381.8 17.4837 1375.44 17.4837 M1375.44 11.0023 Q1385.61 11.0023 1390.95 19.0636 Q1396.34 27.0843 1396.34 42.3968 Q1396.34 57.6687 1390.95 65.73 Q1385.61 73.7508 1375.44 73.7508 Q1365.27 73.7508 1359.88 65.73 Q1354.53 57.6687 1354.53 42.3968 Q1354.53 27.0843 1359.88 19.0636 Q1365.27 11.0023 1375.44 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1428.22 17.4837 Q1421.9 17.4837 1418.7 23.7221 Q1415.54 29.92 1415.54 42.3968 Q1415.54 54.833 1418.7 61.0714 Q1421.9 67.2693 1428.22 67.2693 Q1434.58 67.2693 1437.74 61.0714 Q1440.94 54.833 1440.94 42.3968 Q1440.94 29.92 1437.74 23.7221 Q1434.58 17.4837 1428.22 17.4837 M1428.22 11.0023 Q1438.39 11.0023 1443.74 19.0636 Q1449.12 27.0843 1449.12 42.3968 Q1449.12 57.6687 1443.74 65.73 Q1438.39 73.7508 1428.22 73.7508 Q1418.05 73.7508 1412.67 65.73 Q1407.32 57.6687 1407.32 42.3968 Q1407.32 27.0843 1412.67 19.0636 Q1418.05 11.0023 1428.22 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1464.35 62.2867 L1472.9 62.2867 L1472.9 72.576 L1464.35 72.576 L1464.35 62.2867 M1464.35 29.6769 L1472.9 29.6769 L1472.9 39.9662 L1464.35 39.9662 L1464.35 29.6769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1519.24 65.6895 L1532.61 65.6895 L1532.61 19.5497 L1518.07 22.4663 L1518.07 15.0127 L1532.53 12.096 L1540.71 12.096 L1540.71 65.6895 L1554.08 65.6895 L1554.08 72.576 L1519.24 72.576 L1519.24 65.6895 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1588.11 17.4837 Q1581.79 17.4837 1578.59 23.7221 Q1575.43 29.92 1575.43 42.3968 Q1575.43 54.833 1578.59 61.0714 Q1581.79 67.2693 1588.11 67.2693 Q1594.47 67.2693 1597.63 61.0714 Q1600.83 54.833 1600.83 42.3968 Q1600.83 29.92 1597.63 23.7221 Q1594.47 17.4837 1588.11 17.4837 M1588.11 11.0023 Q1598.28 11.0023 1603.62 19.0636 Q1609.01 27.0843 1609.01 42.3968 Q1609.01 57.6687 1603.62 65.73 Q1598.28 73.7508 1588.11 73.7508 Q1577.94 73.7508 1572.55 65.73 Q1567.21 57.6687 1567.21 42.3968 Q1567.21 27.0843 1572.55 19.0636 Q1577.94 11.0023 1588.11 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1640.89 17.4837 Q1634.57 17.4837 1631.37 23.7221 Q1628.21 29.92 1628.21 42.3968 Q1628.21 54.833 1631.37 61.0714 Q1634.57 67.2693 1640.89 67.2693 Q1647.25 67.2693 1650.41 61.0714 Q1653.61 54.833 1653.61 42.3968 Q1653.61 29.92 1650.41 23.7221 Q1647.25 17.4837 1640.89 17.4837 M1640.89 11.0023 Q1651.06 11.0023 1656.41 19.0636 Q1661.8 27.0843 1661.8 42.3968 Q1661.8 57.6687 1656.41 65.73 Q1651.06 73.7508 1640.89 73.7508 Q1630.73 73.7508 1625.34 65.73 Q1619.99 57.6687 1619.99 42.3968 Q1619.99 27.0843 1625.34 19.0636 Q1630.73 11.0023 1640.89 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1676.18 62.2867 L1684.72 62.2867 L1684.72 72.576 L1676.18 72.576 L1676.18 62.2867 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1720.05 17.4837 Q1713.73 17.4837 1710.53 23.7221 Q1707.37 29.92 1707.37 42.3968 Q1707.37 54.833 1710.53 61.0714 Q1713.73 67.2693 1720.05 67.2693 Q1726.41 67.2693 1729.57 61.0714 Q1732.77 54.833 1732.77 42.3968 Q1732.77 29.92 1729.57 23.7221 Q1726.41 17.4837 1720.05 17.4837 M1720.05 11.0023 Q1730.22 11.0023 1735.56 19.0636 Q1740.95 27.0843 1740.95 42.3968 Q1740.95 57.6687 1735.56 65.73 Q1730.22 73.7508 1720.05 73.7508 Q1709.88 73.7508 1704.49 65.73 Q1699.15 57.6687 1699.15 42.3968 Q1699.15 27.0843 1704.49 19.0636 Q1709.88 11.0023 1720.05 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1806.78 45.9616 Q1803.25 45.9616 1801.23 48.9592 Q1799.24 51.9569 1799.24 57.3041 Q1799.24 62.5703 1801.23 65.6084 Q1803.25 68.6061 1806.78 68.6061 Q1810.22 68.6061 1812.21 65.6084 Q1814.23 62.5703 1814.23 57.3041 Q1814.23 51.9974 1812.21 48.9997 Q1810.22 45.9616 1806.78 45.9616 M1806.78 40.8169 Q1813.18 40.8169 1816.95 45.2729 Q1820.71 49.7289 1820.71 57.3041 Q1820.71 64.8793 1816.9 69.3353 Q1813.14 73.7508 1806.78 73.7508 Q1800.3 73.7508 1796.53 69.3353 Q1792.76 64.8793 1792.76 57.3041 Q1792.76 49.6884 1796.53 45.2729 Q1800.34 40.8169 1806.78 40.8169 M1764.97 16.1469 Q1761.49 16.1469 1759.46 19.1851 Q1757.48 22.1828 1757.48 27.4489 Q1757.48 32.7961 1759.46 35.7938 Q1761.45 38.7915 1764.97 38.7915 Q1768.5 38.7915 1770.48 35.7938 Q1772.51 32.7961 1772.51 27.4489 Q1772.51 22.2233 1770.48 19.1851 Q1768.46 16.1469 1764.97 16.1469 M1801.55 11.0023 L1808.03 11.0023 L1770.2 73.7508 L1763.72 73.7508 L1801.55 11.0023 M1764.97 11.0023 Q1771.37 11.0023 1775.18 15.4583 Q1778.99 19.8737 1778.99 27.4489 Q1778.99 35.1051 1775.18 39.5206 Q1771.41 43.9361 1764.97 43.9361 Q1758.53 43.9361 1754.76 39.5206 Q1751.04 35.0646 1751.04 27.4489 Q1751.04 19.9142 1754.8 15.4583 Q1758.57 11.0023 1764.97 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1892.13 28.9478 L1892.13 35.9153 Q1888.97 34.1734 1885.77 33.3227 Q1882.61 32.4315 1879.37 32.4315 Q1872.12 32.4315 1868.11 37.0496 Q1864.1 41.6271 1864.1 49.9314 Q1864.1 58.2358 1868.11 62.8538 Q1872.12 67.4314 1879.37 67.4314 Q1882.61 67.4314 1885.77 66.5807 Q1888.97 65.6895 1892.13 63.9476 L1892.13 70.8341 Q1889.01 72.2924 1885.65 73.0216 Q1882.33 73.7508 1878.56 73.7508 Q1868.31 73.7508 1862.28 67.3098 Q1856.24 60.8689 1856.24 49.9314 Q1856.24 38.832 1862.32 32.472 Q1868.43 26.1121 1879.05 26.1121 Q1882.49 26.1121 1885.77 26.8413 Q1889.05 27.5299 1892.13 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1922.67 32.4315 Q1916.68 32.4315 1913.19 37.1306 Q1909.71 41.7891 1909.71 49.9314 Q1909.71 58.0738 1913.15 62.7728 Q1916.64 67.4314 1922.67 67.4314 Q1928.63 67.4314 1932.11 62.7323 Q1935.6 58.0333 1935.6 49.9314 Q1935.6 41.8701 1932.11 37.1711 Q1928.63 32.4315 1922.67 32.4315 M1922.67 26.1121 Q1932.4 26.1121 1937.95 32.4315 Q1943.5 38.7509 1943.5 49.9314 Q1943.5 61.0714 1937.95 67.4314 Q1932.4 73.7508 1922.67 73.7508 Q1912.91 73.7508 1907.36 67.4314 Q1901.85 61.0714 1901.85 49.9314 Q1901.85 38.7509 1907.36 32.4315 Q1912.91 26.1121 1922.67 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M1991.17 35.9153 Q1993.97 30.8922 1997.86 28.5022 Q2001.75 26.1121 2007.01 26.1121 Q2014.1 26.1121 2017.95 31.0947 Q2021.8 36.0368 2021.8 45.1919 L2021.8 72.576 L2014.31 72.576 L2014.31 45.4349 Q2014.31 38.913 2012 35.7533 Q2009.69 32.5936 2004.95 32.5936 Q1999.16 32.5936 1995.79 36.4419 Q1992.43 40.2903 1992.43 46.9338 L1992.43 72.576 L1984.94 72.576 L1984.94 45.4349 Q1984.94 38.8725 1982.63 35.7533 Q1980.32 32.5936 1975.5 32.5936 Q1969.79 32.5936 1966.42 36.4824 Q1963.06 40.3308 1963.06 46.9338 L1963.06 72.576 L1955.57 72.576 L1955.57 27.2059 L1963.06 27.2059 L1963.06 34.2544 Q1965.61 30.082 1969.18 28.0971 Q1972.74 26.1121 1977.64 26.1121 Q1982.59 26.1121 1986.03 28.6237 Q1989.51 31.1352 1991.17 35.9153 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2043.88 65.7705 L2043.88 89.8329 L2036.38 89.8329 L2036.38 27.2059 L2043.88 27.2059 L2043.88 34.0924 Q2046.23 30.0415 2049.79 28.0971 Q2053.4 26.1121 2058.38 26.1121 Q2066.64 26.1121 2071.79 32.6746 Q2076.97 39.2371 2076.97 49.9314 Q2076.97 60.6258 2071.79 67.1883 Q2066.64 73.7508 2058.38 73.7508 Q2053.4 73.7508 2049.79 71.8063 Q2046.23 69.8214 2043.88 65.7705 M2069.24 49.9314 Q2069.24 41.7081 2065.83 37.0496 Q2062.47 32.3505 2056.56 32.3505 Q2050.64 32.3505 2047.24 37.0496 Q2043.88 41.7081 2043.88 49.9314 Q2043.88 58.1548 2047.24 62.8538 Q2050.64 67.5124 2056.56 67.5124 Q2062.47 67.5124 2065.83 62.8538 Q2069.24 58.1548 2069.24 49.9314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2089.33 9.54393 L2096.78 9.54393 L2096.78 72.576 L2089.33 72.576 L2089.33 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2151.19 48.0275 L2151.19 51.6733 L2116.91 51.6733 Q2117.4 59.3701 2121.53 63.421 Q2125.71 67.4314 2133.12 67.4314 Q2137.41 67.4314 2141.42 66.3781 Q2145.47 65.3249 2149.44 63.2184 L2149.44 70.267 Q2145.43 71.9684 2141.22 72.8596 Q2137.01 73.7508 2132.67 73.7508 Q2121.82 73.7508 2115.46 67.4314 Q2109.14 61.1119 2109.14 50.3365 Q2109.14 39.1965 2115.13 32.6746 Q2121.17 26.1121 2131.38 26.1121 Q2140.53 26.1121 2145.84 32.0264 Q2151.19 37.9003 2151.19 48.0275 M2143.73 45.84 Q2143.65 39.7232 2140.29 36.0774 Q2136.97 32.4315 2131.46 32.4315 Q2125.22 32.4315 2121.45 35.9558 Q2117.73 39.4801 2117.16 45.8805 L2143.73 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2170.79 14.324 L2170.79 27.2059 L2186.14 27.2059 L2186.14 32.9987 L2170.79 32.9987 L2170.79 57.6282 Q2170.79 63.1779 2172.29 64.7578 Q2173.83 66.3376 2178.49 66.3376 L2186.14 66.3376 L2186.14 72.576 L2178.49 72.576 Q2169.86 72.576 2166.58 69.3758 Q2163.3 66.1351 2163.3 57.6282 L2163.3 32.9987 L2157.83 32.9987 L2157.83 27.2059 L2163.3 27.2059 L2163.3 14.324 L2170.79 14.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2234.76 48.0275 L2234.76 51.6733 L2200.49 51.6733 Q2200.97 59.3701 2205.1 63.421 Q2209.28 67.4314 2216.69 67.4314 Q2220.98 67.4314 2224.99 66.3781 Q2229.04 65.3249 2233.01 63.2184 L2233.01 70.267 Q2229 71.9684 2224.79 72.8596 Q2220.58 73.7508 2216.24 73.7508 Q2205.39 73.7508 2199.03 67.4314 Q2192.71 61.1119 2192.71 50.3365 Q2192.71 39.1965 2198.7 32.6746 Q2204.74 26.1121 2214.95 26.1121 Q2224.1 26.1121 2229.41 32.0264 Q2234.76 37.9003 2234.76 48.0275 M2227.3 45.84 Q2227.22 39.7232 2223.86 36.0774 Q2220.54 32.4315 2215.03 32.4315 Q2208.79 32.4315 2205.02 35.9558 Q2201.3 39.4801 2200.73 45.8805 L2227.3 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2248.04 62.2867 L2256.59 62.2867 L2256.59 72.576 L2248.04 72.576 L2248.04 62.2867 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip052)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  230.922,1177.04 232.984,831.678 235.046,162.047 237.109,299.938 239.171,533.579 241.233,562.587 243.295,998.871 245.357,893.055 247.419,1075.13 249.481,949.433 \n",
       "  251.543,995.996 253.605,963.217 255.667,972.662 257.73,1144.79 259.792,1093.49 261.854,1172.48 263.916,1090.65 265.978,1097.27 268.04,1235.75 270.102,1079.31 \n",
       "  272.164,1247.45 274.226,1118.73 276.288,1260.12 278.35,1104 280.413,1203.61 282.475,1185.86 284.537,1173.74 286.599,1200.44 288.661,1254.36 290.723,1219.38 \n",
       "  292.785,1263.08 294.847,1212.17 296.909,1261.26 298.971,1203.92 301.034,1293.59 303.096,1231.87 305.158,1296.47 307.22,1217.33 309.282,1291.81 311.344,1252.37 \n",
       "  313.406,1288.23 315.468,1308.3 317.53,1296.66 319.592,1294.7 321.654,1304.85 323.717,1284.6 325.779,1312.55 327.841,1275.06 329.903,1302.27 331.965,1324.4 \n",
       "  334.027,1283.8 336.089,1327.73 338.151,1276.41 340.213,1298.31 342.275,1305.72 344.338,1318.98 346.4,1322.85 348.462,1337.29 350.524,1333.12 352.586,1330.02 \n",
       "  354.648,1324.16 356.71,1348.18 358.772,1324.89 360.834,1350.78 362.896,1312.85 364.958,1360.53 367.021,1328.2 369.083,1324.08 371.145,1346.89 373.207,1351.9 \n",
       "  375.269,1347.6 377.331,1345.82 379.393,1329.77 381.455,1351.18 383.517,1331.28 385.579,1335.45 387.641,1367.88 389.704,1344.96 391.766,1341.41 393.828,1346.28 \n",
       "  395.89,1339.51 397.952,1360.48 400.014,1341.96 402.076,1375.22 404.138,1359.63 406.2,1365.56 408.262,1358.74 410.325,1379.07 412.387,1349.39 414.449,1384.37 \n",
       "  416.511,1344.38 418.573,1378.22 420.635,1363.35 422.697,1364 424.759,1361.1 426.821,1386.64 428.883,1353.16 430.945,1386.06 433.008,1338.69 435.07,1387.07 \n",
       "  437.132,1353.97 439.194,1393.71 441.256,1379.51 443.318,1399.82 445.38,1369.13 447.442,1370.51 449.504,1374.83 451.566,1392.58 453.629,1377.63 455.691,1391.08 \n",
       "  457.753,1390.73 459.815,1391.01 461.877,1380.69 463.939,1375.06 466.001,1394.63 468.063,1386.96 470.125,1372.64 472.187,1388.4 474.249,1370.35 476.312,1385.43 \n",
       "  478.374,1370.52 480.436,1386.31 482.498,1397.33 484.56,1381.11 486.622,1400.79 488.684,1399.07 490.746,1358.24 492.808,1394.63 494.87,1394.71 496.933,1390.27 \n",
       "  498.995,1375.58 501.057,1407.77 503.119,1370.6 505.181,1405.45 507.243,1373.02 509.305,1399.14 511.367,1369.2 513.429,1404.95 515.491,1347.49 517.553,1395.52 \n",
       "  519.616,1361.67 521.678,1413.78 523.74,1367.91 525.802,1385.18 527.864,1399.27 529.926,1399.84 531.988,1406.03 534.05,1386.02 536.112,1402.79 538.174,1376.42 \n",
       "  540.237,1400.43 542.299,1405.25 544.361,1402.63 546.423,1400.71 548.485,1392.57 550.547,1404.17 552.609,1399.87 554.671,1379.32 556.733,1390.29 558.795,1393.31 \n",
       "  560.857,1408.65 562.92,1405.15 564.982,1391.98 567.044,1403.26 569.106,1397.25 571.168,1393.79 573.23,1398.19 575.292,1387.08 577.354,1389.85 579.416,1395.33 \n",
       "  581.478,1393.61 583.54,1387.41 585.603,1397.53 587.665,1396.9 589.727,1400.42 591.789,1397 593.851,1402.47 595.913,1412.07 597.975,1394.59 600.037,1403.11 \n",
       "  602.099,1399.17 604.161,1415.55 606.224,1388.23 608.286,1402.54 610.348,1407.79 612.41,1402.53 614.472,1394.88 616.534,1397.27 618.596,1404.34 620.658,1391.26 \n",
       "  622.72,1412.45 624.782,1415.97 626.844,1413.51 628.907,1403.87 630.969,1402.23 633.031,1398.77 635.093,1401.32 637.155,1403.38 639.217,1409.67 641.279,1412 \n",
       "  643.341,1411.74 645.403,1399.4 647.465,1398.32 649.528,1403.22 651.59,1412.29 653.652,1399.34 655.714,1400.81 657.776,1382.87 659.838,1411.21 661.9,1414.54 \n",
       "  663.962,1412.2 666.024,1400.78 668.086,1412.66 670.148,1386.8 672.211,1403.87 674.273,1405.35 676.335,1406.42 678.397,1408.79 680.459,1416.11 682.521,1408.22 \n",
       "  684.583,1415.13 686.645,1391.52 688.707,1409.87 690.769,1400.25 692.832,1405.53 694.894,1402.82 696.956,1404.45 699.018,1408.76 701.08,1408.79 703.142,1401.46 \n",
       "  705.204,1425.3 707.266,1386.29 709.328,1411.26 711.39,1407.66 713.452,1410.29 715.515,1392.41 717.577,1416.27 719.639,1399.42 721.701,1412.4 723.763,1405.09 \n",
       "  725.825,1408.13 727.887,1398.71 729.949,1405.66 732.011,1410.16 734.073,1404.41 736.136,1399.01 738.198,1413.73 740.26,1402.63 742.322,1413.99 744.384,1405.86 \n",
       "  746.446,1412.31 748.508,1409.18 750.57,1417.58 752.632,1399.58 754.694,1410.04 756.756,1416.56 758.819,1406.48 760.881,1408.82 762.943,1400.02 765.005,1405.94 \n",
       "  767.067,1409.9 769.129,1414.64 771.191,1405.86 773.253,1409.3 775.315,1397.52 777.377,1411.17 779.439,1421.05 781.502,1412.06 783.564,1410.59 785.626,1415.84 \n",
       "  787.688,1409.31 789.75,1398.91 791.812,1407.9 793.874,1409.02 795.936,1412.19 797.998,1414.59 800.06,1405.68 802.123,1423.36 804.185,1401.62 806.247,1420.79 \n",
       "  808.309,1401.74 810.371,1423.54 812.433,1399.02 814.495,1405.45 816.557,1403.77 818.619,1407.19 820.681,1410.62 822.743,1419.51 824.806,1418.62 826.868,1405.34 \n",
       "  828.93,1412.75 830.992,1418.55 833.054,1403.77 835.116,1419.67 837.178,1413.12 839.24,1411.25 841.302,1418.24 843.364,1415.52 845.427,1407.51 847.489,1413.09 \n",
       "  849.551,1408.84 851.613,1408.09 853.675,1411.39 855.737,1409.28 857.799,1408.55 859.861,1402.09 861.923,1413.69 863.985,1387.69 866.047,1420.61 868.11,1399.97 \n",
       "  870.172,1419.98 872.234,1408.57 874.296,1424.95 876.358,1403.73 878.42,1416.43 880.482,1404.33 882.544,1418.62 884.606,1409.02 886.668,1409.19 888.731,1409.81 \n",
       "  890.793,1417.07 892.855,1391.01 894.917,1421.75 896.979,1390.92 899.041,1413.93 901.103,1410.64 903.165,1414.56 905.227,1403.99 907.289,1412.1 909.351,1411.53 \n",
       "  911.414,1416.01 913.476,1404.23 915.538,1410.13 917.6,1410.23 919.662,1416.89 921.724,1415.25 923.786,1414.81 925.848,1404.86 927.91,1425.83 929.972,1416.49 \n",
       "  932.035,1406.91 934.097,1412.91 936.159,1408.11 938.221,1419.86 940.283,1406.7 942.345,1419.29 944.407,1403.8 946.469,1416.15 948.531,1405.39 950.593,1418.47 \n",
       "  952.655,1403.71 954.718,1423.73 956.78,1408.86 958.842,1414.41 960.904,1411.02 962.966,1422.34 965.028,1415.89 967.09,1413.37 969.152,1422.32 971.214,1417.71 \n",
       "  973.276,1407.66 975.338,1417.15 977.401,1404.94 979.463,1414.76 981.525,1410.86 983.587,1414.36 985.649,1404.48 987.711,1421.89 989.773,1408.97 991.835,1412.8 \n",
       "  993.897,1411.71 995.959,1412.73 998.022,1414.94 1000.08,1412.69 1002.15,1418.61 1004.21,1411.4 1006.27,1405.03 1008.33,1417.57 1010.39,1412.8 1012.46,1414.72 \n",
       "  1014.52,1419.25 1016.58,1410.52 1018.64,1413.19 1020.7,1423.97 1022.77,1412.02 1024.83,1422.66 1026.89,1406.85 1028.95,1415.66 1031.02,1423.03 1033.08,1417.15 \n",
       "  1035.14,1417.41 1037.2,1420.43 1039.26,1420.99 1041.33,1422.04 1043.39,1409.83 1045.45,1415.55 1047.51,1417.31 1049.57,1424.93 1051.64,1405.04 1053.7,1415.95 \n",
       "  1055.76,1406.37 1057.82,1419.9 1059.88,1415.29 1061.95,1414.51 1064.01,1419.58 1066.07,1421.45 1068.13,1423.11 1070.19,1414.57 1072.26,1410.01 1074.32,1420.36 \n",
       "  1076.38,1415.55 1078.44,1417.98 1080.51,1421.31 1082.57,1420.91 1084.63,1421.13 1086.69,1416.88 1088.75,1410.47 1090.82,1414.72 1092.88,1417.94 1094.94,1413.03 \n",
       "  1097,1424.02 1099.06,1419.56 1101.13,1417.56 1103.19,1408.11 1105.25,1420.56 1107.31,1408.14 1109.37,1411.89 1111.44,1417.4 1113.5,1412.94 1115.56,1419.01 \n",
       "  1117.62,1406.94 1119.69,1427.85 1121.75,1417.17 1123.81,1416.67 1125.87,1415.14 1127.93,1421.98 1130,1411.19 1132.06,1428.77 1134.12,1405.55 1136.18,1428.71 \n",
       "  1138.24,1397.56 1140.31,1419.03 1142.37,1404.72 1144.43,1419.75 1146.49,1408.91 1148.55,1427.52 1150.62,1419.52 1152.68,1413.85 1154.74,1423.9 1156.8,1425.47 \n",
       "  1158.86,1418.16 1160.93,1419.2 1162.99,1419.21 1165.05,1411.45 1167.11,1415.07 1169.18,1403.59 1171.24,1425.52 1173.3,1409.68 1175.36,1425.41 1177.42,1412.33 \n",
       "  1179.49,1426.16 1181.55,1412.76 1183.61,1428.17 1185.67,1411.26 1187.73,1430.22 1189.8,1420.1 1191.86,1418.88 1193.92,1433.01 1195.98,1402.76 1198.04,1422.34 \n",
       "  1200.11,1415.16 1202.17,1414.97 1204.23,1418.78 1206.29,1408.9 1208.36,1427.11 1210.42,1413.91 1212.48,1422.79 1214.54,1419.37 1216.6,1423.87 1218.67,1411.07 \n",
       "  1220.73,1415.94 1222.79,1412.2 1224.85,1413.33 1226.91,1422.61 1228.98,1420.06 1231.04,1419.17 1233.1,1423.04 1235.16,1424.11 1237.22,1424.6 1239.29,1408.53 \n",
       "  1241.35,1431.41 1243.41,1410.71 1245.47,1422.73 1247.53,1414.86 1249.6,1427.71 1251.66,1420.01 1253.72,1410.7 1255.78,1416.32 1257.85,1414.36 1259.91,1417.54 \n",
       "  1261.97,1414.4 1264.03,1424.62 1266.09,1412.37 1268.16,1420.02 1270.22,1420.12 1272.28,1420.97 1274.34,1425.85 1276.4,1415.84 1278.47,1416.91 1280.53,1428.15 \n",
       "  1282.59,1428.6 1284.65,1419.78 1286.71,1413.32 1288.78,1436.76 1290.84,1417.81 1292.9,1432.35 1294.96,1419.33 1297.03,1414.26 1299.09,1438.74 1301.15,1424.46 \n",
       "  1303.21,1422 1305.27,1433.88 1307.34,1398.29 1309.4,1427.72 1311.46,1421.4 1313.52,1417.11 1315.58,1403.82 1317.65,1424.31 1319.71,1419.02 1321.77,1410.86 \n",
       "  1323.83,1414.4 1325.89,1417.45 1327.96,1415.64 1330.02,1419.17 1332.08,1419.43 1334.14,1416.18 1336.21,1418.1 1338.27,1420.43 1340.33,1409.07 1342.39,1429.07 \n",
       "  1344.45,1427.16 1346.52,1411.25 1348.58,1405.38 1350.64,1415.8 1352.7,1418.36 1354.76,1415.26 1356.83,1412.83 1358.89,1425.51 1360.95,1417.3 1363.01,1423.53 \n",
       "  1365.07,1417.05 1367.14,1419.98 1369.2,1432.45 1371.26,1418.08 1373.32,1432.7 1375.38,1419.32 1377.45,1404.91 1379.51,1428.94 1381.57,1409 1383.63,1429.43 \n",
       "  1385.7,1413.01 1387.76,1411.11 1389.82,1426.98 1391.88,1421.76 1393.94,1422.06 1396.01,1416.99 1398.07,1424.71 1400.13,1419.51 1402.19,1431.78 1404.25,1428.59 \n",
       "  1406.32,1420.14 1408.38,1422.99 1410.44,1405.45 1412.5,1421.1 1414.56,1408.42 1416.63,1416.29 1418.69,1411.41 1420.75,1429.31 1422.81,1422.44 1424.88,1419.92 \n",
       "  1426.94,1419.69 1429,1430.19 1431.06,1415.92 1433.12,1428.17 1435.19,1422.63 1437.25,1420.42 1439.31,1414.94 1441.37,1427.88 1443.43,1416.57 1445.5,1434.82 \n",
       "  1447.56,1418.44 1449.62,1426.98 1451.68,1426.43 1453.74,1428.69 1455.81,1411.65 1457.87,1421.58 1459.93,1424.01 1461.99,1422.48 1464.05,1418.43 1466.12,1421.82 \n",
       "  1468.18,1413.29 1470.24,1418.68 1472.3,1416.37 1474.37,1428.99 1476.43,1419 1478.49,1428.42 1480.55,1419.98 1482.61,1430.78 1484.68,1421.94 1486.74,1443.36 \n",
       "  1488.8,1415.51 1490.86,1427.85 1492.92,1412.87 1494.99,1414.82 1497.05,1419.31 1499.11,1427.12 1501.17,1428.72 1503.23,1424.53 1505.3,1431.2 1507.36,1423.49 \n",
       "  1509.42,1427.99 1511.48,1422.04 1513.55,1427.23 1515.61,1429.24 1517.67,1426.06 1519.73,1438.42 1521.79,1426.42 1523.86,1414 1525.92,1414.35 1527.98,1423.19 \n",
       "  1530.04,1424.01 1532.1,1433.74 1534.17,1418.48 1536.23,1431.33 1538.29,1417.99 1540.35,1427.84 1542.41,1422.03 1544.48,1425.34 1546.54,1415.57 1548.6,1428.79 \n",
       "  1550.66,1411.03 1552.73,1431.82 1554.79,1425.31 1556.85,1421.73 1558.91,1425.03 1560.97,1427.25 1563.04,1420.79 1565.1,1431.54 1567.16,1426.48 1569.22,1419.43 \n",
       "  1571.28,1426.83 1573.35,1416.88 1575.41,1419.99 1577.47,1413.66 1579.53,1423.57 1581.59,1421.2 1583.66,1435.84 1585.72,1413.72 1587.78,1432.23 1589.84,1425.73 \n",
       "  1591.9,1422.13 1593.97,1432.49 1596.03,1416.77 1598.09,1420.75 1600.15,1425.51 1602.22,1418.95 1604.28,1426.71 1606.34,1418.49 1608.4,1424.25 1610.46,1418.96 \n",
       "  1612.53,1426.58 1614.59,1425.08 1616.65,1419.26 1618.71,1428.53 1620.77,1434.01 1622.84,1421.12 1624.9,1436.55 1626.96,1424.81 1629.02,1439.53 1631.08,1422.1 \n",
       "  1633.15,1434.28 1635.21,1432.36 1637.27,1433.98 1639.33,1428.71 1641.4,1425.57 1643.46,1427.44 1645.52,1427.46 1647.58,1425.97 1649.64,1417.29 1651.71,1424.78 \n",
       "  1653.77,1418.67 1655.83,1425.41 1657.89,1424.91 1659.95,1427.71 1662.02,1426.77 1664.08,1430.21 1666.14,1435.74 1668.2,1420.27 1670.26,1436.38 1672.33,1424.44 \n",
       "  1674.39,1438.54 1676.45,1415.2 1678.51,1434.99 1680.57,1411.39 1682.64,1446.44 1684.7,1405.22 1686.76,1428.74 1688.82,1408.16 1690.89,1440.89 1692.95,1414.75 \n",
       "  1695.01,1423.55 1697.07,1419.34 1699.13,1438.35 1701.2,1423.98 1703.26,1434.88 1705.32,1417.18 1707.38,1433.86 1709.44,1419.98 1711.51,1432.88 1713.57,1437.93 \n",
       "  1715.63,1432.7 1717.69,1438.85 1719.75,1418.08 1721.82,1416.35 1723.88,1428.76 1725.94,1427.7 1728,1417.38 1730.07,1420.96 1732.13,1428.4 1734.19,1424.64 \n",
       "  1736.25,1424.2 1738.31,1423.56 1740.38,1420.19 1742.44,1429.29 1744.5,1420.56 1746.56,1437.36 1748.62,1418.49 1750.69,1427.7 1752.75,1427.91 1754.81,1440.66 \n",
       "  1756.87,1422.38 1758.93,1426.6 1761,1423.82 1763.06,1425.27 1765.12,1411.38 1767.18,1422.66 1769.24,1417.97 1771.31,1431.1 1773.37,1418.41 1775.43,1431.1 \n",
       "  1777.49,1420.24 1779.56,1430.52 1781.62,1431.6 1783.68,1436.82 1785.74,1421.42 1787.8,1444.76 1789.87,1416.27 1791.93,1431.19 1793.99,1409.7 1796.05,1436.59 \n",
       "  1798.11,1415.11 1800.18,1433.09 1802.24,1425.68 1804.3,1429.92 1806.36,1418.28 1808.42,1443.43 1810.49,1432.69 1812.55,1430.01 1814.61,1411.55 1816.67,1430.95 \n",
       "  1818.74,1418.38 1820.8,1422.58 1822.86,1433.49 1824.92,1429.07 1826.98,1430.09 1829.05,1427.25 1831.11,1431.01 1833.17,1428.68 1835.23,1437.48 1837.29,1429.46 \n",
       "  1839.36,1426.15 1841.42,1423.75 1843.48,1430.57 1845.54,1427.98 1847.6,1434.8 1849.67,1429.53 1851.73,1429.92 1853.79,1435.71 1855.85,1433.58 1857.92,1423.05 \n",
       "  1859.98,1438.71 1862.04,1437.08 1864.1,1435.04 1866.16,1435.1 1868.23,1428.19 1870.29,1417.91 1872.35,1428.6 1874.41,1425.34 1876.47,1431.82 1878.54,1419.56 \n",
       "  1880.6,1438.07 1882.66,1429.31 1884.72,1436.38 1886.78,1428.72 1888.85,1435.72 1890.91,1428.44 1892.97,1442.28 1895.03,1419.39 1897.09,1431.14 1899.16,1439.47 \n",
       "  1901.22,1426.68 1903.28,1426.43 1905.34,1430.45 1907.41,1422.8 1909.47,1430.03 1911.53,1434.78 1913.59,1424.96 1915.65,1433.51 1917.72,1434.96 1919.78,1432.23 \n",
       "  1921.84,1428.79 1923.9,1424.82 1925.96,1428.48 1928.03,1427.35 1930.09,1424.78 1932.15,1437 1934.21,1434.69 1936.27,1437.34 1938.34,1431.41 1940.4,1432.32 \n",
       "  1942.46,1429.81 1944.52,1426.21 1946.59,1423.53 1948.65,1437 1950.71,1426.36 1952.77,1424.24 1954.83,1438.63 1956.9,1412.85 1958.96,1425.5 1961.02,1422.39 \n",
       "  1963.08,1438.45 1965.14,1417.34 1967.21,1433.28 1969.27,1426.75 1971.33,1426.83 1973.39,1429.16 1975.45,1441.29 1977.52,1422.95 1979.58,1434.93 1981.64,1420.65 \n",
       "  1983.7,1435.79 1985.76,1431.28 1987.83,1439.92 1989.89,1419.25 1991.95,1435.49 1994.01,1431.71 1996.08,1428.27 1998.14,1433.06 2000.2,1433.1 2002.26,1437.14 \n",
       "  2004.32,1431.6 2006.39,1441.03 2008.45,1422.88 2010.51,1420.43 2012.57,1424.43 2014.63,1433.52 2016.7,1432.39 2018.76,1420.51 2020.82,1429.26 2022.88,1435.27 \n",
       "  2024.94,1437.51 2027.01,1437.47 2029.07,1427.9 2031.13,1439.07 2033.19,1432.28 2035.26,1428.86 2037.32,1422.58 2039.38,1439.59 2041.44,1435.23 2043.5,1426.52 \n",
       "  2045.57,1413.63 2047.63,1424.96 2049.69,1426.63 2051.75,1438.2 2053.81,1427 2055.88,1431.33 2057.94,1423.65 2060,1436.38 2062.06,1423.29 2064.12,1439.58 \n",
       "  2066.19,1429.31 2068.25,1433.34 2070.31,1432.68 2072.37,1442.45 2074.43,1427.91 2076.5,1447.87 2078.56,1424.17 2080.62,1427.6 2082.68,1421.74 2084.75,1430.06 \n",
       "  2086.81,1421.75 2088.87,1439.65 2090.93,1433.03 2092.99,1436.62 2095.06,1428.88 2097.12,1433.04 2099.18,1438.63 2101.24,1427.09 2103.3,1431.39 2105.37,1435.59 \n",
       "  2107.43,1431.45 2109.49,1419.96 2111.55,1436.49 2113.61,1414.32 2115.68,1439.49 2117.74,1437.85 2119.8,1436.28 2121.86,1426.14 2123.93,1434.98 2125.99,1424.3 \n",
       "  2128.05,1428.13 2130.11,1425.88 2132.17,1425.3 2134.24,1436.16 2136.3,1426.57 2138.36,1425.19 2140.42,1432.49 2142.48,1436.97 2144.55,1424.55 2146.61,1438.81 \n",
       "  2148.67,1442.78 2150.73,1437.11 2152.79,1441.39 2154.86,1438.75 2156.92,1434.43 2158.98,1442.11 2161.04,1424.08 2163.11,1430.63 2165.17,1435.53 2167.23,1426.95 \n",
       "  2169.29,1430.22 2171.35,1437.54 2173.42,1439.12 2175.48,1433.55 2177.54,1439.9 2179.6,1433.24 2181.66,1434.33 2183.73,1443.48 2185.79,1437.74 2187.85,1437.54 \n",
       "  2189.91,1442.82 2191.97,1429.02 2194.04,1438.09 2196.1,1434.64 2198.16,1431.1 2200.22,1429.43 2202.28,1425.68 2204.35,1435.62 2206.41,1434.04 2208.47,1440.47 \n",
       "  2210.53,1437.4 2212.6,1420.57 2214.66,1444.12 2216.72,1430.8 2218.78,1438.76 2220.84,1431.68 2222.91,1434.12 2224.97,1432.48 2227.03,1429.22 2229.09,1428.39 \n",
       "  2231.15,1441.03 2233.22,1433.13 2235.28,1434.12 2237.34,1435.01 2239.4,1426.78 2241.46,1434.49 2243.53,1428.85 2245.59,1431.63 2247.65,1426.01 2249.71,1434.42 \n",
       "  2251.78,1429.56 2253.84,1440.4 2255.9,1426.64 2257.96,1440.81 2260.02,1423.2 2262.09,1437.79 2264.15,1434.49 2266.21,1437.69 2268.27,1431.93 2270.33,1436.86 \n",
       "  2272.4,1429.16 2274.46,1429.45 2276.52,1420.12 2278.58,1442.99 2280.64,1441.4 2282.71,1439.82 2284.77,1429.62 2286.83,1431.97 2288.89,1426 2290.95,1435.47 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip050)\" d=\"\n",
       "M1964.59 272.585 L2279.97 272.585 L2279.97 168.905 L1964.59 168.905  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1964.59,272.585 2279.97,272.585 2279.97,168.905 1964.59,168.905 1964.59,272.585 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip050)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1988.85,220.745 2134.43,220.745 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip050)\" d=\"M2158.69 203.465 L2163.37 203.465 L2163.37 234.089 L2180.2 234.089 L2180.2 238.025 L2158.69 238.025 L2158.69 203.465 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2194.13 215.085 Q2190.71 215.085 2188.72 217.77 Q2186.72 220.432 2186.72 225.085 Q2186.72 229.738 2188.69 232.423 Q2190.68 235.085 2194.13 235.085 Q2197.53 235.085 2199.53 232.4 Q2201.52 229.715 2201.52 225.085 Q2201.52 220.478 2199.53 217.793 Q2197.53 215.085 2194.13 215.085 M2194.13 211.474 Q2199.69 211.474 2202.86 215.085 Q2206.03 218.696 2206.03 225.085 Q2206.03 231.451 2202.86 235.085 Q2199.69 238.696 2194.13 238.696 Q2188.55 238.696 2185.38 235.085 Q2182.23 231.451 2182.23 225.085 Q2182.23 218.696 2185.38 215.085 Q2188.55 211.474 2194.13 211.474 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2229.62 212.863 L2229.62 216.891 Q2227.81 215.965 2225.87 215.502 Q2223.92 215.039 2221.84 215.039 Q2218.67 215.039 2217.07 216.011 Q2215.5 216.983 2215.5 218.928 Q2215.5 220.409 2216.63 221.265 Q2217.77 222.099 2221.19 222.863 L2222.65 223.187 Q2227.19 224.159 2229.09 225.941 Q2231.01 227.701 2231.01 230.872 Q2231.01 234.483 2228.14 236.589 Q2225.29 238.696 2220.29 238.696 Q2218.21 238.696 2215.94 238.279 Q2213.69 237.886 2211.19 237.076 L2211.19 232.677 Q2213.55 233.904 2215.84 234.529 Q2218.14 235.131 2220.38 235.131 Q2223.39 235.131 2225.01 234.113 Q2226.63 233.071 2226.63 231.196 Q2226.63 229.46 2225.45 228.534 Q2224.29 227.608 2220.34 226.752 L2218.85 226.404 Q2214.9 225.571 2213.14 223.858 Q2211.38 222.122 2211.38 219.113 Q2211.38 215.455 2213.97 213.465 Q2216.56 211.474 2221.33 211.474 Q2223.69 211.474 2225.78 211.821 Q2227.86 212.168 2229.62 212.863 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip050)\" d=\"M2254.32 212.863 L2254.32 216.891 Q2252.51 215.965 2250.57 215.502 Q2248.62 215.039 2246.54 215.039 Q2243.37 215.039 2241.77 216.011 Q2240.2 216.983 2240.2 218.928 Q2240.2 220.409 2241.33 221.265 Q2242.46 222.099 2245.89 222.863 L2247.35 223.187 Q2251.89 224.159 2253.78 225.941 Q2255.71 227.701 2255.71 230.872 Q2255.71 234.483 2252.84 236.589 Q2249.99 238.696 2244.99 238.696 Q2242.9 238.696 2240.64 238.279 Q2238.39 237.886 2235.89 237.076 L2235.89 232.677 Q2238.25 233.904 2240.54 234.529 Q2242.84 235.131 2245.08 235.131 Q2248.09 235.131 2249.71 234.113 Q2251.33 233.071 2251.33 231.196 Q2251.33 229.46 2250.15 228.534 Q2248.99 227.608 2245.03 226.752 L2243.55 226.404 Q2239.59 225.571 2237.84 223.858 Q2236.08 222.122 2236.08 219.113 Q2236.08 215.455 2238.67 213.465 Q2241.26 211.474 2246.03 211.474 Q2248.39 211.474 2250.47 211.821 Q2252.56 212.168 2254.32 212.863 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398.394686 seconds (164.21 M allocations: 617.461 GiB, 3.85% gc time, 14.82% compilation time: 4% of which was recompilation)\n"
     ]
    }
   ],
   "source": [
    "epoch_data() = Flux.DataLoader((encoded_x, encoded_y); batchsize=10, shuffle=true)\n",
    "sample_data() = (d = rand(collect(keys(amino_codon)), 10) ; (string_split.(create_genome(d)), d))\n",
    "\n",
    "nepochs = 1000\n",
    "L = []\n",
    "using Plots\n",
    "# @time for e in 1:nepochs\n",
    "#     data = epoch_data()\n",
    "#     for dat in data\n",
    "#         grad = gradient(ps) do\n",
    "#             loss(dat...)            \n",
    "#         end\n",
    "#         push!(L, loss(dat...))\n",
    "#         IJulia.clear_output(true)\n",
    "#         Flux.update!(ADAM(1e-4), ps, grad)\n",
    "#         #ylim=(0,maximum(L)),\n",
    "#         display(plot(L, title=\"Training Epoch $(e) of $(nepochs): $(round(e/nepochs*100, digits=3))% complete.\", label=\"Loss\"))\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# using Plots\n",
    "@time for e in 1:nepochs\n",
    "    data = Transformers.Datasets.batched([sample_data() for i = 1:32])\n",
    "    x, y = pre_process.(data[1]), pre_process.(data[2])\n",
    "    x, y = tokenizer_x(x), tokenizer_y(y)\n",
    "    grad = gradient(ps) do\n",
    "        loss(x, y)\n",
    "    end\n",
    "    push!(L, loss(x, y))\n",
    "    IJulia.clear_output(true)\n",
    "    Flux.update!(ADAM(1e-4), ps, grad)\n",
    "    #lim=(minimum(L),maximum(L)),\n",
    "    display(plot(L, title=\"Training Epoch $(e) of $(nepochs): $(round(e/nepochs*100, digits=3))% complete.\", label=\"Loss\"))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008dd879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function transcribe_protein(x)\n",
    "    #seq = [tokenizer_y(\"1\")]\n",
    "    tok = [\"1\"]\n",
    "    enc = Tencoder(x)\n",
    "    for i = 1:2*length(x)\n",
    "        dec = Tdecoder(tokenizer_y(tok), enc)\n",
    "        toknext = Flux.onecold(collect(dec), labels_y)\n",
    "        push!(tok, toknext[end])\n",
    "        #push!(seq, tokenizer_y(toknext[end]))\n",
    "        tok[end] == \"9\" && break\n",
    "    end\n",
    "    tok\n",
    "end\n",
    "\n",
    "function pointwise_accuracy(xtrain, ytrain)\n",
    "    global_acc = 0\n",
    "    @showprogress for i in 1:minimum([25, length(xtrain)])\n",
    "        xi = transcribe_protein(xtrain[i])\n",
    "        yi = pre_process(ytrain[i])\n",
    "        trunc = minimum([length(xi), length(yi)])\n",
    "        local_acc = sum( xi[1:trunc] .== yi[1:trunc] ) / trunc\n",
    "        global_acc += local_acc\n",
    "    end\n",
    "    return global_acc / length(xtrain)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38228050",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(pointwise_accuracy(tokenizer_x.(pre_process.(train_x)), train_y))\n",
    "display(length.([transcribe_protein(encoded_x[1]), pre_process(train_y[1])]))\n",
    "println(\n",
    "[transcribe_protein(encoded_x[1]), pre_process(train_y[1])],\n",
    "[transcribe_protein(encoded_x[2]), pre_process(train_y[2])]\n",
    "    )\n",
    "\n",
    "#[display(Tencoder(i[2:end])) for i in encoded_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a740c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise_accuracy(tokenizer_x.(pre_process.(test_x)), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tencoder.(encoded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e436c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

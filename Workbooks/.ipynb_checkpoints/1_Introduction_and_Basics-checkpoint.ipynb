{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa630eea",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Deep Learning is a subset of learning of Machine Learning which itself may be considered a subset of Statistics. In its essence Deep Learning aims to achieve matching between two statistical distributions through a powerful graphical structure: the *neural network*. The deep in Deep Learning refers to the number of layers in the graph, and this will become clearer as we proceed through the course, but has to come to more generally imply the size of the models being used.\n",
    "\n",
    "This course will address several topics: we will cover the basic theory and history of neural networks; the primitive structures and functions found in a neural network; how to train a neural network; key methods of optimising training; different computational frameworks to work in; and several modern network topologies and the situations where they are best used. These topics will be addressed through the lecture notes and a series of workbooks which can be worked through:\n",
    "\n",
    "1. Introduction and Basics\n",
    "2. Hopfield Networks, and Multi-Layer Perceptrons\n",
    "3. Gradient descent, accelerated descent, and regularisation.\n",
    "4. Flux: Deep Learning in Julia\n",
    "5. PyTorch, Keras, and Tensor Flow: Deep Learning in Python\n",
    "6. Recurrent Neural Networks\n",
    "7. Convolutional Neural Networks\n",
    "8. Variational Autoencoders\n",
    "9. Generative Adversial Networks\n",
    "10. Transformer Networks\n",
    "11. Graph Neural Networks and beyond...\n",
    "\n",
    "This notebook will cover the basics: the biology, some of the basic building blocks, some essential terms and concepts, and some history. By the end of the notebook we should understand what a neuron is and why collections of model neurons may be able to peform learning tasks. We will code our own learning neuron: the perceptron.\n",
    "\n",
    "## Where are we going?\n",
    "\n",
    "Before we start the course in earnest, we will give a simple example about what we are aiming for: how a neural network can look and what it might do. The cell block below contains code to load in a database and train a neural network to perform classification on unseen data *from scratch*. If you have a GPU it's recommended that you run the function with the keyword ``train=true``. If you do not, don't worry, the code will load in the necessary pre-trained parameters and you can imagine that it was just trained very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Plots, \n",
    "\n",
    "function example_classifier(training_data, training_labels, nepochs; train=false)\n",
    "\n",
    "    if train \n",
    "        network = Chain(Conv, Conv, Conv) |> gpu\n",
    "        loss(x, y) = \n",
    "        grads = gradient(loss, )\n",
    "        opt = ADAM()\n",
    "\n",
    "\n",
    "        @showprogress for t = 1:(nepochs*length(data))\n",
    "            train!()\n",
    "        end\n",
    "    else\n",
    "        load()\n",
    "    end\n",
    "    \n",
    "    function classifier(datum, class_labels)\n",
    "        probs = softmax(network(dataum))\n",
    "        predictor, predict_prob = findmax(probs)\n",
    "        return datum, class_labels[predictor], \"Prediction probability: $(predict_prob)\"\n",
    "    end\n",
    "    \n",
    "    return classifier\n",
    "end\n",
    "\n",
    "train_data = load()\n",
    "train_labels = load()\n",
    "nepochs = 100\n",
    "\n",
    "predict = example_classifier(train_data, train_labels, nepochs; train=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6fe536",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load()\n",
    "test_labels = load()\n",
    "for i in test_data\n",
    "    println(predict(i))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8764b20",
   "metadata": {},
   "source": [
    "## Biology\n",
    "\n",
    "Deep Learning is an offshoot of Theoretical Neuroscience: a field dedicated to modelling and explaining brain related phenomena. Its fundamental units, neurons, and fundamental topographic structure, networks, come together to form the principal tool in the deep learning playbook: neural networks. The etymology of the words suggest a relationship to physical structures found in the brain and this is no accident. Deep Learning has borrowed heavily from insights generated by explanations of experiments performed on the brain.\n",
    "\n",
    "### Neurons\n",
    "\n",
    "For many, the neuron is the fundamental brain unit: an analogy might be to the atom in chemistry. This is a rather simplistic view that is largely wrong, but it is nevertheless a useful starting point to begin developing a mental model of how the brain may work. We will present a somewhat simplified view of neurons to begin with.\n",
    "\n",
    "At its most primitive level a neuron is nothing more than a specialised *cell* that specialises in its ability to develop long-range connections and communicate with other cells. It can be described with a few fundamental structures: the soma (cell body), the axon, and dendrites. These three structures work together to generate and communicate signalling patterns in the form of events called *action potentials* or *spikes*. \n",
    "\n",
    "An action potential is a wave of change in the membrane voltage of a cell. When a cells membrane voltage releases a certain level it opens gated ionic channels which cause a rapid increase in the potential up until a peak where it decays and enters a refractory period; see [Image x]. This wave of voltage modulates the voltage in the membrane patch directly next to it which allows the action potential to be *transmitted* from the soma down the axon and to another cell. The axon terminates at a location called the *synapse* which bridges the axon of one cell with the dendrites of another cell. When the action potential reaches the synapses it triggers a release of vesicles known as *neurotransmitters* which can modulate the potential at the corresponding dendrite. This can be up-modulation or excitatory causing the neighbouring cell to be more likely to fire, or down modulation or *inhibitory* causing the neighbouring cell to be less likely to fire. Neurons are often referred to as excitatory/inhibitory for this reason. Therefore, in tandem these three structures work (in conjunction with other electrical inputs such as stimulus from the sensory organs) to modulate spiking in themselves and other neurons. These patterns of spiking convey information and perform computation e.g. a high spiking rate in a muscular neuron might cause a muscle to contract.\n",
    "\n",
    "Neurons are themselves categorised into specialised subdivisions. The beginning of this classification is often regarded as the beginning of modern neuroscience with Raman y Cajal producing a series of beautiful drawings of stained neurons. There are \n",
    "\n",
    "\n",
    "### Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5a435",
   "metadata": {},
   "source": [
    "\n",
    "1. Classification\n",
    "2. Regression\n",
    "    * Linear Regression\n",
    "    * Weights and Biases\n",
    "3. Models\n",
    "4. Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d454e3",
   "metadata": {},
   "source": [
    "## Mathematical Concepts\n",
    "\n",
    "\n",
    "### Models\n",
    "\n",
    "A model is simply a reduced explanation of some phenomena that generates insight about that phenomena. We can choose any format to outline our model in: words, equations, computational routines. Mathematical and computational models are desirable because they are *precise*. There is abosuletly no ambiguity about what they mean.\n",
    "\n",
    "### Neurone Models\n",
    "\n",
    "#### Biological Models\n",
    "\n",
    "#### Integrate and Fire Models\n",
    "\n",
    "#### Poisson Models\n",
    "\n",
    "#### Activation functions\n",
    "\n",
    "### Network Models\n",
    "\n",
    "### Statistical Models\n",
    "\n",
    "### Classification, Regression, and Generation\n",
    "\n",
    "\n",
    "#### Classification\n",
    "\n",
    "#### Regression\n",
    "\n",
    "#### Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb083fe",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "We now would like to unify our biology and our statistical modelling paradigms. We note that, in a simplisitic sense, a neurons firing rate output can be modelled with a logisitic function. It can therefore be thought of as performing a classification, or some form of logisitic regression. We also note that a neurons output is dependent on the sum of its weighted inputs that arrive through the dendrites and some internal resting state. We can call these dendritic weights the weights $W$ and the resting points the biases $b$. We can write this as a function:\n",
    "\n",
    "$$ u_i = \\sigma\\left(\\sum_jW_{ij} v_j + b_i \\right) $$\n",
    "\n",
    "with $\\sigma$ being the activation function and $v_j$ being the input to the network. When the activation function is a simple thresholding function (i.e. 1 above the threshold and 0 otherwise) we refer to it as a *perceptron* and it is one of the earliest forms of neurally inspired machine learning models. We can draw an immediate analogy to our statistical model and say that the neuron is performing the role of a classifier: when the firing threshold is crossed the neuron is activated and we classify the input as a different type. By tuning the dendritic weights and the baseline rate we can do biological statistics. \n",
    "\n",
    "Assume that the data for the input $v_j$ is $d_i$. The weights are tuned according to a very simple rule:\n",
    "\n",
    "$$W_{ij}(t+1) = W_{ij}(t) + r (u_i - d_i) v_j$$.\n",
    "\n",
    "If you have done a course in linear regression you might immediately realise this as a gradient descent on the square of the errors i.e. we are minimising the mean squared error of the classification data. Alternatively, if you are familiar with biological learning you might understand this as a proxy of a expectation-reward scheme: when a neuron is presented with a divergent output to what is expected neurotransmitters are released which change the weights to move the output closer to what is expected. We can naturally extend this definition to a series of outputs $i \\in 1:N$ and perform the classification in a higher dimensional decision space. These procedures extend analagously. \n",
    "\n",
    "We therefore can see that the neurone (or perceptron) provides a powerful classification or regresion scheme with a natural biological motivation. It unifies very well with our models of linear regressors and classifiers and can be understood through a well-studied statistical lense. Let's train a perceptron on a simple classification task that an early human might have had to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed656411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Matrix{Float64}}:\n",
       " [3.17844 3.98878 ‚Ä¶ 3.55384 3.37955]\n",
       " [4.60585 3.96469 ‚Ä¶ 4.63704 4.6018]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "domestic_class = ['üêà', 'üêÄ', 'üêî', 'üêï']\n",
    "wild_class = ['ü¶•', 'üêó', 'ü¶ä', 'ü¶ì', 'ü¶ò', 'ü¶â', 'ü¶Ñ']\n",
    "domestic = [[0.740562, 0.74002, 1.15348, 0.773644], [0.32662, 0.835877,  1.04288,  1.04741]]\n",
    "wild = [[3.17844  3.98878  3.19889  3.92041  3.10834  3.55384  3.37955], [4.60585  3.96469  4.68477  3.40337  3.21203  4.63704  4.6018]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76b295c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "InexactError",
     "evalue": "InexactError: trunc(UInt16, 128008)",
     "output_type": "error",
     "traceback": [
      "InexactError: trunc(UInt16, 128008)",
      "",
      "Stacktrace:",
      "  [1] throw_inexacterror(f::Symbol, #unused#::Type{UInt16}, val::UInt32)",
      "    @ Core ./boot.jl:614",
      "  [2] checked_trunc_uint",
      "    @ ./boot.jl:644 [inlined]",
      "  [3] toUInt16",
      "    @ ./boot.jl:721 [inlined]",
      "  [4] UInt16",
      "    @ ./boot.jl:767 [inlined]",
      "  [5] UInt16(x::Char)",
      "    @ Base ./char.jl:50",
      "  [6] char_point!(c::UnicodePlots.AsciiCanvas{typeof(identity), typeof(identity)}, char_x::Int64, char_y::Int64, char::Char, color::Symbol)",
      "    @ UnicodePlots ~/.julia/packages/UnicodePlots/kqgQ0/src/canvas.jl:48",
      "  [7] annotate!(c::UnicodePlots.AsciiCanvas{typeof(identity), typeof(identity)}, x::Float64, y::Float64, text::String, color::Symbol; halign::Symbol, valign::Symbol)",
      "    @ UnicodePlots ~/.julia/packages/UnicodePlots/kqgQ0/src/canvas.jl:330",
      "  [8] #annotate!#83",
      "    @ ~/.julia/packages/UnicodePlots/kqgQ0/src/plot.jl:500 [inlined]",
      "  [9] _before_layout_calcs(plt::Plots.Plot{Plots.UnicodePlotsBackend})",
      "    @ Plots ~/.julia/packages/Plots/W75kY/src/backends/unicodeplots.jl:133",
      " [10] prepare_output(plt::Plots.Plot{Plots.UnicodePlotsBackend})",
      "    @ Plots ~/.julia/packages/Plots/W75kY/src/plot.jl:241",
      " [11] _show(io::IOBuffer, #unused#::MIME{Symbol(\"text/plain\")}, plt::Plots.Plot{Plots.UnicodePlotsBackend})",
      "    @ Plots ~/.julia/packages/Plots/W75kY/src/backends/unicodeplots.jl:326",
      " [12] show",
      "    @ ~/.julia/packages/Plots/W75kY/src/backends/unicodeplots.jl:322 [inlined]",
      " [13] show(io::IOBuffer, m::MIME{Symbol(\"text/plain\")}, plt::Plots.Plot{Plots.UnicodePlotsBackend})",
      "    @ Plots ~/.julia/packages/Plots/W75kY/src/output.jl:208",
      " [14] sprint(::Function, ::MIME{Symbol(\"text/plain\")}, ::Vararg{Any}; context::Nothing, sizehint::Int64)",
      "    @ Base ./strings/io.jl:114",
      " [15] sprint",
      "    @ ./strings/io.jl:107 [inlined]",
      " [16] _ijulia_display_dict(plt::Plots.Plot{Plots.UnicodePlotsBackend})",
      "    @ Plots ~/.julia/packages/Plots/W75kY/src/ijulia.jl:41",
      " [17] display_dict(plt::Plots.Plot{Plots.UnicodePlotsBackend})",
      "    @ Plots ~/.julia/packages/Plots/W75kY/src/init.jl:114",
      " [18] #invokelatest#2",
      "    @ ./essentials.jl:729 [inlined]",
      " [19] invokelatest",
      "    @ ./essentials.jl:726 [inlined]",
      " [20] execute_request(socket::ZMQ.Socket, msg::IJulia.Msg)",
      "    @ IJulia ~/.julia/packages/IJulia/AQu2H/src/execute_request.jl:112",
      " [21] #invokelatest#2",
      "    @ ./essentials.jl:729 [inlined]",
      " [22] invokelatest",
      "    @ ./essentials.jl:726 [inlined]",
      " [23] eventloop(socket::ZMQ.Socket)",
      "    @ IJulia ~/.julia/packages/IJulia/AQu2H/src/eventloop.jl:8",
      " [24] (::IJulia.var\"#15#18\")()",
      "    @ IJulia ./task.jl:484"
     ]
    }
   ],
   "source": [
    "unicodeplots()\n",
    "p = plot(; title=\"Animals\")\n",
    "for i in 1:length(domestic[1])\n",
    "    annotate!(p, (domestic[1][i], domestic[2][i], text(domestic_class[i])))\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "374fac7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MarchingCubes ‚îÄ‚îÄ‚îÄ‚îÄ v0.1.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ConstructionBase ‚îÄ v1.4.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Unitful ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ v1.12.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FreeType ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ v4.0.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FileIO ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ v1.16.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m UnicodePlots ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ v3.1.6\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.8/Project.toml`\n",
      " \u001b[90m [b8865327] \u001b[39m\u001b[92m+ UnicodePlots v3.1.6\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.8/Manifest.toml`\n",
      " \u001b[90m [187b0558] \u001b[39m\u001b[92m+ ConstructionBase v1.4.1\u001b[39m\n",
      " \u001b[90m [5789e2e9] \u001b[39m\u001b[92m+ FileIO v1.16.0\u001b[39m\n",
      " \u001b[90m [b38be410] \u001b[39m\u001b[92m+ FreeType v4.0.0\u001b[39m\n",
      " \u001b[90m [299715c1] \u001b[39m\u001b[92m+ MarchingCubes v0.1.4\u001b[39m\n",
      " \u001b[90m [b8865327] \u001b[39m\u001b[92m+ UnicodePlots v3.1.6\u001b[39m\n",
      " \u001b[90m [1986cc42] \u001b[39m\u001b[92m+ Unitful v1.12.0\u001b[39m\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ‚úì \u001b[39m\u001b[90mConstructionBase\u001b[39m\n",
      "\u001b[32m  ‚úì \u001b[39m\u001b[90mFreeType\u001b[39m\n",
      "\u001b[32m  ‚úì \u001b[39m\u001b[90mFileIO\u001b[39m\n",
      "\u001b[32m  ‚úì \u001b[39m\u001b[90mMarchingCubes\u001b[39m\n",
      "\u001b[32m  ‚úì \u001b[39m\u001b[90mUnitful\u001b[39m\n",
      "\u001b[32m  ‚úì \u001b[39mUnicodePlots\n",
      "  6 dependencies successfully precompiled in 63 seconds. 208 already precompiled.\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"UnicodePlots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99fab435",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3ecb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff57ecf",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "Graph neural networks are a recent development in the field of machine learning and allow our networks to exploit topological structures *between* data points rather than implicitly learning some abstract relational patterns through training. This is useful because many data that we wish to operate on has a natural and known graphical structure that will inform relational patterns in some straightforward way: chemical bonds, information flow in a social network etc. By including this topological data into the network itself we can improve the quality of our predictions.\n",
    "\n",
    "More generally, graphs are a generalisation of data and graph neural networks are a generalisation of traditional neural networks. A recent body of work has shown that all of our existing neural network architectures can be expressed as a graph neural network. This field has been called [Geometric Deep Learning](https://arxiv.org/abs/2104.13478) and is exciting new vector of research.\n",
    "\n",
    "\n",
    "## GraphNeuralNetworks.jl\n",
    "\n",
    "\n",
    "In Julia there is excellent package support for Graph Neural Networks (GNNs) through the `GraphNeuralNetworks.jl` package. It operates in a semantically familiar way to `Flux.jl` but the API calls are often specific to GNNs whereas `Flux` is more general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6a7b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using GraphNeuralNetworks, Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f300639",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "\n",
    "A graph is the fundamental mathematical structure from which GNNs get their name. It is a collection of indexes (nodes) and relationships between them (edges). These are typically denoted $V$ and $E$. An edge is defined by two nodes and may be both weighted and directional. A weighted edged is indicated by some real number and may encode some relationship between the nodes. For example, in the molecule $\\text{H}_2\\text{SO}_4$ there are some oxygen molecules with double bonds to the sulfur and some with single bonds to the sulfur and hydrogen respectively and we would describe these oxygen bonds with edges of weights 2 and 1 respectively.\n",
    "\n",
    "![H2S04](./images/h2so4.png) \n",
    "*The graph of the sulfuric acid molecule.*\n",
    "\n",
    "The bonds in this molecule are bidirectional but many data that we are interested in is unidirectional. Consider a virus that is spreading through a population. Each individual (node) can pass the virus to other individuals in the network (graph) based on many factors: spatial proximity, symptoms (coughing), immunosenstivity etc. However, the probability of transmission is conditioned on sending node containing the virus and the receiving node *not* containing the virus: the disease can only spread one way and the graph is therefore unidirectional.\n",
    "\n",
    "## Adjacency Matrices\n",
    "\n",
    "The information about a graphs structure can be encoded in a relatively compact an intuitive structure: the adjancency matrix. The matrix is constructed by indexing each of the nodes in the graph with natural numbers $(1 \\dots n)$ and associating these with the rows and columns of an $n \\times n$ matrix $A$. The edge weights are encoded in the matrix values $A_{ij}$. For bidirectional edges this matrix will be symmetric: $W_{ij}$.\n",
    "\n",
    "## Graphical Data\n",
    "\n",
    "In addition to the edge relationship graph nodes may contain other information in real world networks. In the $\\text{H}_2\\text{SO}_4$ example we implicity encoded the the atomic information into the node: we specified nodes 1 and 2 were H and so forth. More generally, we encode $m$ data features of interest for $n$ into a feature matrix $F$ with $m$ rows and $n$ columns. The two matrices $F$ and $A$ define our graphical data and completely specify the network. It is these matrices that we will operate on to train our neural networks.\n",
    "\n",
    "## Graph Objects in GraphNeuralNetworks\n",
    "\n",
    "Let's create the following simple graph in the GraphNeuralNetworks framework. We will specify the graph with its adjacency matrix. To each of the nodes we will attach a feature vector of length 4 consisting of random data. The graph object will be specified with the `GNNGraph(adjmat; ndata=featuremat)` API call.\n",
    "\n",
    "![graph_labels.png](./images/graph_labels.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "105dbe5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNGraph:\n",
       "    num_nodes = 10\n",
       "    num_edges = 14\n",
       "    ndata:\n",
       "        x => 4×10 Matrix{Float64}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amat = [\n",
    " 0  1  0  0  0  0  0  0  0  0;\n",
    " 1  0  1  1  0  1  0  0  0  0;\n",
    " 0  1  0  0  0  0  0  0  0  0;\n",
    " 0  0  0  0  0  1  0  0  0  0;\n",
    " 0  0  0  0  0  0  1  0  0  0;\n",
    " 0  0  0  0  0  0  0  1  1  0;\n",
    " 0  0  0  0  1  0  0  0  0  0;\n",
    " 0  0  0  0  0  0  0  0  0  0;\n",
    " 0  0  0  0  0  0  1  0  0  1;\n",
    " 0  0  0  0  0  0  0  0  1  0;\n",
    "    ]\n",
    "fmat = rand(4, 10)\n",
    "g = GNNGraph(amat; ndata=fmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0332278",
   "metadata": {},
   "source": [
    "# GNN Architectures\n",
    "\n",
    "What precisely is a Graph Neural Network (GNN)? It is simply a collection of weights and biases which update the state of a node using both the features of the node *and* the states of the neighbouring nodes in the graph. This allows information to flow through the graph. This is also not a new idea - early neural networks adopted this and more fundamentally this is precisely how biological neural networks operate (in a simplified fashion). There are generally three major architectures that are referred to when we talk about GNN architectures:\n",
    "\n",
    "1. Convolutional GNNs\n",
    "2. Attentional GNNs\n",
    "3. Message Passing GNNs\n",
    "\n",
    "\n",
    "## Convolutional GNNs\n",
    "\n",
    "Convolutional GNNs (CGNNs) are perhaps the predominate form of graph neural networks and operate by integrating the states of the direct neighbours of each node after modification by a shared weight matrix $W$. The weight matrix embeds the features $x_i$ into a latent space and these are summed over all the edges often with a normalisation factor $\\alpha_{ij}$ and then passed through an activation function $f$ to generate the state $h_i$:\n",
    "\n",
    "$$h_i = f\\left(\\sum_{e \\in \\text{edges}(i)} \\alpha_{ij} W x_j\\right)$$\n",
    "\n",
    "By summing over all of the neighbours we are performing a convolution and this is where the architecture derives its name. Note that it does not have to be a sum to be a convolution - any permutation invariant function will do. The sum operation happens to be the most popular. \n",
    "\n",
    "The most used form of CGNNs are ConvNets applied to images. Here the topology is defined by each pixels immediate neighbours in the Euclidean sense and the features of the input data are the RGB channels. This topology is very regular and allows the network to capture spatial relationships and rotational/translational symmetries. Neighbourhoods need not be defined by spatial relationships and this flexibility makes CGNNs useful exploratory networks.\n",
    "\n",
    "A CGNN layer is implemented with the `CGNConv` API call and is specified with `Lin=>Lout` where `Lin` is the dimension of the input features and `Lout` is the dimension of the output latents. The activation function is specified by the optional `σ = actfun` argument where the default is the identity function. The scaling is calculated with $\\alpha_{ij} = 1/\\sqrt(|d(i)||d(j)|$ where $d(i)$ is the degree of the $i$'th node. `bias=true` will specify an optional bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bb35410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNConv(10 => 8, #65)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gconv_layer = GCNConv(10=>8, x->tanh.(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be992c6f",
   "metadata": {},
   "source": [
    "## Attentional GNNs\n",
    "\n",
    "Attentional GNNs allow attention mechanisms, which have been successful in analysing sequence data, to be generalised into GNNs. At its core this amounts to allowing the normalising weights between nodes $\\alpha_{ij}$ to be learnable. This allows nodes to pay attention to certain nodes information flow while ignoring others.\n",
    "For example, your social network might include a supervisor and a family member. On the task of advice on a thesis you might like to pay more attention to your supervisor than your family member, but for cooking a traditional dinner the family member might be a better bet. The state update rule now simply includes trainable weights with feature data to facilitate the attention mechanism.\n",
    "\n",
    "\n",
    "$$h_i = f\\left(\\sum_{e \\in \\text{edges}(i)} \\alpha(x_i, x_j) W x_j\\right)$$\n",
    "\n",
    "\n",
    "A common attentional implementation is given by the `GATConv` API call. The number of attentional heads (analagous to attentional heads in the Transformer architecture) is given by the heads keyword. An optional specification is the `negative_slope` keyword which defaults to `0.2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b100a2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATConv(8 => 2, negative_slope=0.2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat_layer = GATConv(8=>4; heads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a3fcb",
   "metadata": {},
   "source": [
    "## Message Passing GNNs\n",
    "Message passing GNNs are the most generic implementation of a neural network. A message vector is computed using both node features and edge features between two different nodes. The messages are then propogated to neighbours and used to update the state of both the edges and the nodes. This is done with the `propogate(function, graph, reduction; xi=targets, xj=sources, e=edges)`. For example, a generic convolutional network layer may be specified on our original graph and feature matrix `fmat` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8595b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14×10 Matrix{Float64}:\n",
       " 0.785263  1.79316   0.785263  0.785263  …  0.71739   1.56053   0.822374\n",
       " 0.848053  2.32327   0.848053  0.848053     0.780251  1.96444   1.10054\n",
       " 1.24747   1.98806   1.24747   1.24747      1.18866   2.10074   0.817065\n",
       " 1.14504   2.2147    1.14504   1.14504      1.27851   2.71648   0.968195\n",
       " 1.05845   1.87567   1.05845   1.05845      1.20024   2.48849   0.790413\n",
       " 0.328097  0.753836  0.328097  0.328097  …  0.333672  0.749137  0.347043\n",
       " 0.573464  1.50686   0.573464  0.573464     0.647398  1.66233   0.700647\n",
       " 0.589453  1.58851   0.589453  0.589453     0.62694   1.56731   0.756376\n",
       " 0.486401  2.21699   0.486401  0.486401     0.487106  1.78054   1.13259\n",
       " 1.04397   2.5399    1.04397   1.04397      1.07237   2.50611   1.18295\n",
       " 0.873112  2.63625   0.873112  0.873112  …  0.929787  2.53615   1.27136\n",
       " 0.526614  2.01766   0.526614  0.526614     0.611734  1.94948   1.00614\n",
       " 0.636326  1.73268   0.636326  0.636326     0.69344   1.73709   0.830054\n",
       " 1.05152   1.88175   1.05152   1.05152      1.01916   1.94149   0.80661"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = rand(14,4)\n",
    "message(xi, xj, e) = W * xj\n",
    "x = GraphNeuralNetworks.propagate(message, g, +, xj=fmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799ec6d",
   "metadata": {},
   "source": [
    "By creating generically complex message functions we can instantiate any existing GNN architecture. It is also through this method that we can implement our own custom layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c0095a",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "Pooling is a useful strategy that has come through convolutional neural networks. It allows layers to be condensed to representative values over pools of candidate parameters. In the convolutional network, for example, a pool is defined by a filter size and this is passed over each feature layer taking a representative value along every stride of the layer e.g. mean/max of all values. Some common pooling layers are:\n",
    "\n",
    "* Top-K pooling: selecting the top k nodes in a neighbourhood over all graph nodes. Implemented with the `TopK(adj, k, input_channel)` API where adj is the adjacency, k is the number of nodes, and input_channel is the dimension along which to pool.\n",
    "* GlobalPool: performing an aggegration over the entire graph to reduce it to a single vector along the feature dimensions e.g. averaging along all nodes features. Implemented through the API call `GlobalPool(func)` e.g. `GlobalPool(mean)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae62ac",
   "metadata": {},
   "source": [
    "## GNN Models\n",
    "\n",
    "We implement these architectures as layers in our GNN which are composed together using the API call `GNNChain`. The chain forms our model and we input data in the form of graph objects with feature vectors. The `GraphNeuralNetworks` package has GPU support through CUDA and models are migrated to the GPU in the same way as Flux using the `gpu(model)` call or by piping operator: `model |> gpu`. Flux layers can be composed into `GNNChains` (provided they are composable) e.g. `BatchNorm`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9385ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNChain(GCNConv(10 => 8, #65), GATConv(8 => 2, negative_slope=0.2), BatchNorm(2))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GNNChain(gconv_layer, gat_layer, BatchNorm(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0889cef",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training a GNN proceeds in a very similar fashion to a regular neural network - they use the regular optimisation and regularisation techniques adopted for earlier neural networks. This is intuitive in that these GNNs are merely generalised functions with understandable outputs and which an optimisation goal can be defined on. Therefore, the generic optimisation methods should automatically generalise. There are still some considerations that need to be kept in mind when training GNNs.\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "The loss function is a salient consideration of any network design and GNNs are not exempt from this. The loss function is task specific and should be considered in a problem-by-problem basis. For GNNs in particular the loss function must be *permutation-invariant*. Specifically, if we were to relabel the indexes in the graph the loss must not change, because the fundamental graph topology will not change if we do this. Any loss can therefore not be index dependent nor can it be non-commutative in its indexes e.g. the product of two numbers are commutative, the product of two matrices are not. The common ones such as `mse` are usual candidates for loss functions.\n",
    "\n",
    "\n",
    "## Batching\n",
    "\n",
    "To batch data compose multiple graphs into a single graph using the `batch` API call. This has a natural intuition: the disjoint subgraphs of a graph behave as independent graphs. To see this easily consider the block structure of the adjancency matrix of a graph with several disjoint subgraphs. Once the graph data has been batched we can use the `DataLoader` API call from Flux to shuffle and batch data points as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a65db",
   "metadata": {},
   "source": [
    "# A Graph Classification Example\n",
    "\n",
    "We will use the TUDataset of graphs with classified labels to perform a simple graph classification task. Specifically we will analyse the PROTEINS dataset which labels graphs derived from protein structures as ezymes or non-ezyments. We can find this in the `MLDatasets` package and use a convenience function `mldataset2gnngraph` to convert it to the format required for `GraphNeuralNetworks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c069e581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element DataLoader(::Tuple{SubArray{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}, 1, Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}, Tuple{Vector{Int64}}, false}, SubArray{Float32, 1, Vector{Float32}, Tuple{Vector{Int64}}, false}}, shuffle=true, batchsize=32, collate=Val{true}())\n",
       "  with first element:\n",
       "  (GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}, 32-element Vector{Float32},)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics, ProgressMeter\n",
    "using MLDatasets: TUDataset\n",
    "data = MLDatasets.TUDataset(\"PROTEINS\")\n",
    "graphdata = mldataset2gnngraph(data)\n",
    "\n",
    "# extract the feature matrix of the data test and define the loss for target training\n",
    "G = [GNNGraph(g, ndata=g.ndata.features) for g in graphdata]\n",
    "y = Float32.((data.graph_data.targets .- 1))\n",
    "\n",
    "# Split the data into testing and training and create a dataloader object\n",
    "train_data, test_data = Flux.splitobs((G, y), at=0.8, shuffle=true)\n",
    "train_loader = Flux.DataLoader(train_data; batchsize=32, shuffle=true, collate=true)\n",
    "test_loader = Flux.DataLoader(test_data; batchsize=32, shuffle=true, collate=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b126cf1a",
   "metadata": {},
   "source": [
    "The model will be defined quite simply: three layers of convolutional architecture with followed by a global pooling layer to take it to a single vector. The classification will be performed by a dense classifier onto the targets (0,1). The activation for each of the convolutional layers will be the `relu` function and the latent dimensions will be 4, 10, and 2. Pooling will be performed by `mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7f7cf99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNChain(GCNConv(1 => 10, relu), GCNConv(10 => 100, relu), GlobalPool{typeof(mean)}(Statistics.mean), Dense(100 => 1, σ))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim1 = size(G[1].ndata.x, 1)\n",
    "dim2 = 10\n",
    "dim3 = 100\n",
    "\n",
    "model = GNNChain(\n",
    "    GCNConv(dim1=>dim2, relu),\n",
    "    GCNConv(dim2=>dim3, relu),\n",
    "    GlobalPool(mean),\n",
    "    Dense(dim3, 1, σ)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa461b",
   "metadata": {},
   "source": [
    "The training proceeds in the normal fashion: we specify the trainable parameters, define an optimiser (in this case, Adam), and an appropriate loss. The loss is chosen to be `logitcrossentropy`. We train our model for 100 epochs. We then validate our training on our test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "afc77d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:18:07\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1087.851435 seconds (13.10 M allocations: 49.187 GiB, 0.68% gc time, 59.36% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 65.39\n",
       " 65.92"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = Flux.params(model)\n",
    "opt = Adam(0.001)\n",
    "loss(s, t) = Flux.logitcrossentropy(vec(model(s, s.ndata.x)), t)\n",
    "\n",
    "@time @showprogress for e in 1:100\n",
    "    Flux.train!((x,y) -> loss(x,y), ps, train_loader, opt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9eca3771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_accuracy(model, data)\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "    for (g, y) in data\n",
    "        n = length(y)\n",
    "        res = vec(model(g, g.ndata.x))\n",
    "        accuracy += mean((res .> 0.5) .== y) * n\n",
    "        total += n\n",
    "    end\n",
    "    return round(accuracy*100/total, digits=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "[test_accuracy(model, train_loader), test_accuracy(model, test_loader)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using CUDA\n",
    "using Transformers\n",
    "using Transformers.Basic #for loading the positional embedding\n",
    "\n",
    "\n",
    "labels = map(string, 1:10)\n",
    "startsym = \"11\"\n",
    "endsym = \"12\"\n",
    "unksym = \"0\"\n",
    "labels = [unksym, startsym, endsym, labels...]\n",
    "vocab = Vocabulary(labels, unksym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42931378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for generate training datas\n",
    "sample_data() = (d = map(string, rand(1:10, 10)); (d,d))\n",
    "#function for adding start & end symbol\n",
    "preprocess(x) = [startsym, x..., endsym]\n",
    "\n",
    "@show sample = preprocess.(sample_data())\n",
    "@show encoded_sample = vocab(sample[1]) #use Vocabulary to encode the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b295eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a Word embedding layer which turn word index to word vector\n",
    "embed = Embed(512, length(vocab)) |> gpu\n",
    "#define a position embedding layer metioned above\n",
    "pe = PositionEmbedding(512) |> gpu\n",
    "\n",
    "#wrapper for get embedding\n",
    "function embedding(x)\n",
    "  we = embed(x, inv(sqrt(512)))\n",
    "  e = we .+ pe(we)\n",
    "  return e\n",
    "end\n",
    "\n",
    "#define 2 layer of transformer\n",
    "encode_t1 = Transformer(512, 8, 64, 2048) |> gpu\n",
    "encode_t2 = Transformer(512, 8, 64, 2048) |> gpu\n",
    "\n",
    "#define 2 layer of transformer decoder\n",
    "decode_t1 = TransformerDecoder(512, 8, 64, 2048) |> gpu\n",
    "decode_t2 = TransformerDecoder(512, 8, 64, 2048) |> gpu\n",
    "\n",
    "#define the layer to get the final output probabilities\n",
    "linear = Positionwise(Dense(512, length(vocab)), logsoftmax) |> gpu\n",
    "\n",
    "function encoder_forward(x)\n",
    "  e = embedding(x)\n",
    "  t1 = encode_t1(e)\n",
    "  t2 = encode_t2(t1)\n",
    "  return t2\n",
    "end\n",
    "\n",
    "function decoder_forward(x, m)\n",
    "  e = embedding(x)\n",
    "  t1 = decode_t1(e, m)\n",
    "  t2 = decode_t2(t1, m)\n",
    "  p = linear(t2)\n",
    "  return p\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87644df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder_forward(encoded_sample)\n",
    "probs = decoder_forward(encoded_sample, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b110b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Flux: onehot\n",
    "function smooth(et)\n",
    "    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n",
    "    p = sm .* (1 .+ -et)\n",
    "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
    "    label\n",
    "end\n",
    "Flux.@nograd smooth\n",
    "\n",
    "#define loss function\n",
    "function loss(x, y)\n",
    "  label = onehot(vocab, y) #turn the index to one-hot encoding\n",
    "  label = smooth(label) #perform label smoothing\n",
    "  enc = encoder_forward(x)\n",
    "  probs = decoder_forward(y, enc)\n",
    "  l = logkldivergence(label[:, 2:end, :], probs[:, 1:end-1, :])\n",
    "  return l\n",
    "end\n",
    "\n",
    "#collect all the parameters\n",
    "ps = Flux.params(embed, pe, encode_t1, encode_t2, decode_t1, decode_t2, linear)\n",
    "opt = ADAM(1e-4)\n",
    "\n",
    "#function for created batched data\n",
    "using Transformers.Datasets: batched\n",
    "\n",
    "#flux function for update parameters\n",
    "using Flux: gradient\n",
    "using Flux.Optimise: update!\n",
    "\n",
    "#define training loop\n",
    "function train!()\n",
    "  @info \"start training\"\n",
    "  for i = 1:1000\n",
    "    data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n",
    "\tx, y = preprocess.(data[1]), preprocess.(data[2])\n",
    "    x, y = vocab(x), vocab(y) #encode the data\n",
    "    x, y = todevice(x, y) #move to gpu\n",
    "    grad = gradient(()->loss(x, y), ps)\n",
    "    if i % 8 == 0\n",
    "        l = loss(x, y)\n",
    "    \tprintln(\"loss = $(l) epoch = (i)\")\n",
    "    end\n",
    "    update!(opt, ps, grad)\n",
    "  end\n",
    "end\n",
    "\n",
    "train!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: onecold\n",
    "function translate(x)\n",
    "    ix = todevice(vocab(preprocess(x)))\n",
    "    seq = [startsym]\n",
    "\n",
    "    enc = encoder_forward(ix)\n",
    "\n",
    "    len = length(ix)\n",
    "    for i = 1:2len\n",
    "        trg = todevice(vocab(seq))\n",
    "        dec = decoder_forward(trg, enc)\n",
    "        #move back to gpu due to argmax wrong result on CuArrays\n",
    "        ntok = onecold(collect(dec), labels)\n",
    "        push!(seq, ntok[end])\n",
    "        ntok[end] == endsym && break\n",
    "    end\n",
    "  seq[2:end-1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b39cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate([\"U\", \"C\", \"G\", \"A\", \"A\", \"G\", \"G\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9cb80c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

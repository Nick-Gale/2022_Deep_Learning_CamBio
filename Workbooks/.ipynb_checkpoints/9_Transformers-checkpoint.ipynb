{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff57ecf",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Transformers are a sophisiticated modern neural network best adapted to sequential data. They are in the process of replacing the recurrent neural network as the de-facto model for sequence analysis. As a brief review: recurrent neural networks incorporate information about sequential dependencies through recurrent connections which allow a hidden state to incorporate variables that have been previously exposed to the network. There are some principle shortcomings with this approach: short memory, exploding/vanishing gradients, and $O(n)$ complexity. Tokens that are historically adjacent to each other have more weight and thus the information in the sequence decays over long ranges; the network has a relatively short memory. The hidden state is called recursively over the course of training and thus the gradients have a tendency to either explode or vanish and various regularisation tricks are needed to stablise the training routine. Finally, the current hidden state depends on the previous hidden state and the data must therefore be parsed sequentially - there is no parallelisation routine. This results in very computationally intensive training.\n",
    "\n",
    "The Transformer was introduced in 2016 in the seminal paper [Attention is all you need](https://arxiv.org/abs/1706.03762) and since then have been the basis of most large language models. They neatly solve all three problems with reccurent neural networks with the idea of attention. It should be noted that attention is not a pioneering feature of this paper. However, the attention mechanisms neatly allows the author to encode long range and importantly asymetric dependencies between sequence tokens. These attention representations are bidirectional and all-to-all allowing for massive parrallelisation within an attention layer. They are also relatively stable in their gradients because attention representations are not recursively computed.\n",
    "\n",
    "The outcomes of this notebook are:\n",
    "\n",
    "* Understanding attention and multi-head attention.\n",
    "* Creating a custom Flux layer to represent attention heads.\n",
    "* Understanding sequence generation.\n",
    "* A comprenhensive look at the transformer architecture.\n",
    "* Implementing a custom Transformer network.\n",
    "* Training a Transformer on biological sequence data.\n",
    "\n",
    "## Self Attention\n",
    "\n",
    "Attention is the key concept in the Transformer model. It heuristically refers to how much each element in the sequence relates to one and other. The simplest form of this is the Euclidean projection of one vector onto another and the attention matrix is accordingly defined by dot-product; recall that the dot product has the geometric intepretation of how much one element projects onto another. Suppose that the sequence elements are $v_i$\n",
    "\n",
    "$$ W_{ij} = v_j v_i^T $$\n",
    "\n",
    "Now that we have created the attention relationships we want to output some intepretable configuration of vectors that incorporate these relationships. The most straightforward approach is a weighted linear combination of all vectors in the sequence: the attention weights are used to incorporate each vectors influence on each other vector.\n",
    "\n",
    "$$ a_i = \\sum_j W_{ij} v_j $$\n",
    "\n",
    "Finally, we want to scale these outputs into the \"interpretable\" regime therefore we apply a normalisation factor (scaling by the sqrt of the dimensionality) and the `softmax` function to convert these output into a probability distribution. These are just implementation details. The next thing we can note, and the algebra suggests it automatically, is that this can be parallelised. We can concatenate all the input vectors into a matrix $A$ and pre-multiply it by the weight matrix $W$ completing the attention transformation in a single step.\n",
    "\n",
    "$$ V = \\text{softmax}\\left(\\frac{W}{\\sqrt{d}}\\right) V$$\n",
    "\n",
    "## Keys, Queries, and Values\n",
    "Up until this point we have not provided any mechanism to *learn*. It would be perculiar if the primary component of a learning algorithm had no learnable parameters. Before we rectify this we need to clarify some terminology. If we apply the same vector concatenation trick to constructing the attention matrix we find that the attention matrix $W$ is the product of two matrices $A$ and $A^T$ which for now we will label $K$ and $Q$. Let's imagine that each of the input vectors where one-hot encoded: they had a 1 in a singular index and a 0 elsewhere. The resulting attention weights would be one if and only if the input indexes were identical: $i = j$. This means that the operation is acting like a dictionary or look-up table would act: if the first vector matches the second vector output a Boolean true. In computer science these are generally labelled *keys* and *queries*. These terms motivate our matrices $K$ and $Q$. The final component of the look up table is to output the value associated with a queried key. These are nothing more than the input vectors again and are encoded in our $V$ matrix.\n",
    "\n",
    "So far, the input vectors have played the key role in all the operations and we cant expect to learn much about their relational structure. Let's suppose that there is a more efficient lookup table that could encoded the data. This would amount to an abitrary set of keys and queries, but the values would remain the same. We can imagine the keys and queries as embedding matrices and this would allow us to embedd our inputs in a lower (or higher) dimensional space. Therefore, if the input tokens $v_i$ are vectors of length $N$ and we want to embed them into a space of dimension $P$ then our key and query matrices should have dimensions $N \\times P$ and $P \\times N$ respectively. These matrices will now be intialised in a way that does not need to depend on $v_i$, usually just a random number in each slot. The goal will be to learn the best form of these matrices. This can be thought of in much the same way as a convolutional filter where the goal was to learn the kernel weights. We finally arrive at the complete attention equation:\n",
    "\n",
    "$$ A(K, Q, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d}}\\right)V $$.\n",
    "\n",
    "## Positional Encoding\n",
    "The key and query matrices allow the data to interact with each other bidirectionally and asymetrically (one element can pay more attention to other elements than those elements pay to it: think of a fan that dotes on a celebrity). However, this comes at the cost of losing sequence information as the indexes are not encoded in the learning protocol. The ordering is usually extremely important for sequentially organised data and we do not want to through out this ordering. To incorporate it we augment our input vectors with a number that indicates the relative ordering. This is commonly chosen to be a sinusoid (but this choice is abitrary). The encoding is typically:\n",
    "\n",
    "$$ p_d(x, i) = \\sin \\left( \\frac{{x^{\\frac{2i}{d}}}}{10000}  \\right) $$\n",
    "\n",
    "This encoding is acting as another embedding in the same fashion as the keys and queries matrices. This, in principle, could also be learned. However, we choose a fixed function because in practice the learned results do not vastly outperform a fixed embedding and they come at a larger cost.\n",
    "\n",
    "## Attention Heads\n",
    "\n",
    "The attention operation in conjunction with the positional encoding form an *attention head*. This is nothing more than a learned representation of what and where each element in the sequence should be pay attention to. It is certainly possible that different sub-sequences mean different things to one and other in different contexts. This can be represented with multiple attention heads operating on the same inputs. As an analogy, think of the different features that can be extracted by having multiple convolutional kernels in each layer. Multihead attention is exteremely easy to implement. Simply concatentate the key/query matrices together linearly. In a similar analogy to convolutional networks attention heads may be composed in layers. The outputs of an attention head are a vector of weighted attention cues. These can be passed to a new set of attention matrices. In doing so we can learn deep and convoluted relationships. We are now ready to create our custom attention head and multiattention head types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b617a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Attention\n",
    "    Q::Matrix\n",
    "    K::Matrix\n",
    "    d::Int\n",
    "    function Attention(sequencedimension, embeddingdimension)\n",
    "        Q = rand(embeddingdimension, sequencedimension)\n",
    "        K = rand(sequencedimension, embeddingdimension)\n",
    "        new(Q, K, sequencedimension)\n",
    "    end\n",
    "end\n",
    "\n",
    "struct MultiHeadAttention\n",
    "    Q::Matrix\n",
    "    K::Matrix\n",
    "    d::Int\n",
    "    function MultiHeadAttention(attentionheads...)\n",
    "        Q = hcat(attentionheads.Q...)\n",
    "        K = vcat(attentionheads.K...)\n",
    "        d = attentionheads[1].d\n",
    "        new(Q, K, d)\n",
    "    end\n",
    "end\n",
    "\n",
    "(a::Union{Attention, MultiHeadAttention})(V) = Flux.softmax(a.K * a.Q ./ sqrt(d)) .* V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372fbb6",
   "metadata": {},
   "source": [
    "# The Transformer Architecture\n",
    "The Transformer uses attention as its principle working mechanism but it itself is a neural network architecture. It is composed of an encoder and a decoder. Each of these has several layers of multi-head attention and in-between the layers are Dense perceptron layers. This allows the vectors to be appropriately transformed/classified at each step. The encoder component is relatively straightforward - feed it the input vectors and record the final output. The decoder layer is slightly more sophisticated.\n",
    "\n",
    "A single pass of the decoder proceeds as follows: take a new input (not one of the encoder inputs) and pass it through an attention layer. This is the self-attention layer of the decoder. For every subsequent attention layer the encoder outputs will be combined with the decoder pass. These will go through multi-head attention layers, be normalised, and then passed through a Dense perceptron layer as in the encoder. \n",
    "\n",
    "[IMG Transfomer]\n",
    "\n",
    "\n",
    "## Output Decoding\n",
    "\n",
    "To generate the output sequence the Transformer proceeds in much the same way as a traditional sequence decoder. It takes a `start` symbol and generates data sequentially until a `stop` symbol is generated. The outputs are encoded in an arbitrarily long vector that is expected to be longer than the length of the output sentence e.g. 512. The output layer is decoded using outputs generated up until the point of the sequence as inputs with a mask of `-Inf` for future ouputs. \n",
    "\n",
    "The masked output is used to generate its own query values and is combined with a positional encoding in the same fashion as the input layer. Then it is generally passed through its own self attention layer with masked multi-head attention before interacting with the encoded keys and values. The queries are combined with the encoder results to generate the next sequential output. This is autoregression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f300639",
   "metadata": {},
   "source": [
    "## Implementing the Transformer\n",
    "\n",
    "The Transformer is an inherently simple architecture and it is fairly straightforward to take a custom type as defined before and proceed through Zygote backpropogation and Flux API calls to complete training. However, as always, implementing this will require uninformative boiler-plate code. We have covered how one might implement custom architectures through custom layer types in previous notebooks and these can be used for reference. In this implementation we will use an existing package in the Flux ecosystem: `Transformers.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "773a045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Transformers, Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c1712da9",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: Dense not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Dense not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[128]:31",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "labels = map(string, 1:10)\n",
    "startsym = \"11\"\n",
    "endsym = \"12\"\n",
    "unksym = \"0\"\n",
    "labels = [unksym, startsym, endsym, labels...]\n",
    "vocab = Transformers.Vocabulary(labels, unksym)\n",
    "\n",
    "sample_data() = (d = map(string, rand(1:10, 10)); (d,d))\n",
    "preprocess(x) = [startsym, x..., endsym]\n",
    "embed = Transformers.Embed(512, length(vocab))\n",
    "pe = Transformers.PositionEmbedding(512)\n",
    "function embedding(x)\n",
    "  we = embed(x, inv(sqrt(512)))\n",
    "  e = we .+ pe(we)\n",
    "  return e\n",
    "end\n",
    "\n",
    "sample = preprocess.(sample_data())\n",
    "encoded_sample = vocab(sample[1]) #use Vocabulary to encode the training data\n",
    "\n",
    "\n",
    "#define 2 layer of transformer\n",
    "encode_t1 = Transformer(512, 8, 64, 2048)\n",
    "encode_t2 = Transformer(512, 8, 64, 2048)\n",
    "\n",
    "#define 2 layer of transformer decoder\n",
    "decode_t1 = TransformerDecoder(512, 8, 64, 2048)\n",
    "decode_t2 = TransformerDecoder(512, 8, 64, 2048)\n",
    "\n",
    "#define the layer to get the final output probabilities\n",
    "linear = Transformers.Positionwise(Dense(512, length(vocab)), logsoftmax)\n",
    "\n",
    "function encoder_forward(x)\n",
    "  e = embedding(x)\n",
    "  t1 = encode_t1(e)\n",
    "  t2 = encode_t2(t1)\n",
    "  return t2\n",
    "end\n",
    "\n",
    "function decoder_forward(x, m)\n",
    "  e = embedding(x)\n",
    "  t1 = decode_t1(e, m)\n",
    "  t2 = decode_t2(t1, m)\n",
    "  p = linear(t2)\n",
    "  return p\n",
    "end\n",
    "\n",
    "enc = encoder_forward(encoded_sample)\n",
    "probs = decoder_forward(encoded_sample, enc)\n",
    "\n",
    "using Flux: onehot\n",
    "function smooth(et)\n",
    "    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n",
    "    p = sm .* (1 .+ -et)\n",
    "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
    "    label\n",
    "end\n",
    "Flux.@nograd smooth\n",
    "\n",
    "#define loss function\n",
    "function loss(x, y)\n",
    "  label = onehot(vocab, y) #turn the index to one-hot encoding\n",
    "  label = smooth(label) #perform label smoothing\n",
    "  enc = encoder_forward(x)\n",
    "  probs = decoder_forward(y, enc)\n",
    "  l = Flux.logitcrossentropy(label[:, 2:end, :], probs[:, 1:end-1, :])\n",
    "  return l\n",
    "end\n",
    "\n",
    "#collect all the parameters\n",
    "ps = Flux.params(embed, pe, encode_t1, encode_t2, decode_t1, decode_t2, linear)\n",
    "opt = ADAM(1e-4)\n",
    "\n",
    "#function for created batched data\n",
    "using Transformers.Datasets: batched\n",
    "\n",
    "#flux function for update parameters\n",
    "using Flux: gradient\n",
    "using Flux.Optimise: update!\n",
    "\n",
    "#define training loop\n",
    "function train!()\n",
    "  @info \"start training\"\n",
    "  for i = 1:100\n",
    "    data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n",
    "\tx, y = preprocess.(data[1]), preprocess.(data[2])\n",
    "    x, y = vocab(x), vocab(y) #encode the data\n",
    "    x, y = todevice(x, y) #move to gpu\n",
    "    agrad = gradient(()->loss(x, y), ps)\n",
    "    if i % 8 == 0\n",
    "        l = loss(x, y)\n",
    "    \tprintln(\"loss = $l\")\n",
    "    end\n",
    "    update!(opt, ps, grad)\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "81d1a100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary{String}(13, unk=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transformers.Vocabulary(labels, unksym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a334168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: start training\n",
      "└ @ Main In[25]:83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13.962333 seconds (20.86 M allocations: 1.391 GiB, 3.64% gc time, 97.33% compilation time: 1% of which was recompilation)\n",
      "  0.408648 seconds (6.63 k allocations: 393.288 MiB, 16.53% gc time)\n",
      "  0.689129 seconds (6.64 k allocations: 393.288 MiB, 33.45% gc time)\n",
      "  0.331040 seconds (6.63 k allocations: 393.288 MiB, 11.22% gc time)\n",
      "  0.313995 seconds (6.63 k allocations: 393.288 MiB, 8.34% gc time)\n",
      "  0.394997 seconds (6.63 k allocations: 393.288 MiB, 21.84% gc time)\n",
      "  0.325699 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.530073 seconds (6.63 k allocations: 393.288 MiB, 44.13% gc time)\n",
      "loss = -968.3592\n",
      "  0.389082 seconds (6.64 k allocations: 393.288 MiB, 24.94% gc time)\n",
      "  0.341255 seconds (6.63 k allocations: 393.288 MiB, 6.63% gc time)\n",
      "  0.326153 seconds (6.63 k allocations: 393.288 MiB, 9.68% gc time)\n",
      "  0.386669 seconds (6.63 k allocations: 393.288 MiB, 26.74% gc time)\n",
      "  0.290921 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.535862 seconds (6.63 k allocations: 393.288 MiB, 47.74% gc time)\n",
      "  0.326931 seconds (6.64 k allocations: 393.288 MiB, 11.07% gc time)\n",
      "  0.341798 seconds (6.64 k allocations: 393.288 MiB, 15.97% gc time)\n",
      "loss = -1059.7325\n",
      "  0.321527 seconds (6.64 k allocations: 393.288 MiB, 10.11% gc time)\n",
      "  0.369808 seconds (6.63 k allocations: 393.288 MiB, 14.75% gc time)\n",
      "  0.366598 seconds (6.64 k allocations: 393.288 MiB, 6.77% gc time)\n",
      "  0.322313 seconds (6.63 k allocations: 393.288 MiB, 17.95% gc time)\n",
      "  0.284869 seconds (6.63 k allocations: 393.288 MiB, 6.38% gc time)\n",
      "  0.328955 seconds (6.63 k allocations: 393.288 MiB, 18.23% gc time)\n",
      "  0.299674 seconds (6.63 k allocations: 393.288 MiB, 10.67% gc time)\n",
      "  0.333864 seconds (6.63 k allocations: 393.288 MiB, 22.43% gc time)\n",
      "loss = -1085.9263\n",
      "  0.372876 seconds (6.63 k allocations: 393.288 MiB, 26.67% gc time)\n",
      "  0.304277 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.538269 seconds (6.63 k allocations: 393.288 MiB, 45.66% gc time)\n",
      "  0.324937 seconds (6.63 k allocations: 393.288 MiB, 9.84% gc time)\n",
      "  0.333987 seconds (6.64 k allocations: 393.288 MiB, 16.39% gc time)\n",
      "  0.311230 seconds (6.63 k allocations: 393.288 MiB, 7.48% gc time)\n",
      "  0.328287 seconds (6.63 k allocations: 393.288 MiB, 14.64% gc time)\n",
      "  0.351684 seconds (6.63 k allocations: 393.288 MiB, 7.25% gc time)\n",
      "loss = -1112.6688\n",
      "  0.353164 seconds (6.63 k allocations: 393.288 MiB, 16.96% gc time)\n",
      "  0.311212 seconds (6.63 k allocations: 393.288 MiB, 5.18% gc time)\n",
      "  0.356916 seconds (6.64 k allocations: 393.288 MiB, 19.80% gc time)\n",
      "  0.300202 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.552292 seconds (6.63 k allocations: 393.288 MiB, 45.03% gc time)\n",
      "  0.349562 seconds (6.63 k allocations: 393.288 MiB, 12.44% gc time)\n",
      "  0.335830 seconds (6.63 k allocations: 393.288 MiB, 15.29% gc time)\n",
      "  0.335084 seconds (6.64 k allocations: 393.288 MiB, 13.81% gc time)\n",
      "loss = -1137.1864\n",
      "  0.323991 seconds (6.63 k allocations: 393.288 MiB, 6.16% gc time)\n",
      "  0.408639 seconds (6.63 k allocations: 393.288 MiB, 22.39% gc time)\n",
      "  0.284017 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.569007 seconds (6.63 k allocations: 393.288 MiB, 42.77% gc time)\n",
      "  0.362340 seconds (6.64 k allocations: 393.288 MiB, 16.36% gc time)\n",
      "  0.394934 seconds (6.63 k allocations: 393.288 MiB, 9.83% gc time)\n",
      "  0.399402 seconds (6.63 k allocations: 393.288 MiB, 11.55% gc time)\n",
      "  0.330889 seconds (6.63 k allocations: 393.288 MiB, 3.11% gc time)\n",
      "loss = -1162.1467\n",
      "  0.330052 seconds (6.63 k allocations: 393.288 MiB, 9.77% gc time)\n",
      "  0.351278 seconds (6.63 k allocations: 393.288 MiB, 22.56% gc time)\n",
      "  0.285678 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.539981 seconds (6.63 k allocations: 393.288 MiB, 48.43% gc time)\n",
      "  0.307638 seconds (6.63 k allocations: 393.288 MiB, 14.79% gc time)\n",
      "  0.343116 seconds (6.63 k allocations: 393.288 MiB, 21.41% gc time)\n",
      "  0.294979 seconds (6.64 k allocations: 393.288 MiB, 11.30% gc time)\n",
      "  0.315936 seconds (6.63 k allocations: 393.288 MiB, 14.35% gc time)\n",
      "loss = -1187.2288\n",
      "  0.332772 seconds (6.64 k allocations: 393.288 MiB, 16.52% gc time)\n",
      "  0.282191 seconds (6.64 k allocations: 393.288 MiB, 3.42% gc time)\n",
      "  0.296813 seconds (6.63 k allocations: 393.288 MiB, 8.88% gc time)\n",
      "  0.334827 seconds (6.64 k allocations: 393.288 MiB, 20.74% gc time)\n",
      "  0.264949 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.528102 seconds (6.63 k allocations: 393.288 MiB, 48.01% gc time)\n",
      "  0.322778 seconds (6.63 k allocations: 393.288 MiB, 13.90% gc time)\n",
      "  0.307079 seconds (6.63 k allocations: 393.288 MiB, 10.21% gc time)\n",
      "loss = -1212.049\n",
      "  0.491878 seconds (6.63 k allocations: 393.288 MiB, 16.25% gc time)\n",
      "  0.341080 seconds (6.63 k allocations: 393.288 MiB, 4.94% gc time)\n",
      "  0.376841 seconds (6.63 k allocations: 393.288 MiB, 8.61% gc time)\n",
      "  0.469992 seconds (6.64 k allocations: 393.288 MiB, 18.22% gc time)\n",
      "  0.297470 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.634511 seconds (6.63 k allocations: 393.288 MiB, 49.92% gc time)\n",
      "  0.335600 seconds (6.63 k allocations: 393.288 MiB, 15.54% gc time)\n",
      "  0.304098 seconds (6.63 k allocations: 393.288 MiB, 10.68% gc time)\n",
      "loss = -1237.4536\n",
      "  0.479878 seconds (6.63 k allocations: 393.288 MiB, 21.51% gc time)\n",
      "  0.300358 seconds (6.64 k allocations: 393.288 MiB, 7.07% gc time)\n",
      "  0.392834 seconds (6.63 k allocations: 393.288 MiB, 21.98% gc time)\n",
      "  0.274239 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.546823 seconds (6.63 k allocations: 393.288 MiB, 46.31% gc time)\n",
      "  0.340747 seconds (6.64 k allocations: 393.288 MiB, 12.60% gc time)\n",
      "  0.321703 seconds (6.63 k allocations: 393.288 MiB, 10.01% gc time)\n",
      "  0.308316 seconds (6.64 k allocations: 393.288 MiB, 11.03% gc time)\n",
      "loss = -1262.9874\n",
      "  0.314031 seconds (6.64 k allocations: 393.288 MiB, 14.57% gc time)\n",
      "  0.287061 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.564447 seconds (6.63 k allocations: 393.288 MiB, 49.24% gc time)\n",
      "  0.302753 seconds (6.64 k allocations: 393.288 MiB, 10.19% gc time)\n",
      "  0.313214 seconds (6.63 k allocations: 393.288 MiB, 13.73% gc time)\n",
      "  0.332143 seconds (6.63 k allocations: 393.288 MiB, 17.18% gc time)\n",
      "  0.302038 seconds (6.63 k allocations: 393.288 MiB, 11.32% gc time)\n",
      "  0.321823 seconds (6.63 k allocations: 393.288 MiB, 15.68% gc time)\n",
      "loss = -1288.9337\n",
      "  0.322112 seconds (6.63 k allocations: 393.288 MiB, 16.26% gc time)\n",
      "  0.340950 seconds (6.63 k allocations: 393.288 MiB, 22.15% gc time)\n",
      "  0.274984 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.580009 seconds (6.63 k allocations: 393.288 MiB, 47.22% gc time)\n",
      "  0.309597 seconds (6.63 k allocations: 393.288 MiB, 13.55% gc time)\n",
      "  0.312789 seconds (6.64 k allocations: 393.288 MiB, 15.30% gc time)\n",
      "  0.317498 seconds (6.63 k allocations: 393.288 MiB, 15.86% gc time)\n",
      "  0.302521 seconds (6.63 k allocations: 393.288 MiB, 5.82% gc time)\n",
      "loss = -1314.8507\n",
      "  0.406666 seconds (6.64 k allocations: 393.288 MiB, 27.22% gc time)\n",
      "  0.287575 seconds (6.63 k allocations: 393.288 MiB)\n",
      "  0.614159 seconds (6.63 k allocations: 393.288 MiB, 50.26% gc time)\n",
      "  0.367648 seconds (6.63 k allocations: 393.288 MiB, 15.23% gc time)\n",
      " 71.504342 seconds (28.39 M allocations: 41.244 GiB, 10.64% gc time, 21.91% compilation time: 1% of which was recompilation)\n"
     ]
    }
   ],
   "source": [
    "@time train!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7ba10f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "String[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux: onecold\n",
    "function translate(x)\n",
    "    ix = todevice(vocab(preprocess(x)))\n",
    "    seq = [startsym]\n",
    "\n",
    "    enc = encoder_forward(ix)\n",
    "\n",
    "    len = length(ix)\n",
    "    for i = 1:2len\n",
    "        trg = todevice(vocab(seq))\n",
    "        dec = decoder_forward(trg, enc)\n",
    "        #move back to gpu due to argmax wrong result on CuArrays\n",
    "        ntok = onecold(collect(dec), labels)\n",
    "        push!(seq, ntok[end])\n",
    "        ntok[end] == endsym && break\n",
    "    end\n",
    "  seq[2:end-1]\n",
    "end\n",
    "translate(map(string, [5,5,6,6,1,2,3,4,7, 10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b20133",
   "metadata": {},
   "source": [
    "## Acid-Amino Sequence Prediction: Learning the Language of Codons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6787c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_codon = Dict( # the amino acid to codon relationship\n",
    "    \"A\" => [\"GCU\", \"GCC\", \"GCA\", \"GCG\"],\n",
    "    \"R\" => [\"CGU\", \"CGC\", \"CGA\", \"CGG\", \"AGA\", \"AGG\"],\n",
    "    \"N\" => [\"AAU\", \"AAC\"],\n",
    "    \"D\" => [\"GAU\", \"GAC\"],\n",
    "    \"B\" => [\"AAU\", \"AAC\", \"GAU\", \"GAC\"],\n",
    "    \"Q\" => [\"CAA\", \"CAG\"],\n",
    "    \"E\" => [\"GAA\", \"GAG\"],\n",
    "    \"Z\" => [\"CAA\", \"CAG\", \"GAA\", \"GAG\"],\n",
    "    \"G\" => [\"GGU\", \"GGC\", \"GGA\", \"GGG\"],\n",
    "    \"H\" => [\"CAU\", \"CAC\"],\n",
    "    \"I\" => [\"AUU\", \"AUC\", \"AUA\"],\n",
    "    \"L\" => [\"CUU\", \"CUC\", \"CUA\", \"CUG\", \"UUA\", \"UUG\"],\n",
    "    \"K\" => [\"AAA\", \"AAG\"],\n",
    "    \"M\" => [\"AUG\"],\n",
    "    \"F\" => [\"UUU\", \"UUC\"],\n",
    "    \"P\" => [\"CCU\", \"CCC\", \"CCA\", \"CCG\"],\n",
    "    \"S\" => [\"UCU\", \"UCC\", \"UCA\", \"UCG\", \"AGU\", \"AGC\"],\n",
    "    \"T\" => [\"ACU\", \"ACC\", \"ACA\", \"ACG\"],\n",
    "    \"W\" => [\"UGG\"],\n",
    "    \"Y\" => [\"UAU\", \"UAC\"],\n",
    "    \"V\" => [\"GUU\", \"GUC\", \"GUA\", \"GUG\"],\n",
    "    #\"1\" => [\"AUG\"],\n",
    "    #\"9\" => [\"UAA\", \"UGA\", \"UAG\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8d128c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"./data/codon.csv\""
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminos = [rand(collect(keys(amino_codon)), rand(40:60)) for i in 1:1000]\n",
    "create_genome(v) = prod(map(x->rand(amino_codon[x]), v))\n",
    "genomes = [create_genome(v) for v in aminos]\n",
    "df = DataFrame(acids=prod.(aminos), genomes=genomes)\n",
    "CSV.write(\"./data/codon.csv\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c5d73",
   "metadata": {},
   "source": [
    "Let's import and inspect the data. They are in the form of strings of letters. Indexing a single element of a string returns a `Char` character type but we want the tokens to be in form of strings. Let's create a processing function `string_split` to do this for us and create test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f0af9ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SDTDFWNMRH\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"UCGGACACUG\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the data\n",
    "using CSV, DataFrames, Transformers\n",
    "string_split(v) = map(x -> string(x), collect(v))\n",
    "df = CSV.File(\"./data/codon.csv\") |> DataFrame;\n",
    "display(df[1,:acids][1:10])\n",
    "display(df[1,:genomes][1:10])\n",
    "\n",
    "train_x = string_split.(df[1:800, :genomes]);\n",
    "train_y = string_split.(df[1:800, :acids]);\n",
    "test_x = string_split.(df[801:end, :genomes]);\n",
    "test_y = string_split.(df[801:end, :acids]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f5588",
   "metadata": {},
   "source": [
    "It's time to start constructing the Transfomer. First, create a convenience function to append start and stop tokens to a vector. Then, create a vocabulary of available tokens. Finally, encode the data with the tokenizer and use these tokens to embed tokens into a space of dimension 64. These will be augmented with positional embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a51b658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisers\n",
    "pre_process(v) = cat(\"1\", v..., \"9\"; dims=1)\n",
    "tokenizer_x = Transformers.Vocabulary(cat(\"1\", unique(train_x[1])..., \"9\", \"0\"; dims=1), \"0\")\n",
    "tokenizer_y = Transformers.Vocabulary(cat(\"1\", unique(train_y[1])..., \"9\", \"0\"; dims=1), \"0\")\n",
    "\n",
    "encoded_x = tokenizer_x.(train_x);\n",
    "encoded_y = tokenizer_y.(train_y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55a1ac",
   "metadata": {},
   "source": [
    "Create a simple embedding function. We will choose the dimensionality to be 64 to keep the model relatively small. The transfomer encoder will be composed of three blocks each with 4 heads and and inner dimensionality of 128. These are much smaller than the original model. The decoder will be similarly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b7f74bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embedding (generic function with 1 method)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 64\n",
    "h = 4\n",
    "hd = 16\n",
    "innerd = 128\n",
    "\n",
    "embed = Transformers.Embed(d, length(tokenizer_x))\n",
    "pe = Transformers.PositionEmbedding(d)\n",
    "function embedding(x)\n",
    "  we = embed(x, inv(sqrt(d)))\n",
    "  e = we .+ pe(we)\n",
    "  return e\n",
    "end\n",
    "\n",
    "encoder(x)\n",
    "    embedx = embed(x)\n",
    "    encode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94824c63",
   "metadata": {},
   "source": [
    "Let's now define our loss. Our final layer is the `softmax` layer converting the tokens into probabilities for the tokens in the output space. It makes sense to `onehot` encode the outputs and perform label smoothing to convert these outputs to a probability distribution. Then, an appropriate loss function is `crossentropy` a measure of the similarity between two probability distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(x, y)\n",
    "    ytarget = Flux.label_smoothing(Flux.onehot(tokenizer_y, y), 0.1f0)\n",
    "    ypred = decoder_forward(y, encoder_forward(x))\n",
    "    return Flux.logitcrossentropy(label[:, 2:end, :], probs[:, 1:end-1, :])\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

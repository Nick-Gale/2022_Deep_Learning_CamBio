
---
title: "Graph Neural Networks"
subtitle: "The True BluePrint"
author: "Nicholas Gale" 
engine: jupyter
execute:
  echo: true
format:
  revealjs:
    keep-tex: true
    monofont: "JuliaMono"
---

# Graphs

# Graphs in Science

* 

* Graphs are generalised non-linear data structures. 

# Formal Definitions

* A graph $G$ is an tuple of nodes $V$ and edges $E$: $(V,E)$.

* The nodes are labelled $\{1, 2, \ldots, N\}$.

* The edges are weighted relationships between nodes: $1 \mapsto 4$ etc. 

# Adjancency Matrix

* An adjancency matrix an object capturing node-edge relationships.

* The rows/columns are the vertex index.

* The matrix value $w_{ij}$ is the edge weight between $i$ and $j$.

:::: { .columns}

::: { .column width="50%"}

```{julia}
display([0 1 2 0; 1 0 1 0; 1 1 1 1; 0 1 2 0;])
```

:::

::: { .column width="50%"}

Insert graph here.

:::

::::

# Feature Matrix

* Graphs can be equipped with a feature matrix to hold data.

* Each node encodes a vector of $F$ measurable features: weight, chromosome number, membrane voltage, etc.

* The feature matrix is the $N \times F$ matrix of concatenated features.

# Permutation Invariance

* A permutation is a relabelling of a graph nodes.

* A permutation matrix $P$ reshuffles an adjacency matrix: $P^{-1} A P$.

* Changing graph labels doesn't change graphs.

* Analysis functions should *also* be permutation invariant e.g. `sum`

# Equivariance

# Neural Networks

* We now want to define a neural network on these graph.

* Our input is the graph and the feature matrix of data.

* We transform inputs $X_i$ on the nodes to latents $H_i$ through a neighbourhood invariant function $g$

* This is the GNN layer. There are three types of $g$.

* Convolutional, Attentional, and Message Passing.

# Convolutional

* We aggregate the sum of features of neighbours multiplied by a constant.

* Might recognise this as a propogation of neural state or neural field equation.

* It doesn't need to be a sum (any permutation invariant function will do).

$$ h_i = f\left(x_i, \sum_{j \wedge (ij)\inE} W_{ij} x_j \right) $$

# Attentional

* We perform a similar procedure but allow the weights to be dynamic.

* They are therefore functions dependent on data and learnable.

$$ h_i = f\left(x_i, \sum_{j \wedge (ij)\inE} W(x_i, x_j) x_j \right) $$

# Message Passing

* Receiver now particpates in constructing the signal to itself.

* Most generic form of graph network.

$$ h_i = f\left(x_i, \sum_{j \wedge (ij)\inE} \phi(x_i,x_j) \right) $$

# State update

* Early GNNs (1980s) applied update rule to latents until convergence.

* Modern GNNs do it a fixed time e.g. 5 or 20. 

$$ h_i(t + 1) = f(\left(h_i(t), \sum_{j \wedge (ij) \in E} \phi(x_i, x_j) \right) $$

# Objectives:

* Node classification: $p(h_i) \in \text{Category Set}$

* Graph classifcation: $p(G) = \in \text{Category Set}$

* Link Prediction: $p(i,j) = e_{ij} $

* These are normally trained on a classifier (dense) network on the latents.

# Loss Functions

* The loss functions for GNNs must be permutation invariant

# Technical Implementation

* **Combine all

# Batching and Padding

* A set of graphs might have variable numbers of nodes

* A solution is to appropriately pad adjanceny matrix with zeros.

* A block matrix has matrices embedded in a matrix: ``Wb = [W1 W2; W3 W4]``.

* A block diagonal matrix with `W2=W3=0` will preserve the structure of W1 and W2.

* Batches can be constructed with block-diagonals of input graphs.

# High Level

* Graphs are generalised forms of natural data: molecules,

* Graphs are defined by their adjacency matrix.

* GNNs are graphs with features defined from data.

* A GNN computes several passes of a message through the graph.

* The latents perform some learning task.

# ConvNets: A familiar smell

* We have already covered a form of GNNs: the conv net.

* The data structure is a grid topology: the edges are bidirectional in neighbouring pixels

* The message passing happens through the convolutional filters.

# Recurrent Nets: A familiar smell

* Another form of GNNs covered is the RNN.

* Line graph: each node has two edges for the data before and after.

* Sequences form a line graph.

* Message passing happens through the internal and input states.

# Generalised Neural Networks

* All of our current neural network topologies can be expressed as GNNs

* Formally: all networks can be generated by considering the symmetries and actions of a graph.

* It's still often better to use optimised pipelines 

# An eye on the future

* The generalisation of neural networks is a recent field: Geometric Deep Learning.

* It has generated a lot of interest recently.

* Looks poised to be well suited to biological data.

# A Halo Example

* Chemicals have a natural graph structure: represented

# AlphaFold

* The AlphaFold network is a GNN

* Here the p

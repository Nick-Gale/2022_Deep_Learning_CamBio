{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Flux\"\n",
        "subtitle: \"A Julia approach to Machine Learning\"\n",
        "author: \"Nicholas Gale and Stephen Eglen\" \n",
        "engine: jupyter\n",
        "execute:\n",
        "  echo: true\n",
        "format:\n",
        "  revealjs:\n",
        "    keep-tex: true\n",
        "    slide-number: true\n",
        "    theme: [serif, custom.scss]\n",
        "    chalkboard: true\n",
        "    incremental: true\n",
        "---"
      ],
      "id": "0d116df4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building Neural Networks\n",
        "\n",
        "* Time consuming.\n",
        "\n",
        "* Full of boilerplate.\n",
        "\n",
        "* Often inefficient.\n",
        "\n",
        "* These imply packages are useful!\n",
        "\n",
        "# Flux\n",
        "\n",
        "* Flux is a Julia package.\n",
        "\n",
        "* It is hackable, extensible, and provides nice syntatic sugar.\n",
        "\n",
        "* Relies on `Zygote` which is an automatic differentation package.\n",
        "\n",
        "# Lecture Outcomes\n",
        "\n",
        "* Revisit primary concepts: automatic differentation, optimisation, loss functions.\n",
        "\n",
        "* Grammar of Flux.\n",
        "\n",
        "* Basic Flux Usage.\n",
        "\n",
        "* Constructing Neural Networks.\n",
        "\n",
        "# Loss Functions\n",
        "\n",
        "* Loss functions allow objective of the network to be found.\n",
        "\n",
        "* Constructed to measure difference between regressor and datas.\n",
        "\n",
        "* Common loss functions are `mse`, `logitcrossentropy`, `binarycrossentropy`, etc.\n",
        "\n",
        "# Differentitation\n",
        "\n",
        "* Backbone of machine learning.\n",
        "\n",
        "* Newtons method or gradient descent is an optimisation method.\n",
        "\n",
        "* Can be computed symbolically, numerically, or automatically: $$ \\frac{df}{dx} = \\lim_{x\\rightarrow 0} \\frac{f(x+h) - f(x)}{h} $$\n",
        "\n",
        "# Automatic Differentitation\n",
        "\n",
        "* Define a dual number as: $d(x) = (x, \\epsilon)$\n",
        "\n",
        "* By Taylor series the derivative is given by $\\epsilon$.\n",
        "\n",
        "* Julia: operator overload all the usual functions to handle dual numbers.\n",
        "\n",
        "* Differentitation is now arbitrary precision and cheaper to compute.\n",
        "\n",
        "# Chain Rule and Backpropagation\n",
        "\n",
        "* Backpropagation is *the* machine learning algorithm.\n",
        "\n",
        "* Errors are back-propagated through layers of network.\n",
        "\n",
        "* Under the hood: chain rule.\n",
        "$$\\frac{\\partial F}{\\partial x} = \\frac{\\partial f_1}{\\partial f_2}\\frac{\\partial f_2}{\\partial f_3}\\ldots\\frac{\\partial f_n}{\\partial x}$$\n",
        "\n",
        "# Zygote\n",
        "\n",
        "* There are many automatic differentation packages in Julia: Flux uses Zygote.\n",
        "\n",
        "* The API call is ``gradient`` and is used in two ways: functions and `do` blocks.\n",
        "\n",
        "* Each method needs to specify the parameters which will be differentiated through `Flux.params` conversion.\n",
        "\n",
        "# Zygote Example\n"
      ],
      "id": "4603d33c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Flux\n",
        "f(x) = sum(tanh.(W * x))\n",
        "W = rand(2,2)\n",
        "x = rand(2,1)\n",
        "\n",
        "# a single instance\n",
        "dfdw = gradient(()->f(x), Flux.Params(W))\n",
        "\n",
        "# as a do block\n",
        "\n",
        "g = gradient(Flux.Params(W)) do\n",
        "\tf(x)\n",
        "end"
      ],
      "id": "5e7c515a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Do blocks are particularly useful for defining callback functions to monitor progress.\n",
        "\n",
        "# Optimisation\n",
        "\n",
        "* A loss function may be optimised in any way.\n",
        "\n",
        "* The random sampling method is as efficient as any for finding *global* minima.\n",
        "\n",
        "* Finding local minima (or just reducing loss) is most often done through gradient descent.\n",
        "\n",
        "# Gradient Descent and Momentum\n",
        "\n",
        "* Gradient descent is defined as: $$W_{t+1} = W_t - \\eta \\frac{\\partial L}{\\partial W} $$\n",
        "\n",
        "* Gradient descent may be accelerated by momentum.\n",
        "\n",
        "* Flux has support for multiple optimisers through an optimiser object class: `opt(lr, hyperparams)`\n",
        "\n",
        "* `Descent(lr), Momentum(lr, alp), ADAM()`\n",
        "\n",
        "#\n",
        "\n",
        "![Gradient Descent Comparison](./images/combined_momentum.gif)\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "\n",
        "* Gradients can be approximated by a subset of data.\n",
        "\n",
        "* Stochastic Gradient Descent is when these subsets are chosen randomly.\n",
        "\n",
        "* Helps avoid local minima and alleviates computer memory constraints.\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "* The activation function mimics the firing rate of a neuron.\n",
        "\n",
        "* It provides a non-linearity to allow complex learning manifolds.\n",
        "\n",
        "* Flux supports all the common functions: `tanh`, `relu`, `σ`, etc.\n",
        "\n",
        "#\n"
      ],
      "id": "d46e3912"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "using Plots\n",
        "x = -2:0.01:2\n",
        "plot(x, [Flux.σ.(x) Flux.tanh.(x) Flux.relu(x)]; labels = [\"σ\" \"tanh\" \"ReLu\"], ticks=false, title=\"Common Activation Functions\")"
      ],
      "id": "6fd21415",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flux layers\n",
        "\n",
        "* Neural networks are composed as layers of weights.\n",
        "\n",
        "* Flux offers default layers which can be found in the documentation: `Dense`, `Conv`, `MaxPool`, `RNNCell` etc.\n",
        "\n",
        "* We can also define our own custom layers by a structure.\n",
        "\n",
        "* Zygote differentates through structure fields \n",
        "\n",
        "# Custom Flux Layer Example\n"
      ],
      "id": "e4e448cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "struct CustomDense\n",
        "\tW\n",
        "\tb\n",
        "\tfunction CustomDense(dimsin::Int, dimsout::Int)\n",
        "\t\tW = rand(dimsout, dimsin)\n",
        "\t\tb = rand(dimsout)\n",
        "\t\tnew(W,b)\n",
        "\tend\n",
        "end\n",
        "# overload object definition\n",
        "(obj::CustomDense)(x) = obj.W * x .+ obj. b\n",
        "dense = CustomDense(5,2)\n",
        "dense(rand(5))"
      ],
      "id": "57a5f992",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dense\n",
        "\n",
        "* The dense layer represents the all-to-all connections.\n",
        "\n",
        "* API is `Dense(dimsin => dimsout)`.\n",
        "\n",
        "* Optional keywords are `bias=true/false`, and activation functions\n",
        "\n",
        "# Conv\n",
        "\n",
        "* The convolutional layer is specified with `Conv((kerneldim1, kerneldim))`\n",
        "\n",
        "* It accepts keywords `stride=(s1,s2)`, `pad=(p1, p2)`, `dilation=(d1,d2)`, `bias`, `activation`.\n",
        "\n",
        "* The layer accepts a 4D tensor: x dimension, y dimension, feature dimension, and data-index.\n",
        "\n",
        "# Flatten, Dropout, Pooling\n",
        "\n",
        "* `flatten` layers take an input and flatten it to a vector.\n",
        "\n",
        "* `Dropout` layers freeze a fraction of parameters in learning.\n",
        "\n",
        "* `BatchNorm` normalises mean/variance.\n",
        "\n",
        "*  `AlphaDropout` layers freeze parameters and normalise mean and variance.\n",
        " \n",
        "* There are extensive pooling layers e.g. `MaxPool` most common.\n",
        "\n",
        "# Chain: Composing a model\n",
        "\n",
        "* Composing a model is simply a chain of layers.\n",
        "\n",
        "* Flux allows this to be intutively done with chain: `model(x) = Chain(Dense(10=>3), Dense(3=5))`\n",
        "\n",
        "* A generically complex model can be given by a function `f`.\n",
        "\n",
        "* Flux can be told this complex function is a model with `@functor f`.\n",
        "\n",
        "# Data and Batching\n",
        "\n",
        "* Data is thought of \"naturally\" in Flux.\n",
        "\n",
        "* It operates on a single point, or a vector of points.\n",
        "\n",
        "* A convenience function `DataLoader` is provided.\n",
        "\n",
        "* It zips data pairs and can optionally shuffle with a given batch size.\n",
        "\n",
        "::: {.fragment}\n"
      ],
      "id": "81c9ae8c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvec = rand(40,100); yvec = rand(100);\n",
        "data = Flux.DataLoader((xvec, yvec); batchsize=5, shuffle=true)\n",
        "for (x, y) in data\n",
        "\t# loop 20 times through batches of 5\n",
        "end"
      ],
      "id": "6d9666a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Loss Functions\n",
        "\n",
        "* Loss functions are defined on individual data pairs.\n",
        "\n",
        "* Flux then aggregates them along a batch along the last dimension.\n",
        "\n",
        "* Loss aggregration is by default the average: can be specified with keyword `agg`.\n",
        "\n",
        "* There are many default loss functions: `mse`, `logitcrossentropy`, `HuberLoss` etc.\n",
        "\n",
        "# Updating parameters\n",
        "\n",
        "* We have a model, data, optimisers, and loss functions and can now update parameters.\n",
        "\n",
        "* `update!` takes an optimiser, gradient values, and updates specified parameters.\n",
        "\n",
        "* `update!(opt, Flux.Params(w...), grad)`\n",
        "\n",
        "* This can be applied through a loop over the data where the gradient is repeatedly computed.\n",
        "\n",
        "# Training\n",
        "\n",
        "* An even easier API to work with is `train!`. \n",
        "\n",
        "* It accepts a loss function, parameters, a data generator, and optimiser.\n",
        "\n",
        "* `train!(loss, Flux.Params(w...), data, opt)`\n",
        "\n",
        "# Callbacks\n",
        "\n",
        "* A callback function provides a printout under certain conditions.\n",
        "\n",
        "* They are passed as optional keywords to train e.g. `train!(...; cb = ()->println(\"This calls back\"))`\n",
        "\n",
        "* By default they print every epoch but can be slowed down with `cb = throttle(some_func, epoch_interval)`\n",
        "\n",
        "* Can be used with `Flux.stop()` to abort training if a condition is met e.g. accuracy goal, timing limit etc.\n",
        "\n",
        "# GPU support\n",
        "\n",
        "* A model can be piped into a GPU/CPU device with the `gpu` and `cpu` APIs\n",
        "\n",
        "* For a GPU based model the data should be converted to CUDA arrays.  Only CUDA is supported.\n",
        "\n",
        "::: {.fragment}\n"
      ],
      "id": "4616d18b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "using CUDA\n",
        "model(x) = Chain(Dense(5=>3), Dense(3=>1)) |> gpu\n",
        "inp = CUDA.rand(5)\n",
        "model(inp)"
      ],
      "id": "f87be71b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Model Saving\n",
        "\n",
        "* To save a model we need the `BSON` package.\n",
        "\n",
        "* A model is saved with `@save \"dir\" model_obj`\n",
        "\n",
        "* A model is loaded with `@load \"dir\" model_obj`\n",
        "\n",
        "* It is recommended to pipe models to a CPU device *before* saving.\n",
        "\n",
        "# Workflow\n",
        "\n",
        "* Within a training loop you can be expressive.\n",
        "\n",
        "* Common to plot loss functions on training and validation sets.\n",
        "\n",
        "* Common to plot a data-visualisation (particularly for generative models)\n",
        "\n",
        "* Can have an indication of training time: elapsed and estimated finish.\n",
        "\n",
        "# Training a CNN\n"
      ],
      "id": "3a5d2726"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Flux, JLD2\n",
        "data = JLD2.load(\"./data/sharks/sharkdata.jld\")\n",
        "training_data, training_labels = [data[\"train_data\"], data[\"train_labels\"]]\n",
        "\n",
        "model = Chain(\n",
        "Flux.Conv((3,3), 3=>10, relu),\n",
        "Flux.MaxPool((2,2)),\n",
        "Flux.Conv((5,5), 10 => 5, relu), \n",
        "Flux.MaxPool((2,2)), \n",
        "Flux.Conv((10,10), 5 => 7, relu), \n",
        "Flux.flatten,\n",
        "Dense(112, length(unique(training_labels)), σ))"
      ],
      "id": "dd5cea18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#\n"
      ],
      "id": "ccc353db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@time for e in 1:100 # epochs\n",
        "\td = Flux.DataLoader((training_data, Flux.onehotbatch(training_labels, unique(training_labels))), shuffle=true, batchsize=5)\n",
        "\tFlux.train!((x,y) -> Flux.logitcrossentropy(Flux.softmax(model(x)),y), Flux.params(model), d, ADAM())\n",
        "end\n",
        "\n",
        "predictions = map(i -> argmax(model(data[\"test_data\"])[:,i]), 1:size(model(data[\"test_data\"]))[2])\n",
        "labels = [\"thresher\", \"nurse\", \"basking\"]\n",
        "pred_labels = [labels[predictions[i]] for i in 1:length(predictions)]\n",
        "@show acc = sum(pred_labels .== data[\"test_labels\"])/length(pred_labels);"
      ],
      "id": "75616c04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary\n",
        "\n",
        "* Machine learning is sophisticated methods of optimising an objective/loss function.\n",
        "\n",
        "* Flux allows us to abstract the boiler-plate of machine learning code.\n",
        "\n",
        "* It is fast, flexible, and allows us to define models simply with a chain and layer structure.\n",
        "\n",
        "* Can focus on the conceptual difficulties, rather than the technical challenges.\n"
      ],
      "id": "bd635db3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.8",
      "language": "julia",
      "display_name": "Julia 1.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
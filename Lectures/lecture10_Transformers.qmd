---
title: "Generative Adverserial Networks"
subtitle: "Generative Modelling with Neural Networks"
author: "Nicholas Gale" 
engine: jupyter
execute:
  echo: true
format:
  revealjs:
    keep-tex: true
    monofont: "JuliaMono"
---

# Where are we going?

# Traditional Sequence Analysis

* Sequence analysis has been typically performed by recurrent neural networks.

* Exploding/vanishing gradients from recursion.

* Information decay leading to short memory.

* O(n) complexity in sequence length.

# Some solutions.

* LSTMs: forget gates to preserve information.

* Gated Recurrent Units: similar to LSTM but 

* Other proposals: none ideal, all sequential. 

# Transformers

* Transformers are the latest development in large scale sequence analysis.

* "Attention is all you need" (2016)

* Address many problems with RNNs

* Workhorse behind many "magical" applications e.g. voice assistant and language translation.

# Attention: bare bones

* Transformers leverage the idea of attention: not new.

* Attention is computed between all elements in a sequence.

* Asymetric, weighted relationship between sequence elements.

* All units are considered independently: massive parallelisation.

# Self Attention

* Self attention is similar to constructing the Hopfield network weights.

* It is a series of weighted matrices for each input.

* For each vector $x_i$ find relations with all other vectors $x_j$ using Hebb rule and normalise with `softmax`.

$$ a_{ij} = x_i^T x_j $$

$$ A_{ij} = \frac{\exp(a_{ij})}{\sum_j \exp(a_ij} $$

# Querys and Keys

* Imagine the vectors are one-hot batched: (0,0,1, \ldots, 0,0)

* The product $x_i^Tx_j$ will be one only in the indexes $i$ and $j$.

* Therefore, this operation is acting like a look-up table.

* Transformers inherit the language and call these *keys* and *queries*.

# Self Attention Retrevial

* An input is weighted by the row of the attention matrix corresponding to its index.

* The ouput is the weighted sum of all vectors by this attention row: $$ y_i = \sum_j A_{ij} x_j $$

# Self Attention Generalised

* We would like to let the keys and queries not be fully determined by a lookup value.

* We imagine them as linear transforms of the total dictionary embedding values into a new space.

* Therefore, they are matrices (K, Q) with a rank $k \times q$ and $q \times v$.

* This allows us to compress our key/value representation to a lower dimensionality.
   
* The objective of the Transformer is therefore to learn the weights of this embedding.

# Transfomer Attention Head

* The transformer attention mechanism is simply a generalisation of the self-attention mechanism.

$$ A(K,Q,V) = \text{softmax}(\frac{K^TQ}{\sqrt{d_k})V $$

* K represents the keys, and Q the querys. V represents the sequence of vectors.

* We scale by the number of dimensions for gradient stability in training.

* A sequence of length $L$ can be encoded in the matrix $V$ with dimension $v \times L$.

* The attention mechanism is one matrix calculation. Fast.

# Multi-head attention

* An attention head can be thought of like a convolutional filter.

* A transformer layer can have multiple heads.

* Each of these heads will learn to focus on different semantic relationships.

* This can be efficiently encoded by simply concatenating each individual head.

* Multi-head attention remains a single matrix calculation.

# Positional Encoding

* Attention lets us forget sequence order to parallelise computation.

* Sequence orders are important e.g. gene sequence cant be scrambled.

* A positional encoding function is used to inject this order into the learning.

* Typically: 

$$ p_k(i) = cos(i/10000^k) $$
$$ p_k(i) = sin(i/10000^k) $$

# Normalisation and Feed-forward pass.

* The output of the attention head is first layer normalised.

* A feed-foward network is used after attention and positional encoding.

* Each output vector is passed through the FF-network.

* The outputs are then layer normalised.

# Encoder

* The encoder is established with N layers.

* Each layer has a multi-head attention block with a feed-forward network.

* Key and Query embedding sizes are often chosen to partition the vector length.

* The output is a series of vectors which can be optionally merged into a single latent vector.

# Decoder

* The decoder is similarly composed of multihead attention layers.

* The latents of the encoder are combined with each decoder layer.

* This is done by composing the encoder latents with the outputs to create *keys* and *queries*.

* The keys and queries are composed with the values of the output layer *only*.

* These are layer-normalised.

* This generates the output sequence at a given recursion stage.

# Sequence Generation

* Sequences are generated with a start symbol and terminated with a stop symbol.

* The outputs are fed through the network as inputs until stopping.

* The output vector is fixed at an arbitrary length i.e. 512

* Future outputs are masked with `-Inf` to prevent left flowing information: autoregressive.

# Loss Functions

* The final step is to `softmax` outputs to generate a probability distribution against a dictionary.

* The natural loss function is `crossentropy`.

* Loss functions may be abritrary.

# Regularisation

* The original paper used dropout for the layer parameters and label smoothing.

* Dropout improves stability and convergence time.

* Label smoothing increases model perplexity (at the cost of labelled accuracy).


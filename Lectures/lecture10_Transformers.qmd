---
title: "Generative Adverserial Networks"
subtitle: "Generative Modelling with Neural Networks"
author: "Nicholas Gale and Stephen Eglen" 
engine: jupyter
execute:
  echo: true
format:
  revealjs:
    keep-tex: true
    slide-number: true
    theme: [serif, custom.scss]
    chalkboard: true 
    backgroundcolor: white
    incremental: true
---

# Traditional Sequence Analysis

* Sequence analysis has been typically performed by recurrent neural networks.

* Exploding/vanishing gradients from recursion.

* Information decay leading to short memory.

* O(n) complexity in sequence length.

# Some solutions.

* LSTMs: forget gates to preserve information.

* Gated Recurrent Units: similar to LSTM forget gates but no output. 

* Other proposals: none ideal due to O(n) complexity.

# Transformers

* "Attention is all you need" (2016)

* Transformers are the latest development in large scale sequence analysis.

* Address many problems with RNNs

* Workhorse behind many "magical" applications e.g. voice assistant and language translation.

# Attention: bare bones

* Transformers leverage the idea of attention: not new.

* Attention is computed between all elements in a sequence: a weighted relationship between locations.

* All units are considered independently: massive parallelisation.

# Attention

* Simple attention relies on nothing more than a Euclidean projection: dot product.

* ![](./images/dot_product.png)

# Simple attention: computation

* For each vector pair compute the dot product between them  and normalise over all pairs with `softmax`. $$ a_{ij} = x_i^T x_j $$ $$ A_{ij} = \frac{\exp(a_{ij})}{\sum_j \exp(a)_{ij}} $$

# Attention Retrevial

* An input is weighted by the row of the attention matrix corresponding to its index.

* The ouput is the weighted sum of all vectors by this attention row: $$ y_i = \sum_j A_{ij} x_j $$

# Querys and Keys

* Imagine the vectors are one-hot batched: (0,0,1, \ldots, 0,0)

* The product $x_i^Tx_j$ will be one only in the indexes $i=j$.

* This operation is acting like a look-up table.

* Transformers inherit the language and call these *keys* and *queries*.

# Attention Generalised

* We would like to let the keys, queries, and values not be fully determined by Euclidean representation. 

* We imagine them as linear transforms of the total dictionary embedding values into a new space.

* This allows us to compress our key/query/value representation to a lower dimensionality.

# 

![](./images/embedding_dimension.png)

# Transformers Bare Bones
   
* The transformer model is composed of two attentional models: an encoder and decoder.

* Input is in the form of vectors of tokens e.g. genome sequence.
 
* The encoder transforms input to an encoded attentional sequence.

* The decoder autoregressively uses the encoder ouput to an ouput sequence.

* The output sequence is decoded with a decoder dictionary e.g. amino acids.


# Transfomers.jl

* `Transformers.jl` is built on top of Flux.

* It has similar GPU support (and similiar drawbacks).

* Similar grammar but there are subtleties - documentation is helpful! 

* Open source contribution - documentation could be added to! Anyone can do this.


# Transformer Pre-Processing

* Token labels are encoded into a lookup table (Vocabulary).

* The input sequence tokens are first wrapped by "Start" and "End" tokens and encoded by the Vocabulary. 

* This encoded representation is embedded into vectors of length $v$.

#

```{julia}
#| echo: false
#| eval: true
using Transformers;
```

::: {.fragment}
```{julia}
#| incremental: true
#| eval: false
tokenizer = Transformers.Vocabulary(labels, unknown_symbol)
sequence = [start_token, sample_sequence_tokens..., end_token]
encoded_sequence = tokenizer(sequence)
embedder = Transformers.Embed(512, length(tokenizer)
embedded_seq = embedder(encoded_sequence)
```
:::

# Positional Encoding

* Attention lets us forget sequence order to parallelise computation.

* Sequence orders are important e.g. gene sequence cant be scrambled.

* A positional encoding function is used to inject this order into the learning.

#

* Positional encodings can be learnt but typically use a fucntion: $$ p_{2k}(i) = cos(i/10000^k) $$ $$ p_{2k+1}(i) = sin(i/10000^k) $$

::: {.fragment}
```{julia}
#| eval: false
positional_embedder = Transformers.PositionEmbedding(512)
em_postional_seq = embedded_seq .+ positional_embedder(embedded_seq)
```
:::

# 

![](./images/embedding_position.png)

# Transfomer Attention Head

* A sequence of length $L$ can be encoded by matrix $X$ with dimension $v \times L$.

* The Transformer attention mechanism is given by: $$A(K,Q,V) = \text{softmax}\left(\frac{K^TQ}{\sqrt{d_k}}\right) \odot V$$ $$K = W^KX, Q = W^QX, V = W^VX$$

#

![](./images/keys_queries.png)

# Attention Matrices

* Each input vector forms a row and these can be concatenated for efficiency into a matrix. 

* K represents the keys, Q the querys, V the values. Softmax is calculated row by row. $\odot$ is element-wise product.

* We scale by the number of dimensions for gradient stability in training.

#

![](./images/KQV.png)

#

![](./images/attention_head.png)

# Multi-head attention

* A transformer layer can have multiple heads analag. convolutional filter.

* Each of these heads will learn to focus on different semantic relationships.

* This can be efficiently encoded by simply concatenating each individual head. Usually, $n * h_d = d$.

* Head dimension is *not* a hyper-parameter.

# 

![](./images/multihead.png)

# Residual and Normalisation

* The outputs of the attention mechanism are mutliplied by matrix $W^O$ of dimension $d \times v$.

* The pre-attention inputs (residuals) are added to the re-embedded attention transformed inputs.

* This combined vector is then layer-normalised.

#

![](./images/residuals.png)

# Feed Forward Network

* The normalised self-attention and residuals are passed through a feed-foward network: $$F(x) = \text{ReLu}(W_1x + b_1)W_2 + b_2$$


# Inner Dimensions

* The inner-dimension is independent of the embedding dimension e.g. 2048.

* The feed forward network is shared between all tokens.

* The residuals are added to the ouput of the FFN and layer-normalised.

#

# Encoder Block/Layer

* The operations of self-attention, feed forward networks, and feed-forward make an encoder block.

* This can be thought of as a layer in a regular NN.

* The Encoder is formed of several encoder blocks e.g. 6

:::{.fragment}
```{julia}
#| eval: false
encoder_block = Transformers.Transformer(model_d, n_heads, head_d, inner_d)
encoder_block_output = encoder_block(block_input)
encoder = Chain(block1, ..., blockn)
```
:::

# 

![](./images/encoder_block.png)

# Decoder Block

* A decoder block has a self-attention, an encoder-attention, and a feed-forward network.

* The encoder-decoder-attention keys and values are constructed using output from encoder (and learnable matrices).

* These keys and values are shared across all decoder blocks.

```{julia}
#| eval: false
decoder_block = Transformers.TransformerDecoder(model_d, n_heads, head_d, inner_d)
decoder_block_output = decoder_block(block_input, encoder_output)
decoder = Chain(block1, ..., blockn)
```
#

![](./images/decoder_block.png)


# Sequence Generation

* Sequences are generated with a start symbol and terminated with a stop symbol.

* The outputs are fed through the network as inputs until stopping.

* The output vector is fixed at an arbitrary length i.e. 512

* Future outputs are masked with `-Inf` to prevent left flowing information: autoregressive.

# Loss Functions

* The final step is to `softmax` outputs to generate a probability distribution against a dictionary/vocabulary.

* The natural loss function is `crossentropy`.

* Loss functions may be abritrary.

# Regularisation

* The original paper used dropout for the layer parameters and label smoothing.

* Dropout improves stability and convergence time.

* Label smoothing increases model perplexity (at the cost of labelled accuracy).

# Model Summary

* The encoder takes positional and contextual inputs and the decoder autoregressively produces outputs.

* The encoder and decoders use attention heads and a feed-forward network to perform the learning.

* The attention mechanism transforms embedding into a different subspace through keys/queries/values matrices.

* Keys, Queries, Values are learned and represent optimal information relationships in the problem context.

#

![](./images/transformer_model.png)

# Transformers in Biology

* Transformers are in relative infancy - lots of work to be done.

* The obvious candidate is sequence analysis: genome and protein.

* Some interesting developments: gene transcription factors using Enformer (Deep Mind)

* Protein Prediction tasks.

# Transforming the Language Of Life.

* Protein prediction can be done classicaly: HMMs and BLAST. Exponentially computationally expensive.

* CNNs and RNNs are computationally more efficient but task depedent and dont generalise.

* Authors propose Transformer model PRoBERTa: pre-trained agnostic amino acid sequence representation.

* Protein prediction tasks: binary PPI and protein family classification.

# Problems

* Authors managed to achieve state of the art performance on target tasks (tasks not too important)

* The resulting model is vastly more computationally efficient: 128 GPUs for 4 days => 4 GPUs for 18 hours.

* Still difficult to reproduce. GPUs are top of the line and few people have access to this many.

* Transformers in general are large models and pose a reproducibility problem.

# Summary

* Transformers are a complex but powerful sequence analysis architecture.

* Julia offers support through `Transformers.jl`.

* Very successful, but computationally demanding and not easily reproducible.

* Relatively unplumbed in Biology.

# References

[Attention is all you need; Vaswani et. al. (2017)](https://arxiv.org/abs/1706.03762)

[Effective gene expression prediction from sequence by integrating long-range interactions; Avsec et. al. (2021)](https://www.nature.com/articles/s41592-021-01252-x)

[Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks; Nambiar et. al. (2020)](https://www.biorxiv.org/content/10.1101/2020.06.15.153643v1.full)

{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Generative Adverserial Networks\"\n",
        "subtitle: \"Generative Modelling with Neural Networks\"\n",
        "author: \"Nicholas Gale\" \n",
        "engine: jupyter\n",
        "execute:\n",
        "  echo: true\n",
        "format:\n",
        "  revealjs:\n",
        "    keep-tex: true\n",
        "    monofont: \"JuliaMono\"\n",
        "---"
      ],
      "id": "858ca3d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Traditional Sequence Analysis\n",
        "\n",
        "* Sequence analysis has been typically performed by recurrent neural networks.\n",
        "\n",
        "* Exploding/vanishing gradients from recursion.\n",
        "\n",
        "* Information decay leading to short memory.\n",
        "\n",
        "* O(n) complexity in sequence length.\n",
        "\n",
        "# Some solutions.\n",
        "\n",
        "* LSTMs: forget gates to preserve information.\n",
        "\n",
        "* Gated Recurrent Units: similar to LSTM but \n",
        "\n",
        "* Other proposals: none ideal, all sequential. \n",
        "\n",
        "# Transformers\n",
        "\n",
        "* Transformers are the latest development in large scale sequence analysis.\n",
        "\n",
        "* \"Attention is all you need\" (2016)\n",
        "\n",
        "* Address many problems with RNNs\n",
        "\n",
        "* Workhorse behind many \"magical\" applications e.g. voice assistant and language translation.\n",
        "\n",
        "# Attention: bare bones\n",
        "\n",
        "* Transformers leverage the idea of attention: not new.\n",
        "\n",
        "* Attention is computed between all elements in a sequence: a weighted relationship between elements.\n",
        "\n",
        "* All units are considered independently: massive parallelisation.\n",
        "\n",
        "# Attention: bare bones\n",
        "\n",
        "* Simple attention is similar to constructing the Hopfield network weights.\n",
        "\n",
        "* For each vector pair compute the dot product between them.\n",
        "\n",
        "* For each vector $x_i$ find relations with all other vectors $x_j$ using Hebb rule and normalise with `softmax`. $$ a_{ij} = x_i^T x_j $$ $$ A_{ij} = \\frac{\\exp(a)_{ij})}{\\sum_j \\exp(a)_{ij} $$\n",
        "\n",
        "# Querys and Keys\n",
        "\n",
        "* Imagine the vectors are one-hot batched: (0,0,1, \\ldots, 0,0)\n",
        "\n",
        "* The product $x_i^Tx_j$ will be one only in the indexes $i=j$.\n",
        "\n",
        "* Therefore, this operation is acting like a look-up table.\n",
        "\n",
        "* Transformers inherit the language and call these *keys* and *queries*.\n",
        "\n",
        "# Attention Retrevial\n",
        "\n",
        "* An input is weighted by the row of the attention matrix corresponding to its index.\n",
        "\n",
        "* The ouput is the weighted sum of all vectors by this attention row: $$ y_i = \\sum_j A_{ij} x_j $$\n",
        "\n",
        "# Attention Generalised\n",
        "\n",
        "* We would like to let the keys, queries, and values not be fully determined by a lookup value.\n",
        "\n",
        "* We imagine them as linear transforms of the total dictionary embedding values into a new space.\n",
        "\n",
        "* Therefore, they are matrices $(W^K, W^Q, W^V)$ with a size $v \\times d$.\n",
        "\n",
        "* This allows us to compress our key/query/value representation to a lower dimensionality.\n",
        "\n",
        "# Transformers\n",
        "   \n",
        "* The transformer model is composed of two attentional models: an encoder and decoder.\n",
        "\n",
        "* Input is in the form of vectors of tokens e.g. genome sequence.\n",
        " \n",
        "* The encoder transforms input to an encoded attentional sequence.\n",
        "\n",
        "* The decoder autoregressively uses the encoder ouput to an ouput sequence.\n",
        "\n",
        "* The output sequence is decoded with a decoder dictionary e.g. amino acids.\n",
        "\n",
        "# Transformer Pre-Processing\n",
        "\n",
        "* Token labels are encoded into a lookup table (Vocabulary).\n",
        "\n",
        "* The input sequence tokens are first wrapped by \"Start\" and \"End\" tokens and encoded by the Vocabulary. \n",
        "\n",
        "* This encoded representation is embedded into vectors of length $v$.\n",
        "\n",
        "::: {.fragment}"
      ],
      "id": "34b2c92a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "incremental": true
      },
      "source": [
        "#| eval: false\n",
        "tokenizer = Vocabulary(labels, unknown_symbol)\n",
        "sequence = [start, sample_sequence_tokens..., end]\n",
        "encoded_sequence = tokenizer(sequence)\n",
        "embedder = Transformers.Embed(512, length(tokenizer)\n",
        "embedded_seq = embedder(encoded_sequence)"
      ],
      "id": "c34ebf8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Positional Encoding\n",
        "\n",
        "* Attention lets us forget sequence order to parallelise computation.\n",
        "\n",
        "* Sequence orders are important e.g. gene sequence cant be scrambled.\n",
        "\n",
        "* A positional encoding function is used to inject this order into the learning.\n",
        "\n",
        "#\n",
        "\n",
        "* Positional encodings can be learnt but typically use a fucntion: $$ p_{2k}(i) = cos(i/10000^k) $$ $$ p_{2k+1}(i) = sin(i/10000^k) $$\n",
        "\n",
        "::: {.fragment}\n",
        "positional_embedder = Position(512)\n",
        "em_postional_seq = embedded_seq .+ postitional_embedder(embedded_seq)\n",
        ":::\n",
        "\n",
        "# Transfomer Attention Head\n",
        "\n",
        "* A sequence of length $L$ can be encoded by matrix $X$ with dimension $v \\times L$.\n",
        "\n",
        "* The transformer attention mechanism is simply a generalisation of this attention mechanism: $$ A(K,Q,V) = \\text{softmax}(\\frac{K^TQ}{\\sqrt{d_k}) \\odot V $$ $$K = W^KX, Q = W^QX, V = W^VX$$\n",
        "\n",
        "* K represents the keys, Q the querys, V the values. Softmax is calculated row by row. $\\odot$ is element-wise product.\n",
        "\n",
        "* We scale by the number of dimensions for gradient stability in training.\n",
        "\n",
        "# Multi-head attention\n",
        "\n",
        "* An attention head can be thought of like a convolutional filter.\n",
        "\n",
        "* A transformer layer can have multiple heads.\n",
        "\n",
        "* Each of these heads will learn to focus on different semantic relationships.\n",
        "\n",
        "* This can be efficiently encoded by simply concatenating each individual head. Usually, $n * h_d = d$.\n",
        "\n",
        "# Residual and Normalisation\n",
        "\n",
        "* The outputs of the attention mechanism are mutliplied by matrix $W^O$ of dimension $d \\times v$.\n",
        "\n",
        "* The pre-attention inputs (residuals) are added to the re-embedded attention transformed inputs.\n",
        "\n",
        "* This combined vector is then layer-normalised.\n",
        "\n",
        "* These residuals allow positional embeddings to be preserved.\n",
        "\n",
        "# Feed Forward Network\n",
        "\n",
        "* The normalised self-attention and residuals are passed through a feed-foward network: $$\\text{F(x) = \\text{ReLu}(W_1x + b_1)W_2 + b_2$$\n",
        "\n",
        "* The inner-dimension is independent of the embedding dimension e.g. 2048.\n",
        "\n",
        "* The feed forward network is shared between all tokens.\n",
        "\n",
        "* The residuals are added to the ouput of the FFN and layer-normalised.\n",
        "\n",
        "#\n",
        "\n",
        "* Image of Encoder Block*\n",
        "\n",
        "# Encoder Block/Layer\n",
        "\n",
        "* The operations of self-attention, feed forward networks, and feed-forward make an encoder block.\n",
        "\n",
        "* This can be thought of as a layer in a regular NN.\n",
        "\n",
        "* The Encoder is formed of several encoder blocks e.g. 6\n",
        "\n",
        ":::{.fragment}"
      ],
      "id": "a337ce24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "encoder_block = Transformers.Transformer(model_d, n_heads, head_d, inner_d)\n",
        "encoder_block_output = encoder_block(block_input)\n",
        "encoder = Chain(block1, ..., blockn)"
      ],
      "id": "b82ebcbf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Decoder Block\n",
        "\n",
        "* A decoder block has a self-attention, an encoder-attention, and a feed-forward network.\n",
        "\n",
        "* These are layer-normalised and apart from the encoder-attention behave as before.\n",
        "\n",
        "* The encoder-attention takes its queries from the decoder input.\n",
        "\n",
        "* The keys and values are constructed using the output from the top encoder layer.\n",
        "\n",
        "* These keys and values are shared across all decoder blocks.\n",
        "\n",
        "# \n"
      ],
      "id": "8e2dfc3b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "decoder_block = Transformers.TransformerDecoder(model_d, n_heads, head_d, inner_d)\n",
        "decoder_block_output = decoder_block(block_input, encoder_output)\n",
        "decoder = Chain(block1, ..., blockn)"
      ],
      "id": "0e61c006",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sequence Generation\n",
        "\n",
        "* Sequences are generated with a start symbol and terminated with a stop symbol.\n",
        "\n",
        "* The outputs are fed through the network as inputs until stopping.\n",
        "\n",
        "* The output vector is fixed at an arbitrary length i.e. 512\n",
        "\n",
        "* Future outputs are masked with `-Inf` to prevent left flowing information: autoregressive.\n",
        "\n",
        "# Loss Functions\n",
        "\n",
        "* The final step is to `softmax` outputs to generate a probability distribution against a dictionary/vocabulary.\n",
        "\n",
        "* The natural loss function is `crossentropy`.\n",
        "\n",
        "* Loss functions may be abritrary.\n",
        "\n",
        "# Regularisation\n",
        "\n",
        "* The original paper used dropout for the layer parameters and label smoothing.\n",
        "\n",
        "* Dropout improves stability and convergence time.\n",
        "\n",
        "* Label smoothing increases model perplexity (at the cost of labelled accuracy).\n",
        "\n",
        "# Model Summary\n",
        "\n",
        "* The encoder takes positional and contextual inputs and the decoder autoregressively produces outputs.\n",
        "\n",
        "* The encoder and decoders use attention heads and a feed-forward network to perform the learning.\n",
        "\n",
        "* The attention mechanism transforms embedding into a different subspace through keys/queries/values matrices.\n",
        "\n",
        "* Keys, Queries, Values are learned and represent optimal information relationships in the problem context.\n",
        "\n",
        "# Transformers in Biology\n",
        "\n",
        "* Transformers are in relative infancy - lots of work to be done.\n",
        "\n",
        "* The obvious candidate is sequence analysis: genome and protein.\n",
        "\n",
        "* Some interesting developments: gene transcription factors using Enformer (Deep Mind)\n",
        "\n",
        "* Protein Prediction tasks.\n",
        "\n",
        "# Transforming the Language Of Life.\n",
        "\n",
        "* Protein prediction can be done classicaly: HMMs and BLAST. Exponentially computationally expensive.\n",
        "\n",
        "* CNNs and RNNs are computationally more efficient but task depedent and dont generalise.\n",
        "\n",
        "* Authors propose Transformer model PRoBERTa: pre-trained agnostic amino acid sequence representation.\n",
        "\n",
        "* Protein prediction tasks: binary PPI and protein family classification.\n",
        "\n",
        "# Problems\n",
        "\n",
        "* Authors managed to achieve state of the art performance on tasks (tasks not too important)\n",
        "\n",
        "* The resulting model is vastly more computationally efficient: 128 GPUs for 4 days => 4 GPUs for 18 hours.\n",
        "\n",
        "* Still difficult to reproduce. GPUs are top of the line and few people have access to this many.\n",
        "\n",
        "* Transformers in general are large models and pose a reproducibility problem.\n",
        "\n",
        "# Summary\n",
        "\n",
        "* Transformers are "
      ],
      "id": "c0b79b5d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.8",
      "language": "julia",
      "display_name": "Julia 1.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
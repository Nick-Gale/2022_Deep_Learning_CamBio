{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Generative Adverserial Networks\"\n",
        "subtitle: \"Generative Modelling with Neural Networks\"\n",
        "author: \"Nicholas Gale and Stephen Eglen\" \n",
        "engine: jupyter\n",
        "execute:\n",
        "  echo: true\n",
        "format:\n",
        "  revealjs:\n",
        "    keep-tex: true\n",
        "    slide-number: true\n",
        "    theme: [serif, custom.scss]\n",
        "    chalkboard: true \n",
        "    backgroundcolor: white\n",
        "    incremental: true\n",
        "---"
      ],
      "id": "96af4b33"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What are GANs\n",
        "\n",
        "* GANs are a generative model of neural networks\n",
        "\n",
        "* They learn a statistical distributions properties.\n",
        "\n",
        "* They can then be used to generate samples from that distribution.\n",
        "\n",
        "* Famous examples are style transfer and face generation.\n",
        "\n",
        "# The Basic Idea\n",
        "\n",
        "* A GAN is really a composition of two models: a generator and a discriminator.\n",
        "\n",
        "* The generator is trained to trick the discriminator.\n",
        "\n",
        "* The discriminator is trained to filter real data from fake.\n",
        "\n",
        "* The goals are opposed: adverserial.\n",
        "\n",
        "# Components\n",
        "\n",
        "* Suppose the data are generated from some distribution: $$ x \\sim \\text{Dist}_\\text{Data}(\\alpha...) $$ \n",
        "\n",
        ":::{.incremental}\n",
        "\n",
        "* ![](./images/GANarchitecture_data.png){fig-align=\"center\"}\n",
        "\n",
        ":::\n",
        "\n",
        "#\n",
        "\n",
        "* The discriminator is a function mapping to a binary target: $$ D \\rightarrow \\{ 0, 1 \\} $$ \n",
        "\n",
        ":::{.incremental}\n",
        "\n",
        "* ![](./images/GANarchitecture_discriminator.png){fig-align=\"center\"}\n",
        "\n",
        ":::\n",
        "\n",
        "#\n",
        "\n",
        "* The generator $G$ takes a latent vector $z \\sim U(0,1)^k$ and generates a fake data sample: $$G(z) = y \\sim \\text{Dist}_\\text{Gen}(\\alpha^\\prime \\ldots) $$\n",
        "\n",
        ":::{.incremental}\n",
        "\n",
        "* ![](./images/GANarchitecture_generator.png){fig-align=\"center\"}\n",
        "\n",
        ":::\n",
        "\n",
        "#\n",
        "\n",
        "* The training goal is to approximate the data generating distribution: $$ \\text{Dist}_\\text{Gen}(\\alpha ^\\prime \\ldots) \\approx \\text{Dist}_\\text{Data}(\\alpha \\ldots) $$\n",
        "\n",
        ":::{.incremental}\n",
        "\n",
        "* ![](./images/GANarchitecture.png){fig-align=\"center\"}\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.notes}\n",
        "* We are always interested in some data generating distribution.\n",
        "\n",
        "* The discriminator needs to be trained to see if the data is real (1) or fake (0)\n",
        "\n",
        "* The generator needs to trick the discriminator and is a probability distribution from a random variable $Z \\sim U(0,1)^k$. \n",
        "\n",
        "* The latent space is commonly uniform between 0 and 1. More dimensions are added for complexity.\n",
        ":::\n",
        "\n",
        "# Construction\n",
        "\n",
        "* $G$ and $D$ are arbitrary probability transforms.\n",
        "\n",
        "* Neural networks are common choices: generalised functions.\n",
        "\n",
        "* Desirable to use data to inform network architecture.\n",
        "\n",
        "* `ConvTranspose` is a usual Flux layer for generator.\n",
        "\n",
        "# Loss Functions\n",
        "\n",
        "* The discriminator is acting as a binary classifier.\n",
        "\n",
        "* The natural loss function is `logitbinarycrossentropy`: $$ L = -E_{x\\in\\text{Dist}_\\text{Data}} [\\log(D(x))] - E_{y \\in \\text{Dist}_\\text{Gen}}[\\log(1-D(G(z))]$$\n",
        "\n",
        "# Discriminator Loss\n",
        "\n",
        "* The discriminator aims to minimise the above loss.\n",
        "\n",
        "* The generator is targeting a vector in the data distribution.\n",
        "\n",
        "* For labelled data $(x,y)$ this reduces to: $$ L_D(x, y) = -y\\log(D(x)) - (1 - y) \\log(1 - D(x))$$\n",
        "\n",
        "# Generator Loss\n",
        "\n",
        "* The generator is aiming to trick the discriminator.\n",
        "\n",
        "* Natural reward is to flip the classification loss.\n",
        "\n",
        "* It doesn't need to know the data: this is implicitly learned. $$ L_G(z) = -\\log(D(G(z))) $$\n",
        "\n",
        "::: {.notes}\n",
        "* Implicit learning is good\n",
        "\n",
        "* You cant just learn the copy function from available data\n",
        ":::\n",
        "\n",
        "# Training\n",
        "\n",
        "* Training proceeds in a two-step fashion.\n",
        "\n",
        "* First, batches are presented to the generator and G updated.\n",
        "\n",
        "* Second, batches are presented to discriminator and D updated.\n",
        "\n",
        "# A simple example\n",
        "\n",
        "Consider a non-trivial probability distribution:\n",
        "\n",
        "![](./images/gan_pdf.png)\n",
        "\n",
        "# Training Code\n"
      ],
      "id": "cf3ab86b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-line-numbers: \"1,2,3,4,9,10\"\n",
        "D(samples) = Chain(Dense(samples=>100, relu), Dense(100=>50, relu), Dense(50=>1, Ïƒ))\n",
        "G(input, samples) = Chain(Dense(input=>50, relu), Dense(50=>1000, relu), Dense(1000=>samples, x->0.5*tanh(x), bias=false))\n",
        "lossD(x, y, dsc) = -mean(y.*log.(dsc(x))) .- mean((1 .- y).*log.(1 .- dsc(x)))\n",
        "lossG(z, gen, dsc) = -.mean(log.(dsc(gen(z))))\n",
        "\n",
        "# Training functions\n",
        "function train_discriminator(dsc, gen, gendata, real_data, batch, opt)\n",
        "\tdata = DataLoader(hcat(gendata, realdata), batchsize=batch, shuffle=true)\n",
        "\tFlux.train!(lossD, Flux.params(dsc), data, opt)\n",
        "\tFlux.train!(lossG, Flux.params(gen), gendata, opt)\n",
        "end"
      ],
      "id": "4a73a055",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Result\n",
        "\n",
        "![](./images/gan_pdf_learning.gif)\n",
        "\n",
        "# Game Theory\n",
        "\n",
        "* $G$ and $D$ are playing a two player game.\n",
        "\n",
        "* This loss function is a min-max game.\n",
        "\n",
        "* Training converges to a Nash equilibrium.\n",
        "\n",
        "* The global minima of the game is $\\text{D}_\\text{Data}(\\alpha \\ldots) = \\text{D}_\\text{Gen}(\\alpha^\\prime \\ldots)$\n",
        "\n",
        "# MNIST\n",
        "\n",
        "* A famous dataset of handwritten numbers.\n",
        "\n",
        "* Very common machine learning benchmark.\n",
        "\n",
        "* ![MNIST Sample](./images/mnist.png) \n",
        "\n",
        "# Instability\n",
        "\n",
        "* GANs are difficult to train even on simple data such as MNIST.\n",
        "\n",
        "* Getting a GAN to converge is often a case of \"machine teaching\".\n",
        "\n",
        "* This is valuable in and of itself - helps construct the mental model of the problem.\n",
        "\n",
        "* There are some common GAN instabilities: saturation and mode collapse.\n",
        "\n",
        "# Code\n"
      ],
      "id": "32f9d6f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "distinit(shape...) = randn(Float32, shape) * 0.025f0\n",
        "\n",
        "discriminator = Chain(\n",
        "        Conv((4, 4), 1 => 64; stride = 2, pad = SamePad(), init = distinit), x->leakyrelu.(x, 0.2f0),\n",
        "        Dropout(0.3),\n",
        "        Conv((4, 4), 64 => 128; stride = 2, pad = SamePad(), init = distinit), x->leakyrelu.(x, 0.2f0),\n",
        "        Dropout(0.3),\n",
        "        Flux.flatten,\n",
        "        Dense(7 * 7 * 128, 1) \n",
        "    ) |> device\n",
        "\n",
        "generator = Chain(\n",
        "        Dense(latent_dim, 7*7*256, bias=false),\n",
        "        BatchNorm(7*7*256, relu),\n",
        "        x -> reshape(x, 7, 7, 256, :),\n",
        "        ConvTranspose((5, 5), 256 => 128; stride = 1, pad = SamePad(), init = distinit, bias=false),\n",
        "        BatchNorm(128, relu),\n",
        "        ConvTranspose((5, 5), 128 => 64; stride = 2, pad = SamePad(), init = distinit, bias=false),\n",
        "        BatchNorm(64, relu),\n",
        "        ConvTranspose((5, 5), 64 => 1, tanh; stride = 2, pad = SamePad(), init = distinit, bias=false),\n",
        "    ) |> device"
      ],
      "id": "8bbb9422",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss Saturation\n",
        "\n",
        "* The min-max loss function often saturates: the gradients go to zero.\n",
        "\n",
        "* This can happen when the discriminator gives an overly confident rejection.\n",
        "\n",
        "* Less of a problem with Wasserstein Loss GANS.\n",
        "\n",
        "# Loss Saturation\n",
        "\n",
        "![](./images/gan_saturate.gif)\n",
        "\n",
        "# Saturation Fixes\n",
        "\n",
        "* Adjust learning rate down for the discriminator.\n",
        "\n",
        "* Change discriminator complexity.\n",
        "\n",
        "* Apply dropout and other regularisation techniques.\n",
        "\n",
        "# Mode Collapse\n",
        "\n",
        "* The generator doesn't know the variability of the data.\n",
        "\n",
        "* It can produce a very reliable subset which tricks the generator.\n",
        "\n",
        "* This is known as mode-collapse: the target data is a composition of modes.\n",
        "\n",
        "# Mode Collapse\n",
        "\n",
        "![](./images/gan_mode_collapse.gif)\n",
        "\n",
        "# Mode Collapse Fixes\n",
        "\n",
        "* Increase latent size: more degrees of freedom to optimise.\n",
        "\n",
        "* Modify optimiser and learning schedule: a slower rate helps.\n",
        "\n",
        "* Regularisation: dropout and normalisation are useful.\n",
        "\n",
        "# Training Practice\n",
        "\n",
        "* GANs should be monitored.\n",
        "\n",
        "* Leverage your visual system: loss, discriminator accuracy, and generated data.\n",
        "\n",
        "* Generator and discriminator loss should be separated.\n",
        "\n",
        "* Real and fake accuracy should be separated.\n",
        "\n",
        "#\n",
        "\n",
        "![](./images/gan_arbitrary_train.gif)\n",
        "\n",
        "# Regularisation\n",
        "\n",
        "* Mode collapse is often related to Lipschitz continuity being violated in the discriminator: gradient penalty mechanism.\n",
        "\n",
        "* Weight normalisation is often performed through Spectral Normalisation: $$ L += \\sqrt{\\Vert W \\Vert_1 \\Vert W \\Vert_\\infty} $$\n",
        "\n",
        "# Regularisation Tricks\n",
        "\n",
        "* Discriminator is trained with multiple updates for each generator update.\n",
        "\n",
        "* Wasserstein loss (Earth Movers Distance) redefines the game to arbitrary maximimisation - not classification.\n",
        "\n",
        "* Usual regularisation on individual networks: dropout, early stopping etc.\n",
        "\n",
        "# Data Augmentation\n",
        "\n",
        "* A useful feature of GANs is data augmentation: supplementing data when there is none.\n",
        "\n",
        "* We simply generate data by sampling in the latent space and passing it through the generator.\n",
        "\n",
        "* This is only useful when we are *certain* the GAN has converged correctly - serious error potential.\n",
        "\n",
        "* Any mismatch between reality and generator will carry to downstream analysis.\n",
        "\n",
        "# Problem: How to control variance?\n",
        "\n",
        "* A problem in experimental sciences: day-to-day variance, and experimenter-experimenter variance.\n",
        "\n",
        "* Same underlying biology (or other natural phenomena).\n",
        "\n",
        "* We would like to remove the bias introduced by different experiments.\n",
        "\n",
        "# Batch Equalisation\n",
        "\n",
        "* A modern approach is to use GANs as a style transfer for data.\n",
        "\n",
        "* Each batch of data is assumed to have some underlying constant semantics.\n",
        "\n",
        "* Each batch has natural bias and variance introduced by batch effect.\n",
        "\n",
        "* Train a generator to map each batch to a target batch - style transfer.\n",
        "\n",
        "* This removes the batch effect and increases analysis power.\n",
        "\n",
        "# Summary\n",
        "\n",
        "* A GAN is a two-player game between a generator model and a discriminator model.\n",
        "\n",
        "* The discriminator classifies real/fake data and the generator plays to trick it.\n",
        "\n",
        "* The game is globally minimised when the generator matches the data-generating distribution.\n",
        "\n",
        "* The generator can be used to analyse a distribution, augment/generate data, and normalise datasets.\n",
        "\n",
        "# References\n",
        "\n",
        "[Generative Adversarial Nets, Goodfellow et. al. (2014)](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)\n",
        "\n",
        "[Wasserstein generative adversarial networks, Arkovsky et. al. (2017)](https://proceedings.mlr.press/v70/arjovsky17a.html)\n",
        "\n",
        "[Batch equalization with a generative adversarial network, Qian et. al. (2020)](https://academic.oup.com/bioinformatics/article/36/Supplement_2/i875/6055901)"
      ],
      "id": "401c8933"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.8",
      "language": "julia",
      "display_name": "Julia 1.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}